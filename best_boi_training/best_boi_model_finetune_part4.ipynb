{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import base64\n",
    "import io\n",
    "import zipfile\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, ConcatDataset, Dataset\n",
    "from torchvision import models, transforms, datasets\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Use your existing class definitions\n",
    "class CompositeDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def base64_to_image(self, base64_string):\n",
    "        image_data = base64.b64decode(base64_string)\n",
    "        image = Image.open(io.BytesIO(image_data)).convert('L')\n",
    "        image_array = np.array(image, dtype=np.float32) / 255.0\n",
    "        return torch.from_numpy(image_array).unsqueeze(0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        image = self.base64_to_image(item['composite'])\n",
    "        label = torch.tensor(int(item['true_digit']), dtype=torch.long)\n",
    "        return image, label\n",
    "\n",
    "class SelectionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = [item for sublist in data for item in sublist]  # Flatten the list of lists\n",
    "\n",
    "    def base64_to_image(self, base64_string):\n",
    "        image_data = base64.b64decode(base64_string)\n",
    "        image = Image.open(io.BytesIO(image_data)).convert('L')\n",
    "        image_array = np.array(image, dtype=np.float32) / 255.0\n",
    "        return torch.from_numpy(image_array).unsqueeze(0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        image = self.base64_to_image(item['selected_image'])\n",
    "        label = torch.tensor(int(item['true_digit']), dtype=torch.long)\n",
    "        return image, label\n",
    "\n",
    "class ExperimentalDataset(Dataset):\n",
    "    def __init__(self, images, labels):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.images[idx], self.labels[idx]\n",
    "\n",
    "class WeightedCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self, class_weights):\n",
    "        super().__init__()\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        return F.cross_entropy(outputs, targets, weight=self.class_weights)\n",
    "\n",
    "# Sanity check functions\n",
    "def load_data(test_file_path, train_file_path):\n",
    "    with open(test_file_path, 'r') as f:\n",
    "        test_data = json.load(f)\n",
    "    \n",
    "    with open(train_file_path, 'r') as f:\n",
    "        train_data = json.load(f)\n",
    "    \n",
    "    return test_data, train_data\n",
    "\n",
    "def create_dummy_ensemble():\n",
    "    model = models.resnet18(pretrained=False)\n",
    "    model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "    model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "    return model\n",
    "\n",
    "def load_all_experimental_data(test_digits_folder):\n",
    "    train_images = []\n",
    "    train_labels = []\n",
    "    test_images = []\n",
    "    test_labels = []\n",
    "    participant_data = {}\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((16, 16)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,)) #PLEASE CHANGE THIS TO THE TRAINING NORMALIZATION VALUES\n",
    "    ])\n",
    "\n",
    "    for filename in os.listdir(test_digits_folder):\n",
    "        if filename.endswith('.zip') and filename.startswith('experiment_results_participant'):\n",
    "            participant_number = int(filename.split('participant')[1].split('.')[0])\n",
    "            zip_filepath = os.path.join(test_digits_folder, filename)\n",
    "\n",
    "            participant_train_images = []\n",
    "            participant_train_labels = []\n",
    "            participant_test_images = []\n",
    "            participant_test_labels = []\n",
    "\n",
    "            with zipfile.ZipFile(zip_filepath, 'r') as zip_ref:\n",
    "                for img_filename in zip_ref.namelist():\n",
    "                    if img_filename.endswith('.png'):\n",
    "                        with zip_ref.open(img_filename) as file:\n",
    "                            img = Image.open(file).convert('L')  # Convert to grayscale\n",
    "                            img_tensor = transform(img)\n",
    "                            \n",
    "                            digit = int(img_filename.split('_')[0])\n",
    "                            \n",
    "                            if 'composite' in img_filename:\n",
    "                                test_images.append(img_tensor)\n",
    "                                test_labels.append(digit)\n",
    "                                participant_test_images.append(img_tensor)\n",
    "                                participant_test_labels.append(digit)\n",
    "                            else:\n",
    "                                train_images.append(img_tensor)\n",
    "                                train_labels.append(digit)\n",
    "                                participant_train_images.append(img_tensor)\n",
    "                                participant_train_labels.append(digit)\n",
    "\n",
    "            participant_data[participant_number] = {\n",
    "                'train': (torch.stack(participant_train_images), torch.tensor(participant_train_labels)),\n",
    "                'test': (torch.stack(participant_test_images), torch.tensor(participant_test_labels))\n",
    "            }\n",
    "\n",
    "    print(f\"Total training images: {len(train_images)}\")\n",
    "    print(f\"Total test images: {len(test_images)}\")\n",
    "    \n",
    "    for participant, data in participant_data.items():\n",
    "        print(f\"Participant {participant}:\")\n",
    "        print(f\"  Training images: {len(data['train'][0])}\")\n",
    "        print(f\"  Test images: {len(data['test'][0])}\")\n",
    "\n",
    "    return (torch.stack(train_images), torch.tensor(train_labels), \n",
    "            torch.stack(test_images), torch.tensor(test_labels),\n",
    "            participant_data)\n",
    "\n",
    "def sanity_check():\n",
    "    print(\"Running sanity check...\")\n",
    "    \n",
    "    # Load data\n",
    "    test_data, train_data = load_data('training_data/test_set/participant_0.json', 'training_data/training_set/participant_0.json')\n",
    "    \n",
    "    if not test_data or not train_data:\n",
    "        print(\"Error: Could not load data files.\")\n",
    "        return\n",
    "    \n",
    "    # Load experimental data\n",
    "    exp_train_images, exp_train_labels, exp_test_images, exp_test_labels, _ = load_all_experimental_data('test_digits')\n",
    "    \n",
    "    # Create small datasets\n",
    "    train_composite_dataset = CompositeDataset(test_data[:10])\n",
    "    train_selection_dataset = SelectionDataset(train_data[:1])\n",
    "    val_composite_dataset = CompositeDataset(test_data[10:20])\n",
    "    exp_train_dataset = ExperimentalDataset(exp_train_images[:100], exp_train_labels[:100])\n",
    "    exp_test_dataset = ExperimentalDataset(exp_test_images[:20], exp_test_labels[:20])\n",
    "    \n",
    "    # Create inverted MNIST dataset\n",
    "    mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,)),\n",
    "        transforms.Lambda(lambda x: 1 - x)  # Invert colors\n",
    "    ]))\n",
    "    mnist_train_subset = torch.utils.data.Subset(mnist_train, range(100))  # Use only 100 samples for sanity check\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_composite_loader = DataLoader(train_composite_dataset, batch_size=2, shuffle=True)\n",
    "    train_selection_loader = DataLoader(train_selection_dataset, batch_size=2, shuffle=True)\n",
    "    val_loader = DataLoader(val_composite_dataset, batch_size=2, shuffle=False)\n",
    "    exp_train_loader = DataLoader(exp_train_dataset, batch_size=2, shuffle=True)\n",
    "    exp_test_loader = DataLoader(exp_test_dataset, batch_size=2, shuffle=False)\n",
    "    mnist_train_loader = DataLoader(mnist_train_subset, batch_size=2, shuffle=True)\n",
    "    \n",
    "    # Create a dummy ensemble model\n",
    "    model = create_dummy_ensemble()\n",
    "    \n",
    "    # Calculate class weights (considering all datasets)\n",
    "    combined_dataset = ConcatDataset([train_composite_dataset, train_selection_dataset, exp_train_dataset, mnist_train_subset])\n",
    "    class_weights = calculate_class_weights(combined_dataset)\n",
    "    \n",
    "    # Run a single epoch of training\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    criterion = WeightedCrossEntropyLoss(class_weights.to(device))\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "    \n",
    "    print(\"Training on composite images...\")\n",
    "    train_single_epoch(model, train_composite_loader, criterion, optimizer, device)\n",
    "    \n",
    "    print(\"Training on selection images...\")\n",
    "    train_single_epoch(model, train_selection_loader, criterion, optimizer, device)\n",
    "    \n",
    "    print(\"Training on experimental images...\")\n",
    "    train_single_epoch(model, exp_train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    print(\"Training on inverted MNIST images...\")\n",
    "    train_single_epoch(model, mnist_train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    print(\"Validating...\")\n",
    "    val_accuracy = evaluate_model(model, val_loader, device)\n",
    "    print(f\"Validation Accuracy (Composite): {val_accuracy:.2f}%\")\n",
    "    \n",
    "    exp_accuracy = evaluate_model(model, exp_test_loader, device)\n",
    "    print(f\"Validation Accuracy (Experimental): {exp_accuracy:.2f}%\")\n",
    "    \n",
    "    print(\"Sanity check completed successfully!\")\n",
    "\n",
    "def train_single_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    for inputs, labels in loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Epoch completed.\")\n",
    "\n",
    "def calculate_class_weights(dataset):\n",
    "    class_counts = torch.zeros(10)\n",
    "    for _, label in dataset:\n",
    "        class_counts[label] += 1\n",
    "    return 1.0 / (class_counts + 1e-5)  # Add small epsilon to avoid division by zero\n",
    "\n",
    "def evaluate_model(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return 100 * correct / total\n",
    "\n",
    "# Run the sanity check\n",
    "sanity_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import base64\n",
    "import io\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, ConcatDataset, Dataset\n",
    "from torchvision import models, transforms, datasets\n",
    "from tqdm import tqdm\n",
    "\n",
    "class CompositeDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def base64_to_image(self, base64_string):\n",
    "        image_data = base64.b64decode(base64_string)\n",
    "        image = Image.open(io.BytesIO(image_data)).convert('L')\n",
    "        image_array = np.array(image, dtype=np.float32) / 255.0\n",
    "        return torch.from_numpy(image_array).unsqueeze(0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        image = self.base64_to_image(item['composite'])\n",
    "        label = torch.tensor(int(item['true_digit']), dtype=torch.long)\n",
    "        return image, label\n",
    "\n",
    "class SelectionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = [item for sublist in data for item in sublist]  # Flatten the list of lists\n",
    "\n",
    "    def base64_to_image(self, base64_string):\n",
    "        image_data = base64.b64decode(base64_string)\n",
    "        image = Image.open(io.BytesIO(image_data)).convert('L')\n",
    "        image_array = np.array(image, dtype=np.float32) / 255.0\n",
    "        return torch.from_numpy(image_array).unsqueeze(0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        image = self.base64_to_image(item['selected_image'])\n",
    "        label = torch.tensor(int(item['true_digit']), dtype=torch.long)\n",
    "        return image, label\n",
    "\n",
    "def load_data(train_folder, test_folder, num_participants=5):\n",
    "    train_data = []\n",
    "    for file in os.listdir(train_folder)[:num_participants]:\n",
    "        with open(os.path.join(train_folder, file), 'r') as f:\n",
    "            train_data.extend(json.load(f))\n",
    "    \n",
    "    test_data = []\n",
    "    for file in os.listdir(test_folder)[:num_participants]:  # Use the same number of participants for test data\n",
    "        with open(os.path.join(test_folder, file), 'r') as f:\n",
    "            test_data.extend(json.load(f))\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "def sanity_check():\n",
    "    print(\"Running sanity check...\")\n",
    "    \n",
    "    # Load data\n",
    "    train_data, test_data = load_data('training_data/training_set', 'training_data/test_set', num_participants=2)\n",
    "    \n",
    "    if not train_data or not test_data:\n",
    "        print(\"Error: Could not load data files.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Loaded {len(train_data)} training samples and {len(test_data)} test samples.\")\n",
    "    \n",
    "    # Load experimental data\n",
    "    exp_train_images, exp_train_labels, exp_test_images, exp_test_labels, _ = load_all_experimental_data('test_digits')\n",
    "    \n",
    "    print(f\"Loaded {len(exp_train_images)} experimental training samples and {len(exp_test_images)} experimental test samples.\")\n",
    "    \n",
    "    # Create small datasets\n",
    "    train_composite_dataset = CompositeDataset(test_data[:10])\n",
    "    train_selection_dataset = SelectionDataset(train_data[:1])\n",
    "    val_composite_dataset = CompositeDataset(test_data[10:20])\n",
    "    exp_train_dataset = ExperimentalDataset(exp_train_images[:100], exp_train_labels[:100])\n",
    "    exp_test_dataset = ExperimentalDataset(exp_test_images[:20], exp_test_labels[:20])\n",
    "    \n",
    "    # Create inverted MNIST dataset\n",
    "    mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,)),\n",
    "        transforms.Lambda(lambda x: 1 - x)  # Invert colors\n",
    "    ]))\n",
    "    mnist_train_subset = torch.utils.data.Subset(mnist_train, range(100))  # Use only 100 samples for sanity check\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_composite_loader = DataLoader(train_composite_dataset, batch_size=2, shuffle=True)\n",
    "    train_selection_loader = DataLoader(train_selection_dataset, batch_size=2, shuffle=True)\n",
    "    val_loader = DataLoader(val_composite_dataset, batch_size=2, shuffle=False)\n",
    "    exp_train_loader = DataLoader(exp_train_dataset, batch_size=2, shuffle=True)\n",
    "    exp_test_loader = DataLoader(exp_test_dataset, batch_size=2, shuffle=False)\n",
    "    mnist_train_loader = DataLoader(mnist_train_subset, batch_size=2, shuffle=True)\n",
    "    \n",
    "    print(\"Data loaders created successfully.\")\n",
    "    print(\"Sanity check completed successfully!\")\n",
    "\n",
    "# Run the sanity check\n",
    "sanity_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import base64\n",
    "import io\n",
    "import zipfile\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, ConcatDataset, Dataset, Subset\n",
    "from torchvision import models, transforms, datasets\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "class CompositeDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def base64_to_image(self, base64_string):\n",
    "        image_data = base64.b64decode(base64_string)\n",
    "        image = Image.open(io.BytesIO(image_data)).convert('L')\n",
    "        image_array = np.array(image, dtype=np.float32) / 255.0\n",
    "        return torch.from_numpy(image_array).unsqueeze(0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        image = self.base64_to_image(item['composite'])\n",
    "        label = torch.tensor(int(item['true_digit']), dtype=torch.long)\n",
    "        return image, label\n",
    "\n",
    "class SelectionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = [item for sublist in data for item in sublist]  # Flatten the list of lists\n",
    "\n",
    "    def base64_to_image(self, base64_string):\n",
    "        image_data = base64.b64decode(base64_string)\n",
    "        image = Image.open(io.BytesIO(image_data)).convert('L')\n",
    "        image_array = np.array(image, dtype=np.float32) / 255.0\n",
    "        return torch.from_numpy(image_array).unsqueeze(0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        image = self.base64_to_image(item['selected_image'])\n",
    "        label = torch.tensor(int(item['true_digit']), dtype=torch.long)\n",
    "        return image, label\n",
    "\n",
    "class ExperimentalDataset(Dataset):\n",
    "    def __init__(self, images, labels):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.images[idx], self.labels[idx]\n",
    "\n",
    "class WeightedCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self, class_weights):\n",
    "        super().__init__()\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        return F.cross_entropy(outputs, targets, weight=self.class_weights)\n",
    "\n",
    "def load_data(train_folder, test_folder, num_participants=100):\n",
    "    train_data = []\n",
    "    for file in os.listdir(train_folder)[:num_participants]:\n",
    "        with open(os.path.join(train_folder, file), 'r') as f:\n",
    "            train_data.extend(json.load(f))\n",
    "    \n",
    "    test_data = []\n",
    "    for file in os.listdir(test_folder)[:num_participants]:\n",
    "        with open(os.path.join(test_folder, file), 'r') as f:\n",
    "            test_data.extend(json.load(f))\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "def load_all_experimental_data(test_digits_folder):\n",
    "    train_images = []\n",
    "    train_labels = []\n",
    "    test_images = []\n",
    "    test_labels = []\n",
    "    participant_data = {}\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((16, 16)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "    for filename in os.listdir(test_digits_folder):\n",
    "        if filename.endswith('.zip') and filename.startswith('experiment_results_participant'):\n",
    "            participant_number = int(filename.split('participant')[1].split('.')[0])\n",
    "            zip_filepath = os.path.join(test_digits_folder, filename)\n",
    "\n",
    "            participant_train_images = []\n",
    "            participant_train_labels = []\n",
    "            participant_test_images = []\n",
    "            participant_test_labels = []\n",
    "\n",
    "            with zipfile.ZipFile(zip_filepath, 'r') as zip_ref:\n",
    "                for img_filename in zip_ref.namelist():\n",
    "                    if img_filename.endswith('.png'):\n",
    "                        with zip_ref.open(img_filename) as file:\n",
    "                            img = Image.open(file).convert('L')  # Convert to grayscale\n",
    "                            img_tensor = transform(img)\n",
    "                            \n",
    "                            digit = int(img_filename.split('_')[0])\n",
    "                            \n",
    "                            if 'composite' in img_filename:\n",
    "                                test_images.append(img_tensor)\n",
    "                                test_labels.append(digit)\n",
    "                                participant_test_images.append(img_tensor)\n",
    "                                participant_test_labels.append(digit)\n",
    "                            else:\n",
    "                                train_images.append(img_tensor)\n",
    "                                train_labels.append(digit)\n",
    "                                participant_train_images.append(img_tensor)\n",
    "                                participant_train_labels.append(digit)\n",
    "\n",
    "            participant_data[participant_number] = {\n",
    "                'train': (torch.stack(participant_train_images), torch.tensor(participant_train_labels)),\n",
    "                'test': (torch.stack(participant_test_images), torch.tensor(participant_test_labels))\n",
    "            }\n",
    "\n",
    "    return (torch.stack(train_images), torch.tensor(train_labels), \n",
    "            torch.stack(test_images), torch.tensor(test_labels),\n",
    "            participant_data)\n",
    "\n",
    "def create_ensemble():\n",
    "    model = models.resnet50(pretrained=True)\n",
    "    model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "    model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "    return model\n",
    "\n",
    "def calculate_class_weights(dataset):\n",
    "    class_counts = torch.zeros(10)\n",
    "    for _, label in dataset:\n",
    "        class_counts[label] += 1\n",
    "    return 1.0 / (class_counts + 1e-5)\n",
    "\n",
    "def evaluate_model(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return 100 * correct / total\n",
    "\n",
    "def train_model(model, train_loaders, val_loaders, criterion, optimizer, scheduler, num_epochs, device, save_dir):\n",
    "    best_val_acc = 0\n",
    "    start_time = time.time()\n",
    "    last_save_time = start_time\n",
    "    \n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(\"-\" * 10)\n",
    "        \n",
    "        model.train()\n",
    "        for phase, loader in train_loaders.items():\n",
    "            running_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            progress_bar = tqdm(loader, desc=f\"Training on {phase}\")\n",
    "            for inputs, labels in progress_bar:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                \n",
    "                progress_bar.set_postfix({'Loss': loss.item(), 'Accuracy': 100 * correct / total})\n",
    "            \n",
    "            epoch_loss = running_loss / len(loader.dataset)\n",
    "            epoch_acc = 100 * correct / total\n",
    "            print(f\"{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.2f}%\")\n",
    "        \n",
    "        # Validation step\n",
    "        model.eval()\n",
    "        val_acc = {}\n",
    "        for phase, loader in val_loaders.items():\n",
    "            acc = evaluate_model(model, loader, device)\n",
    "            val_acc[phase] = acc\n",
    "            print(f\"Validation Accuracy ({phase}): {acc:.2f}%\")\n",
    "        \n",
    "        # Save the best model\n",
    "        if val_acc['composite'] > best_val_acc:\n",
    "            best_val_acc = val_acc['composite']\n",
    "            torch.save(model.state_dict(), os.path.join(save_dir, 'best_model.pth'))\n",
    "            print(f\"Saved new best model with validation accuracy: {best_val_acc:.2f}%\")\n",
    "        \n",
    "        # Periodic saving (every hour)\n",
    "        current_time = time.time()\n",
    "        if current_time - last_save_time > 3600:  # 3600 seconds = 1 hour\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'best_val_acc': best_val_acc,\n",
    "            }, os.path.join(save_dir, f'checkpoint_epoch_{epoch}.pth'))\n",
    "            print(f\"Saved periodic checkpoint at epoch {epoch}\")\n",
    "            last_save_time = current_time\n",
    "        \n",
    "        scheduler.step(val_acc['composite'])\n",
    "        \n",
    "        # Check if we've reached 36 hours\n",
    "        if current_time - start_time > 36 * 3600:\n",
    "            print(\"Reached 36 hours of training. Stopping.\")\n",
    "            break\n",
    "    \n",
    "    # Save final model\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'best_val_acc': best_val_acc,\n",
    "    }, os.path.join(save_dir, 'final_model.pth'))\n",
    "    print(\"Saved final model\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Load all datasets\n",
    "    train_data, test_data = load_data('training_data/training_set', 'training_data/test_set', num_participants=100)\n",
    "    exp_train_images, exp_train_labels, exp_test_images, exp_test_labels, _ = load_all_experimental_data('test_digits')\n",
    "    \n",
    "    # Create datasets\n",
    "    train_composite_dataset = CompositeDataset(test_data)\n",
    "    train_selection_dataset = SelectionDataset(train_data)\n",
    "    exp_train_dataset = ExperimentalDataset(exp_train_images, exp_train_labels)\n",
    "    exp_test_dataset = ExperimentalDataset(exp_test_images, exp_test_labels)\n",
    "    \n",
    "    # Create inverted MNIST dataset\n",
    "    mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,)),\n",
    "        transforms.Lambda(lambda x: 1 - x)  # Invert colors\n",
    "    ]))\n",
    "    mnist_test = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,)),\n",
    "        transforms.Lambda(lambda x: 1 - x)  # Invert colors\n",
    "    ]))\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loaders = {\n",
    "        'composite': DataLoader(train_composite_dataset, batch_size=64, shuffle=True, num_workers=4),\n",
    "        'selection': DataLoader(train_selection_dataset, batch_size=64, shuffle=True, num_workers=4),\n",
    "        'experimental': DataLoader(exp_train_dataset, batch_size=64, shuffle=True, num_workers=4),\n",
    "        'mnist': DataLoader(mnist_train, batch_size=64, shuffle=True, num_workers=4)\n",
    "    }\n",
    "    \n",
    "    val_loaders = {\n",
    "        'composite': DataLoader(CompositeDataset(test_data[-1000:]), batch_size=64, shuffle=False, num_workers=4),\n",
    "        'experimental': DataLoader(exp_test_dataset, batch_size=64, shuffle=False, num_workers=4),\n",
    "        'mnist': DataLoader(mnist_test, batch_size=64, shuffle=False, num_workers=4)\n",
    "    }\n",
    "\n",
    "    model = create_ensemble().to(device)\n",
    "\n",
    "    combined_dataset = ConcatDataset([train_composite_dataset, train_selection_dataset, exp_train_dataset, mnist_train])\n",
    "    class_weights = calculate_class_weights(combined_dataset).to(device)\n",
    "\n",
    "    criterion = WeightedCrossEntropyLoss(class_weights)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=5, verbose=True)\n",
    "\n",
    "    save_dir = 'model_36_hours'\n",
    "    trained_model = train_model(model, train_loaders, val_loaders, criterion, optimizer, scheduler, num_epochs=1000, device=device, save_dir=save_dir)\n",
    "\n",
    "    print(\"Training completed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ran for 3200 and was just frozen, didn't do much of anything. Making a 4 hour version instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import base64\n",
    "import io\n",
    "import zipfile\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, ConcatDataset, Dataset, Subset\n",
    "from torchvision import models, transforms, datasets\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompositeDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def base64_to_image(self, base64_string):\n",
    "        image_data = base64.b64decode(base64_string)\n",
    "        image = Image.open(io.BytesIO(image_data)).convert('L')\n",
    "        image_array = np.array(image, dtype=np.float32) / 255.0\n",
    "        return torch.from_numpy(image_array).unsqueeze(0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        image = self.base64_to_image(item['composite'])\n",
    "        label = torch.tensor(int(item['true_digit']), dtype=torch.long)\n",
    "        return image, label\n",
    "\n",
    "class SelectionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = [item for sublist in data for item in sublist]  # Flatten the list of lists\n",
    "\n",
    "    def base64_to_image(self, base64_string):\n",
    "        image_data = base64.b64decode(base64_string)\n",
    "        image = Image.open(io.BytesIO(image_data)).convert('L')\n",
    "        image_array = np.array(image, dtype=np.float32) / 255.0\n",
    "        return torch.from_numpy(image_array).unsqueeze(0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        image = self.base64_to_image(item['selected_image'])\n",
    "        label = torch.tensor(int(item['true_digit']), dtype=torch.long)\n",
    "        return image, label\n",
    "\n",
    "class ExperimentalDataset(Dataset):\n",
    "    def __init__(self, images, labels):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.images[idx], self.labels[idx]\n",
    "\n",
    "class WeightedCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self, class_weights):\n",
    "        super().__init__()\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        return F.cross_entropy(outputs, targets, weight=self.class_weights)\n",
    "\n",
    "def load_data(train_folder, test_folder, num_participants=100):\n",
    "    train_data = []\n",
    "    for file in os.listdir(train_folder)[:num_participants]:\n",
    "        with open(os.path.join(train_folder, file), 'r') as f:\n",
    "            train_data.extend(json.load(f))\n",
    "    \n",
    "    test_data = []\n",
    "    for file in os.listdir(test_folder)[:num_participants]:\n",
    "        with open(os.path.join(test_folder, file), 'r') as f:\n",
    "            test_data.extend(json.load(f))\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "def load_all_experimental_data(test_digits_folder):\n",
    "    train_images = []\n",
    "    train_labels = []\n",
    "    test_images = []\n",
    "    test_labels = []\n",
    "    participant_data = {}\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((16, 16)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "    for filename in os.listdir(test_digits_folder):\n",
    "        if filename.endswith('.zip') and filename.startswith('experiment_results_participant'):\n",
    "            participant_number = int(filename.split('participant')[1].split('.')[0])\n",
    "            zip_filepath = os.path.join(test_digits_folder, filename)\n",
    "\n",
    "            participant_train_images = []\n",
    "            participant_train_labels = []\n",
    "            participant_test_images = []\n",
    "            participant_test_labels = []\n",
    "\n",
    "            with zipfile.ZipFile(zip_filepath, 'r') as zip_ref:\n",
    "                for img_filename in zip_ref.namelist():\n",
    "                    if img_filename.endswith('.png'):\n",
    "                        with zip_ref.open(img_filename) as file:\n",
    "                            img = Image.open(file).convert('L')  # Convert to grayscale\n",
    "                            img_tensor = transform(img)\n",
    "                            \n",
    "                            digit = int(img_filename.split('_')[0])\n",
    "                            \n",
    "                            if 'composite' in img_filename:\n",
    "                                test_images.append(img_tensor)\n",
    "                                test_labels.append(digit)\n",
    "                                participant_test_images.append(img_tensor)\n",
    "                                participant_test_labels.append(digit)\n",
    "                            else:\n",
    "                                train_images.append(img_tensor)\n",
    "                                train_labels.append(digit)\n",
    "                                participant_train_images.append(img_tensor)\n",
    "                                participant_train_labels.append(digit)\n",
    "\n",
    "            participant_data[participant_number] = {\n",
    "                'train': (torch.stack(participant_train_images), torch.tensor(participant_train_labels)),\n",
    "                'test': (torch.stack(participant_test_images), torch.tensor(participant_test_labels))\n",
    "            }\n",
    "\n",
    "    return (torch.stack(train_images), torch.tensor(train_labels), \n",
    "            torch.stack(test_images), torch.tensor(test_labels),\n",
    "            participant_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loaders, val_loaders, criterion, optimizer, scheduler, num_epochs, device, save_dir):\n",
    "    best_val_acc = 0\n",
    "    start_time = time.time()\n",
    "    last_save_time = start_time\n",
    "\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        print(\"-\" * 10)\n",
    "\n",
    "        model.train()\n",
    "        for phase, loader in train_loaders.items():\n",
    "            running_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for inputs, labels in tqdm(loader, desc=f\"Training on {phase}\"):\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "            epoch_loss = running_loss / len(loader.dataset)\n",
    "            epoch_acc = 100 * correct / total\n",
    "            print(f\"{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.2f}%\")\n",
    "\n",
    "        # Validation step\n",
    "        model.eval()\n",
    "        val_acc = {}\n",
    "        for phase, loader in val_loaders.items():\n",
    "            acc = evaluate_model(model, loader, device)\n",
    "            val_acc[phase] = acc\n",
    "            print(f\"Validation Accuracy ({phase}): {acc:.2f}%\")\n",
    "\n",
    "        # Save the best model\n",
    "        if val_acc['composite'] > best_val_acc:\n",
    "            best_val_acc = val_acc['composite']\n",
    "            torch.save(model.state_dict(), os.path.join(save_dir, 'best_model.pth'))\n",
    "            print(f\"Saved new best model with validation accuracy: {best_val_acc:.2f}%\")\n",
    "\n",
    "        # Periodic saving (every hour)\n",
    "        current_time = time.time()\n",
    "        if current_time - last_save_time > 3600:  # 3600 seconds = 1 hour\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'best_val_acc': best_val_acc,\n",
    "            }, os.path.join(save_dir, f'checkpoint_epoch_{epoch}.pth'))\n",
    "            print(f\"Saved periodic checkpoint at epoch {epoch}\")\n",
    "            last_save_time = current_time\n",
    "\n",
    "        scheduler.step(val_acc['composite'])\n",
    "\n",
    "        # Print time elapsed\n",
    "        time_elapsed = time.time() - start_time\n",
    "        print(f\"Time elapsed: {time_elapsed / 3600:.2f} hours\")\n",
    "\n",
    "    # Save final model\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'best_val_acc': best_val_acc,\n",
    "    }, os.path.join(save_dir, 'final_model.pth'))\n",
    "    print(\"Saved final model\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_class_weights(dataset):\n",
    "    class_counts = torch.zeros(10)\n",
    "    for _, label in dataset:\n",
    "        class_counts[label] += 1\n",
    "    return 1.0 / (class_counts + 1e-5)\n",
    "\n",
    "def evaluate_model(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return 100 * correct / total\n",
    "\n",
    "def create_ensemble():\n",
    "    model = models.resnet50(pretrained=True)\n",
    "    model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "    model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "class CompositeDataset(Dataset):\n",
    "    def __init__(self, data, max_samples=None, transform=None):\n",
    "        self.data = data[:max_samples] if max_samples else data\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.Resize((16, 16)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        image = self.load_image(item['composite'])\n",
    "        label = torch.tensor(int(item['true_digit']), dtype=torch.long)\n",
    "        return image, label\n",
    "\n",
    "    def load_image(self, base64_string):\n",
    "        image_data = base64.b64decode(base64_string)\n",
    "        image = Image.open(io.BytesIO(image_data)).convert('L')\n",
    "        image = self.transform(image)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sai\\Desktop\\NN_digit_classifier\\Southampton_NN_digit_classifier\\env\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sai\\Desktop\\NN_digit_classifier\\Southampton_NN_digit_classifier\\env\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\Sai\\Desktop\\NN_digit_classifier\\Southampton_NN_digit_classifier\\env\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/50\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training on composite:   0%|          | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Load a subset of the data\n",
    "    train_data, test_data = load_data('training_data/training_set', 'training_data/test_set', num_participants=20)  # Reduced from 100 to 20\n",
    "    exp_train_images, exp_train_labels, exp_test_images, exp_test_labels, _ = load_all_experimental_data('test_digits')\n",
    "\n",
    "    # Create datasets\n",
    "    #train_composite_dataset = CompositeDataset(test_data[:1000])  # Use only the first 1000 samples\n",
    "    max_samples = 1000  # Adjust this number based on your available memory and desired dataset size\n",
    "    train_composite_dataset = CompositeDataset(test_data, max_samples=max_samples)\n",
    "    train_selection_dataset = SelectionDataset(train_data[:1000])  # Use only the first 1000 samples\n",
    "    exp_train_dataset = ExperimentalDataset(exp_train_images[:1000], exp_train_labels[:1000])  # Use only the first 1000 samples\n",
    "\n",
    "    # Create inverted MNIST dataset (use a subset)\n",
    "    mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,)),\n",
    "        transforms.Lambda(lambda x: 1 - x)  # Invert colors\n",
    "    ]))\n",
    "    mnist_train_subset = Subset(mnist_train, range(1000))  # Use only the first 1000 samples\n",
    "\n",
    "    mnist_test = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,)),\n",
    "        transforms.Lambda(lambda x: 1 - x)  # Invert colors\n",
    "    ]))\n",
    "    mnist_test_subset = Subset(mnist_test, range(1000))  # Use only the first 1000 samples\n",
    "\n",
    "    # Create data loaders with smaller batch size and fewer workers\n",
    "    train_loaders = {\n",
    "        'composite': DataLoader(train_composite_dataset, batch_size=32, shuffle=True, num_workers=2),\n",
    "        'selection': DataLoader(train_selection_dataset, batch_size=32, shuffle=True, num_workers=2),\n",
    "        'experimental': DataLoader(exp_train_dataset, batch_size=32, shuffle=True, num_workers=2),\n",
    "        'mnist': DataLoader(mnist_train_subset, batch_size=32, shuffle=True, num_workers=2)\n",
    "    }\n",
    "    val_loaders = {\n",
    "        'composite': DataLoader(CompositeDataset(test_data[-500:]), batch_size=32, shuffle=False, num_workers=2),\n",
    "        'experimental': DataLoader(ExperimentalDataset(exp_test_images[:500], exp_test_labels[:500]), batch_size=32, shuffle=False, num_workers=2),\n",
    "        'mnist': DataLoader(mnist_test_subset, batch_size=32, shuffle=False, num_workers=2)\n",
    "    }\n",
    "\n",
    "    # Use a smaller ResNet model\n",
    "    model = models.resnet18(pretrained=True)\n",
    "    model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "    model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "    model = model.to(device)\n",
    "\n",
    "    combined_dataset = ConcatDataset([train_composite_dataset, train_selection_dataset, exp_train_dataset, mnist_train_subset])\n",
    "    class_weights = calculate_class_weights(combined_dataset).to(device)\n",
    "    criterion = WeightedCrossEntropyLoss(class_weights)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=5, verbose=True)\n",
    "\n",
    "    save_dir = 'model_checkpoints'\n",
    "    trained_model = train_model(model, train_loaders, val_loaders, criterion, optimizer, scheduler, num_epochs=50, device=device, save_dir=save_dir)\n",
    "    print(\"Training completed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this just keeps getting stuck without actually running anything or making me anything which absolutely sucks. What we are going to do is try to run a different model without as much data at the same time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import base64\n",
    "import io\n",
    "import zipfile\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, ConcatDataset, Dataset, Subset\n",
    "from torchvision import models, transforms, datasets\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(x, noise_factor=0.1):\n",
    "    noisy = x + torch.randn_like(x) * noise_factor\n",
    "    return torch.clamp(noisy, 0., 1.)\n",
    "\n",
    "def invert_colors(x):\n",
    "    return 1 - x\n",
    "\n",
    "def visualize_datasets(mnist_train, mnist_noisy_train, exp_train_dataset, num_samples=5):\n",
    "    fig, axes = plt.subplots(3, num_samples, figsize=(15, 9))\n",
    "    \n",
    "    datasets = [\n",
    "        (\"MNIST (Inverted)\", mnist_train),\n",
    "        (\"MNIST Noisy (Inverted)\", mnist_noisy_train),\n",
    "        (\"Experimental\", exp_train_dataset)\n",
    "    ]\n",
    "    \n",
    "    for i, (name, dataset) in enumerate(datasets):\n",
    "        for j in range(num_samples):\n",
    "            if name == \"Experimental\":\n",
    "                image, label = dataset[j]\n",
    "            else:\n",
    "                image, label = dataset[j]\n",
    "                image = invert_colors(image) if \"Inverted\" in name else image\n",
    "                if \"Noisy\" in name:\n",
    "                    image = add_noise(image)\n",
    "            \n",
    "            ax = axes[i][j]\n",
    "            ax.imshow(image.squeeze(), cmap='gray')\n",
    "            ax.set_title(f\"{name}: {label}\")\n",
    "            ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('dataset_samples.png')\n",
    "    plt.close()\n",
    "    print(\"Dataset samples saved as 'dataset_samples.png'\")\n",
    "\n",
    "def create_model():\n",
    "    model = models.resnet50(weights=None)\n",
    "    model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "    model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "    return model\n",
    "\n",
    "def train_model(model, train_loaders, val_loaders, criterion, optimizer, scheduler, num_epochs, device, save_dir):\n",
    "    best_val_acc = 0\n",
    "    start_time = time.time()\n",
    "    last_save_time = start_time\n",
    "\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        print(\"-\" * 10)\n",
    "\n",
    "        model.train()\n",
    "        for phase, loader in train_loaders.items():\n",
    "            running_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            progress_bar = tqdm(loader, desc=f\"Training on {phase}\")\n",
    "            for batch_idx, (inputs, labels) in enumerate(progress_bar):\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                \n",
    "                if batch_idx % 10 == 0:  # Update every 10 batches\n",
    "                    progress_bar.set_postfix({\n",
    "                        'Loss': loss.item(),\n",
    "                        'Accuracy': 100 * correct / total,\n",
    "                        'LR': optimizer.param_groups[0]['lr']\n",
    "                    })\n",
    "\n",
    "            epoch_loss = running_loss / len(loader.dataset)\n",
    "            epoch_acc = 100 * correct / total\n",
    "            print(f\"{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.2f}%\")\n",
    "\n",
    "        # Validation step\n",
    "        model.eval()\n",
    "        val_acc = {}\n",
    "        for phase, loader in val_loaders.items():\n",
    "            acc = evaluate_model(model, loader, device)\n",
    "            val_acc[phase] = acc\n",
    "            print(f\"Validation Accuracy ({phase}): {acc:.2f}%\")\n",
    "\n",
    "        # Save the best model\n",
    "        if val_acc['mnist'] > best_val_acc:\n",
    "            best_val_acc = val_acc['mnist']\n",
    "            torch.save(model.state_dict(), os.path.join(save_dir, 'best_model.pth'))\n",
    "            print(f\"Saved new best model with validation accuracy: {best_val_acc:.2f}%\")\n",
    "\n",
    "        # Periodic saving (every hour)\n",
    "        current_time = time.time()\n",
    "        if current_time - last_save_time > 3600:  # 3600 seconds = 1 hour\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'best_val_acc': best_val_acc,\n",
    "            }, os.path.join(save_dir, f'checkpoint_epoch_{epoch}.pth'))\n",
    "            print(f\"Saved periodic checkpoint at epoch {epoch}\")\n",
    "            last_save_time = current_time\n",
    "\n",
    "        scheduler.step(val_acc['mnist'])\n",
    "\n",
    "        # Print time elapsed\n",
    "        time_elapsed = (time.time() - start_time) / 3600  # in hours\n",
    "        print(f\"Time elapsed: {time_elapsed:.2f} hours\")\n",
    "\n",
    "    # Save final model\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'best_val_acc': best_val_acc,\n",
    "    }, os.path.join(save_dir, 'final_model.pth'))\n",
    "    print(\"Saved final model\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "def add_noise(image, noise_factor=0.5):\n",
    "    # Convert image to range [0, 1]\n",
    "    image = image * 0.3081 + 0.1307  # Reverse the normalization\n",
    "    \n",
    "    # Add noise\n",
    "    noise = torch.randn(image.size()) * noise_factor\n",
    "    noisy_image = image + noise\n",
    "    \n",
    "    # Clip to ensure values are in [0, 1]\n",
    "    noisy_image = torch.clamp(noisy_image, 0., 1.)\n",
    "    \n",
    "    # Renormalize\n",
    "    noisy_image = (noisy_image - 0.1307) / 0.3081\n",
    "    \n",
    "    return noisy_image\n",
    "\n",
    "class NoisyMNISTDataset(Dataset):\n",
    "    def __init__(self, mnist_dataset, noise_factor=0.5):\n",
    "        self.mnist_dataset = mnist_dataset\n",
    "        self.noise_factor = noise_factor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mnist_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.mnist_dataset[idx]\n",
    "        noisy_image = add_noise(image, self.noise_factor)\n",
    "        return noisy_image, label\n",
    "\n",
    "def load_noisy_mnist_data(train=True, noise_factor=0.5):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((16, 16)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,)),\n",
    "        transforms.Lambda(lambda x: 1 - x)  # Invert colors\n",
    "    ])\n",
    "    mnist_dataset = datasets.MNIST(root='./data', train=train, download=True, transform=transform)\n",
    "    noisy_dataset = NoisyMNISTDataset(mnist_dataset, noise_factor)\n",
    "    return noisy_dataset\n",
    "\n",
    "def visualize_datasets(mnist_train, mnist_noisy_train, exp_train_dataset, num_samples=5):\n",
    "    fig, axes = plt.subplots(3, num_samples, figsize=(15, 9))\n",
    "    \n",
    "    datasets = [\n",
    "        (\"MNIST\", mnist_train),\n",
    "        (\"MNIST Noisy\", mnist_noisy_train),\n",
    "        (\"Experimental\", exp_train_dataset)\n",
    "    ]\n",
    "    \n",
    "    for i, (name, dataset) in enumerate(datasets):\n",
    "        for j in range(num_samples):\n",
    "            image, label = dataset[j]\n",
    "            \n",
    "            # Denormalize the image for visualization\n",
    "            image = image * 0.3081 + 0.1307\n",
    "            \n",
    "            ax = axes[i][j]\n",
    "            ax.imshow(image.squeeze(), cmap='gray', vmin=0, vmax=1)\n",
    "            ax.set_title(f\"{name}: {label}\")\n",
    "            ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('dataset_samples.png')\n",
    "    plt.close()\n",
    "    print(\"Dataset samples saved as 'dataset_samples.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Dataset samples saved as 'dataset_samples.png'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 65\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining completed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[24], line 52\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     49\u001b[0m model \u001b[38;5;241m=\u001b[39m create_model()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     51\u001b[0m combined_dataset \u001b[38;5;241m=\u001b[39m ConcatDataset([mnist_inverted_train, mnist_noisy_train, ExperimentalDataset(exp_train_images, exp_train_labels)])\n\u001b[1;32m---> 52\u001b[0m class_weights \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_class_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined_dataset\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     53\u001b[0m criterion \u001b[38;5;241m=\u001b[39m WeightedCrossEntropyLoss(class_weights)\n\u001b[0;32m     54\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m, in \u001b[0;36mcalculate_class_weights\u001b[1;34m(dataset)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_class_weights\u001b[39m(dataset):\n\u001b[0;32m      2\u001b[0m     class_counts \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, label \u001b[38;5;129;01min\u001b[39;00m dataset:\n\u001b[0;32m      4\u001b[0m         class_counts[label] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m (class_counts \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-5\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Sai\\Desktop\\NN_digit_classifier\\Southampton_NN_digit_classifier\\env\\lib\\site-packages\\torch\\utils\\data\\dataset.py:350\u001b[0m, in \u001b[0;36mConcatDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    349\u001b[0m     sample_idx \u001b[38;5;241m=\u001b[39m idx \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcumulative_sizes[dataset_idx \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m--> 350\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdataset_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43msample_idx\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Sai\\Desktop\\NN_digit_classifier\\Southampton_NN_digit_classifier\\env\\lib\\site-packages\\torchvision\\datasets\\mnist.py:146\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    143\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 146\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    149\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32mc:\\Users\\Sai\\Desktop\\NN_digit_classifier\\Southampton_NN_digit_classifier\\env\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m t(img)\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Load experimental data\n",
    "    exp_train_images, exp_train_labels, exp_test_images, exp_test_labels, _ = load_all_experimental_data('test_digits')\n",
    "\n",
    "    # Create MNIST dataset\n",
    "    mnist_transform = transforms.Compose([\n",
    "        transforms.Resize((16, 16)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=mnist_transform)\n",
    "    mnist_test = datasets.MNIST(root='./data', train=False, download=True, transform=mnist_transform)\n",
    "\n",
    "    # Create noisy MNIST dataset\n",
    "    mnist_noisy_train = NoisyMNISTDataset(mnist_train, noise_factor=0.5)\n",
    "    mnist_noisy_test = NoisyMNISTDataset(mnist_test, noise_factor=0.5)\n",
    "\n",
    "    # Create inverted MNIST dataset\n",
    "    invert_transform = transforms.Compose([\n",
    "        transforms.Resize((16, 16)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,)),\n",
    "        transforms.Lambda(lambda x: 1 - x)  # Invert colors\n",
    "    ])\n",
    "    mnist_inverted_train = datasets.MNIST(root='./data', train=True, download=True, transform=invert_transform)\n",
    "    mnist_inverted_test = datasets.MNIST(root='./data', train=False, download=True, transform=invert_transform)\n",
    "\n",
    "    # Visualize samples from each dataset\n",
    "    visualize_datasets(mnist_inverted_train, mnist_noisy_train, ExperimentalDataset(exp_train_images, exp_train_labels))\n",
    "\n",
    "    # Wait for user input before proceeding\n",
    "    input(\"Press Enter to start training after reviewing the dataset samples...\")\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loaders = {\n",
    "        'mnist_inverted': DataLoader(mnist_inverted_train, batch_size=64, shuffle=True, num_workers=4),\n",
    "        'mnist_noisy': DataLoader(mnist_noisy_train, batch_size=64, shuffle=True, num_workers=4),\n",
    "        'experimental': DataLoader(ExperimentalDataset(exp_train_images, exp_train_labels), batch_size=64, shuffle=True, num_workers=4)\n",
    "    }\n",
    "    val_loaders = {\n",
    "        'mnist_inverted': DataLoader(mnist_inverted_test, batch_size=64, shuffle=False, num_workers=4),\n",
    "        'mnist_noisy': DataLoader(mnist_noisy_test, batch_size=64, shuffle=False, num_workers=4),\n",
    "        'experimental': DataLoader(ExperimentalDataset(exp_test_images, exp_test_labels), batch_size=64, shuffle=False, num_workers=4)\n",
    "    }\n",
    "\n",
    "    model = create_model().to(device)\n",
    "\n",
    "    combined_dataset = ConcatDataset([mnist_inverted_train, mnist_noisy_train, ExperimentalDataset(exp_train_images, exp_train_labels)])\n",
    "    class_weights = calculate_class_weights(combined_dataset).to(device)\n",
    "    criterion = WeightedCrossEntropyLoss(class_weights)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=5, verbose=True)\n",
    "\n",
    "    save_dir = 'model_50_epochs'\n",
    "    num_epochs = 50\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    trained_model = train_model(model, train_loaders, val_loaders, criterion, optimizer, scheduler, num_epochs, device, save_dir)\n",
    "    print(\"Training completed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x600 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "\n",
    "def add_background_noise(image, noise_factor=0.5):\n",
    "    # Invert the image (black digits on white background)\n",
    "    image = 1 - image\n",
    "    \n",
    "    # Create a noise mask (1 for background, 0 for digit)\n",
    "    mask = (image > 0.7).float()\n",
    "    \n",
    "    # Generate noise\n",
    "    noise = torch.randn_like(image) * noise_factor\n",
    "    \n",
    "    # Apply noise only to the background\n",
    "    noisy_image = image + noise * mask\n",
    "    \n",
    "    # Clip to ensure values are in [0, 1]\n",
    "    noisy_image = torch.clamp(noisy_image, 0., 1.)\n",
    "    \n",
    "    return noisy_image\n",
    "\n",
    "def visualize_noisy_mnist(num_samples=5):\n",
    "    # Load MNIST dataset\n",
    "    mnist_train = torchvision.datasets.MNIST(root='./data', train=True, download=True, \n",
    "                                             transform=transforms.ToTensor())\n",
    "    \n",
    "    # Create a figure\n",
    "    fig, axes = plt.subplots(2, num_samples, figsize=(15, 6))\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Get a sample image\n",
    "        image, label = mnist_train[i]\n",
    "        \n",
    "        # Add noise to the image\n",
    "        noisy_image = add_background_noise(image)\n",
    "        \n",
    "        # Display original image (inverted)\n",
    "        axes[0, i].imshow(1 - image.squeeze(), cmap='gray')\n",
    "        axes[0, i].set_title(f\"Original: {label}\")\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        # Display noisy image\n",
    "        axes[1, i].imshow(1 - noisy_image.squeeze(), cmap='gray')\n",
    "        axes[1, i].set_title(f\"Noisy: {label}\")\n",
    "        axes[1, i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run the visualization\n",
    "visualize_noisy_mnist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
