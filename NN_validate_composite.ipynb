{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x800 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of extracted composite images: 10\n",
      "Extracted digits: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Specify the path to your test_digits folder and the zip file\n",
    "test_digits_folder = 'test_digits'\n",
    "zip_filename = 'experiment_results_participant65.zip'\n",
    "zip_filepath = os.path.join(test_digits_folder, zip_filename)\n",
    "\n",
    "# Create a dictionary to store the extracted images\n",
    "extracted_images = {}\n",
    "\n",
    "# Open the zip file\n",
    "with zipfile.ZipFile(zip_filepath, 'r') as zip_ref:\n",
    "    # List all files in the zip\n",
    "    file_list = zip_ref.namelist()\n",
    "    \n",
    "    # Filter for composite images\n",
    "    composite_images = [f for f in file_list if f.endswith('_composite_participant65.png')]\n",
    "    \n",
    "    # Extract and store the composite images\n",
    "    for image_name in composite_images:\n",
    "        with zip_ref.open(image_name) as file:\n",
    "            img = Image.open(file)\n",
    "            img_array = np.array(img)\n",
    "            \n",
    "            # Extract digit (ground truth) from filename\n",
    "            digit = int(image_name.split('_')[0])\n",
    "            \n",
    "            extracted_images[digit] = img_array\n",
    "\n",
    "# Display the extracted images\n",
    "plt.figure(figsize=(15, 8))\n",
    "for i, (digit, img_array) in enumerate(sorted(extracted_images.items())):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(img_array, cmap='gray')\n",
    "    plt.title(f\"Digit: {digit}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print some statistics\n",
    "print(f\"Number of extracted composite images: {len(extracted_images)}\")\n",
    "print(\"Extracted digits:\", sorted(extracted_images.keys()))\n",
    "\n",
    "# You can now use the extracted_images dictionary for further analysis\n",
    "# For example, to access the image array for digit 5:\n",
    "# digit_5_array = extracted_images[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "object __array__ method not producing an array",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Load the pre-trained model\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmnist_noisy_16x16_classifier.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Function to preprocess image for the model\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_image\u001b[39m(img_array):\n",
      "File \u001b[1;32mc:\\Users\\saith\\Desktop\\NN_Classifier\\Southampton_NN_digit_classifier\\env\\Lib\\site-packages\\keras\\src\\saving\\saving_api.py:194\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[0;32m    188\u001b[0m         filepath,\n\u001b[0;32m    189\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[0;32m    190\u001b[0m         \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m,\n\u001b[0;32m    191\u001b[0m         safe_mode\u001b[38;5;241m=\u001b[39msafe_mode,\n\u001b[0;32m    192\u001b[0m     )\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.hdf5\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m--> 194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlegacy_h5_format\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model_from_hdf5\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcompile\u001b[39;49m\n\u001b[0;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    200\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure the file is an accessible `.keras` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    201\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzip file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    202\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\saith\\Desktop\\NN_Classifier\\Southampton_NN_digit_classifier\\env\\Lib\\site-packages\\keras\\src\\legacy\\saving\\legacy_h5_format.py:138\u001b[0m, in \u001b[0;36mload_model_from_hdf5\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    133\u001b[0m     model \u001b[38;5;241m=\u001b[39m saving_utils\u001b[38;5;241m.\u001b[39mmodel_from_config(\n\u001b[0;32m    134\u001b[0m         model_config, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects\n\u001b[0;32m    135\u001b[0m     )\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;66;03m# set weights\u001b[39;00m\n\u001b[1;32m--> 138\u001b[0m     \u001b[43mload_weights_from_hdf5_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_weights\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcompile\u001b[39m:\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;66;03m# instantiate optimizer\u001b[39;00m\n\u001b[0;32m    142\u001b[0m     training_config \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mattrs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_config\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\saith\\Desktop\\NN_Classifier\\Southampton_NN_digit_classifier\\env\\Lib\\site-packages\\keras\\src\\legacy\\saving\\legacy_h5_format.py:376\u001b[0m, in \u001b[0;36mload_weights_from_hdf5_group\u001b[1;34m(f, model)\u001b[0m\n\u001b[0;32m    369\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    370\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeight count mismatch for layer #\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    371\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe current model, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in the save file). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    372\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayer expects \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(symbolic_weights)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m weight(s). Received \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    373\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(weight_values)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m saved weight(s)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    374\u001b[0m         )\n\u001b[0;32m    375\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ref_v, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(symbolic_weights, weight_values):\n\u001b[1;32m--> 376\u001b[0m         \u001b[43mref_v\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massign\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_level_model_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[0;32m    379\u001b[0m     symbolic_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[0;32m    380\u001b[0m         \u001b[38;5;66;03m# model.weights\u001b[39;00m\n\u001b[0;32m    381\u001b[0m         v\n\u001b[0;32m    382\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39m_trainable_variables \u001b[38;5;241m+\u001b[39m model\u001b[38;5;241m.\u001b[39m_non_trainable_variables\n\u001b[0;32m    383\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mweights\n\u001b[0;32m    384\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\saith\\Desktop\\NN_Classifier\\Southampton_NN_digit_classifier\\env\\Lib\\site-packages\\keras\\src\\backend\\common\\variables.py:224\u001b[0m, in \u001b[0;36mKerasVariable.assign\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21massign\u001b[39m(\u001b[38;5;28mself\u001b[39m, value):\n\u001b[1;32m--> 224\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m shape_equal(value\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape):\n\u001b[0;32m    226\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    227\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe shape of the target variable and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    228\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe shape of the target value in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    232\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget variable: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    233\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\saith\\Desktop\\NN_Classifier\\Southampton_NN_digit_classifier\\env\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:66\u001b[0m, in \u001b[0;36mVariable._convert_to_tensor\u001b[1;34m(self, value, dtype)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_convert_to_tensor\u001b[39m(\u001b[38;5;28mself\u001b[39m, value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\saith\\Desktop\\NN_Classifier\\Southampton_NN_digit_classifier\\env\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:125\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[1;34m(x, dtype, sparse)\u001b[0m\n\u001b[0;32m    123\u001b[0m         x \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(x)\n\u001b[0;32m    124\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mcast(x, dtype)\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m dtype:\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, tf\u001b[38;5;241m.\u001b[39mSparseTensor):\n",
      "File \u001b[1;32mc:\\Users\\saith\\Desktop\\NN_Classifier\\Southampton_NN_digit_classifier\\env\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\saith\\Desktop\\NN_Classifier\\Southampton_NN_digit_classifier\\env\\Lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:108\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    106\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[0;32m    107\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: object __array__ method not producing an array"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import load_model\n",
    "from PIL import Image\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = load_model('mnist_noisy_16x16_classifier.h5')\n",
    "\n",
    "# Function to preprocess image for the model\n",
    "def preprocess_image(img_array):\n",
    "    print(f\"Original image shape: {img_array.shape}\")\n",
    "    print(f\"Image data type: {img_array.dtype}\")\n",
    "    \n",
    "    # Ensure the image is 2D (grayscale)\n",
    "    if len(img_array.shape) == 3:\n",
    "        img_array = np.mean(img_array, axis=-1)\n",
    "    \n",
    "    # Ensure the image is 16x16\n",
    "    if img_array.shape != (16, 16):\n",
    "        raise ValueError(f\"Unexpected image shape: {img_array.shape}\")\n",
    "    \n",
    "    # Reshape and normalize\n",
    "    img_array = img_array.reshape(1, 16, 16, 1)\n",
    "    img_array = img_array.astype('float32') / 255.0\n",
    "    return img_array\n",
    "\n",
    "# Path to the zip file\n",
    "zip_filepath = 'test_digits/experiment_results_participant65.zip'\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Open the zip file and process images\n",
    "with zipfile.ZipFile(zip_filepath, 'r') as zip_ref:\n",
    "    for filename in zip_ref.namelist():\n",
    "        if filename.endswith('_composite_participant65.png'):\n",
    "            true_label = int(filename.split('_')[0])\n",
    "            with zip_ref.open(filename) as file:\n",
    "                img = Image.open(file)\n",
    "                img_array = np.array(img)\n",
    "                \n",
    "                print(f\"\\nProcessing image: {filename}\")\n",
    "                try:\n",
    "                    processed_img = preprocess_image(img_array)\n",
    "                    \n",
    "                    # Get model prediction\n",
    "                    prediction = model.predict(processed_img)[0]\n",
    "                    predicted_label = np.argmax(prediction)\n",
    "                    confidence = prediction[predicted_label]\n",
    "                    \n",
    "                    results[true_label] = {\n",
    "                        'predicted_label': predicted_label,\n",
    "                        'confidence': confidence,\n",
    "                        'image': img_array\n",
    "                    }\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {filename}: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "# Display results\n",
    "plt.figure(figsize=(20, 10))\n",
    "for i, (true_label, data) in enumerate(sorted(results.items())):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(data['image'], cmap='gray')\n",
    "    plt.title(f\"True: {true_label}, Pred: {data['predicted_label']}\\nConf: {data['confidence']:.2f}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed results\n",
    "print(\"\\nDetailed Results:\")\n",
    "print(\"True Label | Predicted Label | Confidence\")\n",
    "print(\"-\" * 45)\n",
    "for true_label, data in sorted(results.items()):\n",
    "    print(f\"{true_label:^10} | {data['predicted_label']:^15} | {data['confidence']:^10.2f}\")\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = sum(1 for true_label, data in results.items() if true_label == data['predicted_label']) / len(results)\n",
    "print(f\"\\nOverall Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Calculate average confidence\n",
    "avg_confidence = sum(data['confidence'] for data in results.values()) / len(results)\n",
    "print(f\"Average Confidence: {avg_confidence:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's address the issue of improving the model's robustness. The current results show low accuracy and a bias towards predicting '2', which indicates that the model needs significant improvement. Here are several strategies we can employ to make the model more robust:\n",
    "\n",
    "Data Augmentation:\n",
    "Increase the variety of training data by applying random transformations.\n",
    "\n",
    "Noise Injection:\n",
    "Add various types of noise to the training images to make the model more resilient to noisy inputs.\n",
    "\n",
    "Ensemble Methods:\n",
    "Train multiple models and combine their predictions.\n",
    "\n",
    "Regularization:\n",
    "Apply techniques like dropout or L2 regularization to prevent overfitting.\n",
    "\n",
    "Architecture Modification:\n",
    "Experiment with different network architectures.\n",
    "\n",
    "Transfer Learning:\n",
    "Use a pre-trained model on a larger dataset and fine-tune it for your specific task.\n",
    "\n",
    "Balanced Dataset:\n",
    "Ensure your training data has a balanced representation of all classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "object __array__ method not producing an array",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Load the three models\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m models \u001b[38;5;241m=\u001b[39m [\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmnist_noisy_16x16_model_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m)]\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mensemble_predict\u001b[39m(image):\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# Ensure image is in the correct shape (1, 16, 16, 1)\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m image\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m1\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\saith\\Desktop\\NN_Classifier\\Southampton_NN_digit_classifier\\env\\Lib\\site-packages\\keras\\src\\saving\\saving_api.py:194\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[0;32m    188\u001b[0m         filepath,\n\u001b[0;32m    189\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[0;32m    190\u001b[0m         \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m,\n\u001b[0;32m    191\u001b[0m         safe_mode\u001b[38;5;241m=\u001b[39msafe_mode,\n\u001b[0;32m    192\u001b[0m     )\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.hdf5\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m--> 194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlegacy_h5_format\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model_from_hdf5\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcompile\u001b[39;49m\n\u001b[0;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    200\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure the file is an accessible `.keras` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    201\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzip file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    202\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\saith\\Desktop\\NN_Classifier\\Southampton_NN_digit_classifier\\env\\Lib\\site-packages\\keras\\src\\legacy\\saving\\legacy_h5_format.py:138\u001b[0m, in \u001b[0;36mload_model_from_hdf5\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    133\u001b[0m     model \u001b[38;5;241m=\u001b[39m saving_utils\u001b[38;5;241m.\u001b[39mmodel_from_config(\n\u001b[0;32m    134\u001b[0m         model_config, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects\n\u001b[0;32m    135\u001b[0m     )\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;66;03m# set weights\u001b[39;00m\n\u001b[1;32m--> 138\u001b[0m     \u001b[43mload_weights_from_hdf5_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_weights\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcompile\u001b[39m:\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;66;03m# instantiate optimizer\u001b[39;00m\n\u001b[0;32m    142\u001b[0m     training_config \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mattrs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_config\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\saith\\Desktop\\NN_Classifier\\Southampton_NN_digit_classifier\\env\\Lib\\site-packages\\keras\\src\\legacy\\saving\\legacy_h5_format.py:376\u001b[0m, in \u001b[0;36mload_weights_from_hdf5_group\u001b[1;34m(f, model)\u001b[0m\n\u001b[0;32m    369\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    370\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeight count mismatch for layer #\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    371\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe current model, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in the save file). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    372\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayer expects \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(symbolic_weights)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m weight(s). Received \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    373\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(weight_values)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m saved weight(s)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    374\u001b[0m         )\n\u001b[0;32m    375\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ref_v, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(symbolic_weights, weight_values):\n\u001b[1;32m--> 376\u001b[0m         \u001b[43mref_v\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massign\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_level_model_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[0;32m    379\u001b[0m     symbolic_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[0;32m    380\u001b[0m         \u001b[38;5;66;03m# model.weights\u001b[39;00m\n\u001b[0;32m    381\u001b[0m         v\n\u001b[0;32m    382\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39m_trainable_variables \u001b[38;5;241m+\u001b[39m model\u001b[38;5;241m.\u001b[39m_non_trainable_variables\n\u001b[0;32m    383\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mweights\n\u001b[0;32m    384\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\saith\\Desktop\\NN_Classifier\\Southampton_NN_digit_classifier\\env\\Lib\\site-packages\\keras\\src\\backend\\common\\variables.py:224\u001b[0m, in \u001b[0;36mKerasVariable.assign\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21massign\u001b[39m(\u001b[38;5;28mself\u001b[39m, value):\n\u001b[1;32m--> 224\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m shape_equal(value\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape):\n\u001b[0;32m    226\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    227\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe shape of the target variable and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    228\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe shape of the target value in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    232\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget variable: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    233\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\saith\\Desktop\\NN_Classifier\\Southampton_NN_digit_classifier\\env\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:66\u001b[0m, in \u001b[0;36mVariable._convert_to_tensor\u001b[1;34m(self, value, dtype)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_convert_to_tensor\u001b[39m(\u001b[38;5;28mself\u001b[39m, value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\saith\\Desktop\\NN_Classifier\\Southampton_NN_digit_classifier\\env\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:125\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[1;34m(x, dtype, sparse)\u001b[0m\n\u001b[0;32m    123\u001b[0m         x \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(x)\n\u001b[0;32m    124\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mcast(x, dtype)\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m dtype:\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, tf\u001b[38;5;241m.\u001b[39mSparseTensor):\n",
      "File \u001b[1;32mc:\\Users\\saith\\Desktop\\NN_Classifier\\Southampton_NN_digit_classifier\\env\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\saith\\Desktop\\NN_Classifier\\Southampton_NN_digit_classifier\\env\\Lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:108\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    106\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[0;32m    107\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: object __array__ method not producing an array"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "\n",
    "# Load the three models\n",
    "models = [load_model(f'mnist_noisy_16x16_model_{i+1}.h5') for i in range(3)]\n",
    "\n",
    "def ensemble_predict(image):\n",
    "    # Ensure image is in the correct shape (1, 16, 16, 1)\n",
    "    if image.shape != (1, 16, 16, 1):\n",
    "        image = image.reshape(1, 16, 16, 1)\n",
    "    predictions = [model.predict(image) for model in models]\n",
    "    return np.mean(predictions, axis=0)\n",
    "\n",
    "# In your main loop, replace the prediction step with:\n",
    "prediction = ensemble_predict(processed_img)[0]\n",
    "predicted_label = np.argmax(prediction)\n",
    "confidence = prediction[predicted_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "object __array__ method not producing an array",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Load the three pre-trained models\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m models \u001b[38;5;241m=\u001b[39m [\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmnist_noisy_16x16_model_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m)]\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Function to preprocess image for the model\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_image\u001b[39m(img_array):\n",
      "File \u001b[1;32mc:\\Users\\saith\\Desktop\\NN_Classifier\\Southampton_NN_digit_classifier\\env\\Lib\\site-packages\\keras\\src\\saving\\saving_api.py:194\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[0;32m    188\u001b[0m         filepath,\n\u001b[0;32m    189\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[0;32m    190\u001b[0m         \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m,\n\u001b[0;32m    191\u001b[0m         safe_mode\u001b[38;5;241m=\u001b[39msafe_mode,\n\u001b[0;32m    192\u001b[0m     )\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.hdf5\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m--> 194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlegacy_h5_format\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model_from_hdf5\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcompile\u001b[39;49m\n\u001b[0;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    200\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure the file is an accessible `.keras` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    201\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzip file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    202\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\saith\\Desktop\\NN_Classifier\\Southampton_NN_digit_classifier\\env\\Lib\\site-packages\\keras\\src\\legacy\\saving\\legacy_h5_format.py:138\u001b[0m, in \u001b[0;36mload_model_from_hdf5\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    133\u001b[0m     model \u001b[38;5;241m=\u001b[39m saving_utils\u001b[38;5;241m.\u001b[39mmodel_from_config(\n\u001b[0;32m    134\u001b[0m         model_config, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects\n\u001b[0;32m    135\u001b[0m     )\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;66;03m# set weights\u001b[39;00m\n\u001b[1;32m--> 138\u001b[0m     \u001b[43mload_weights_from_hdf5_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_weights\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcompile\u001b[39m:\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;66;03m# instantiate optimizer\u001b[39;00m\n\u001b[0;32m    142\u001b[0m     training_config \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mattrs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_config\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\saith\\Desktop\\NN_Classifier\\Southampton_NN_digit_classifier\\env\\Lib\\site-packages\\keras\\src\\legacy\\saving\\legacy_h5_format.py:376\u001b[0m, in \u001b[0;36mload_weights_from_hdf5_group\u001b[1;34m(f, model)\u001b[0m\n\u001b[0;32m    369\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    370\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeight count mismatch for layer #\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    371\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe current model, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in the save file). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    372\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayer expects \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(symbolic_weights)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m weight(s). Received \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    373\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(weight_values)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m saved weight(s)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    374\u001b[0m         )\n\u001b[0;32m    375\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ref_v, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(symbolic_weights, weight_values):\n\u001b[1;32m--> 376\u001b[0m         \u001b[43mref_v\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massign\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_level_model_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[0;32m    379\u001b[0m     symbolic_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[0;32m    380\u001b[0m         \u001b[38;5;66;03m# model.weights\u001b[39;00m\n\u001b[0;32m    381\u001b[0m         v\n\u001b[0;32m    382\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39m_trainable_variables \u001b[38;5;241m+\u001b[39m model\u001b[38;5;241m.\u001b[39m_non_trainable_variables\n\u001b[0;32m    383\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mweights\n\u001b[0;32m    384\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\saith\\Desktop\\NN_Classifier\\Southampton_NN_digit_classifier\\env\\Lib\\site-packages\\keras\\src\\backend\\common\\variables.py:224\u001b[0m, in \u001b[0;36mKerasVariable.assign\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21massign\u001b[39m(\u001b[38;5;28mself\u001b[39m, value):\n\u001b[1;32m--> 224\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m shape_equal(value\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape):\n\u001b[0;32m    226\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    227\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe shape of the target variable and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    228\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe shape of the target value in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    232\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget variable: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    233\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\saith\\Desktop\\NN_Classifier\\Southampton_NN_digit_classifier\\env\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:66\u001b[0m, in \u001b[0;36mVariable._convert_to_tensor\u001b[1;34m(self, value, dtype)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_convert_to_tensor\u001b[39m(\u001b[38;5;28mself\u001b[39m, value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\saith\\Desktop\\NN_Classifier\\Southampton_NN_digit_classifier\\env\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:125\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[1;34m(x, dtype, sparse)\u001b[0m\n\u001b[0;32m    123\u001b[0m         x \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(x)\n\u001b[0;32m    124\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mcast(x, dtype)\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m dtype:\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, tf\u001b[38;5;241m.\u001b[39mSparseTensor):\n",
      "File \u001b[1;32mc:\\Users\\saith\\Desktop\\NN_Classifier\\Southampton_NN_digit_classifier\\env\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\saith\\Desktop\\NN_Classifier\\Southampton_NN_digit_classifier\\env\\Lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:108\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    106\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[0;32m    107\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: object __array__ method not producing an array"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import load_model\n",
    "from PIL import Image\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Load the three pre-trained models\n",
    "models = [load_model(f'mnist_noisy_16x16_model_{i+1}.h5') for i in range(3)]\n",
    "\n",
    "# Function to preprocess image for the model\n",
    "def preprocess_image(img_array):\n",
    "    print(f\"Original image shape: {img_array.shape}\")\n",
    "    print(f\"Image data type: {img_array.dtype}\")\n",
    "    \n",
    "    # Ensure the image is 2D (grayscale)\n",
    "    if len(img_array.shape) == 3:\n",
    "        img_array = np.mean(img_array, axis=-1)\n",
    "    \n",
    "    # Ensure the image is 16x16\n",
    "    if img_array.shape != (16, 16):\n",
    "        raise ValueError(f\"Unexpected image shape: {img_array.shape}\")\n",
    "    \n",
    "    # Reshape and normalize\n",
    "    img_array = img_array.reshape(1, 16, 16, 1)\n",
    "    img_array = img_array.astype('float32') / 255.0\n",
    "    return img_array\n",
    "\n",
    "# Function to get ensemble predictions\n",
    "def ensemble_predict(image):\n",
    "    predictions = [model.predict(image) for model in models]\n",
    "    return np.mean(predictions, axis=0)\n",
    "\n",
    "# Path to the zip file\n",
    "zip_filepath = 'test_digits/experiment_results_participant65.zip'\n",
    "\n",
    "# Dictionary to store results for each model and ensemble\n",
    "results = {i: {} for i in range(4)}  # 0, 1, 2 for individual models, 3 for ensemble\n",
    "\n",
    "# Open the zip file and process images\n",
    "with zipfile.ZipFile(zip_filepath, 'r') as zip_ref:\n",
    "    for filename in zip_ref.namelist():\n",
    "        if filename.endswith('_composite_participant65.png'):\n",
    "            true_label = int(filename.split('_')[0])\n",
    "            with zip_ref.open(filename) as file:\n",
    "                img = Image.open(file)\n",
    "                img_array = np.array(img)\n",
    "                \n",
    "                print(f\"\\nProcessing image: {filename}\")\n",
    "                try:\n",
    "                    processed_img = preprocess_image(img_array)\n",
    "                    \n",
    "                    # Get predictions for each model and ensemble\n",
    "                    for i in range(3):\n",
    "                        prediction = models[i].predict(processed_img)[0]\n",
    "                        predicted_label = np.argmax(prediction)\n",
    "                        confidence = prediction[predicted_label]\n",
    "                        results[i][true_label] = {\n",
    "                            'predicted_label': predicted_label,\n",
    "                            'confidence': confidence,\n",
    "                            'image': img_array\n",
    "                        }\n",
    "                    \n",
    "                    # Ensemble prediction\n",
    "                    ensemble_prediction = ensemble_predict(processed_img)[0]\n",
    "                    ensemble_label = np.argmax(ensemble_prediction)\n",
    "                    ensemble_confidence = ensemble_prediction[ensemble_label]\n",
    "                    results[3][true_label] = {\n",
    "                        'predicted_label': ensemble_label,\n",
    "                        'confidence': ensemble_confidence,\n",
    "                        'image': img_array\n",
    "                    }\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {filename}: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "# Function to display and print results\n",
    "def display_results(model_results, model_name):\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    for i, (true_label, data) in enumerate(sorted(model_results.items())):\n",
    "        plt.subplot(2, 5, i+1)\n",
    "        plt.imshow(data['image'], cmap='gray')\n",
    "        plt.title(f\"True: {true_label}, Pred: {data['predicted_label']}\\nConf: {data['confidence']:.2f}\")\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f\"Results for {model_name}\", fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\nDetailed Results for {model_name}:\")\n",
    "    print(\"True Label | Predicted Label | Confidence\")\n",
    "    print(\"-\" * 45)\n",
    "    for true_label, data in sorted(model_results.items()):\n",
    "        print(f\"{true_label:^10} | {data['predicted_label']:^15} | {data['confidence']:^10.2f}\")\n",
    "\n",
    "    accuracy = sum(1 for true_label, data in model_results.items() if true_label == data['predicted_label']) / len(model_results)\n",
    "    avg_confidence = sum(data['confidence'] for data in model_results.values()) / len(model_results)\n",
    "    print(f\"\\nOverall Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"Average Confidence: {avg_confidence:.2f}\")\n",
    "\n",
    "# Display and print results for each model and ensemble\n",
    "for i in range(4):\n",
    "    model_name = f\"Model {i+1}\" if i < 3 else \"Ensemble\"\n",
    "    display_results(results[i], model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ooof now we always predict 1\n",
    "\n",
    "I apologize for the disappointing results. This outcome suggests that our models are severely underfitting or have converged to a suboptimal solution. Let's analyze the situation and consider some improvements:\n",
    "\n",
    "Model Complexity: Our current model might be too simple for the task, especially given the noisy and small (16x16) images.\n",
    "\n",
    "Training Data: The resizing of MNIST to 16x16 might have lost too much information.\n",
    "\n",
    "Regularization: We might be over-regularizing, causing the model to underfit.\n",
    "\n",
    "Learning Rate: The learning rate might be too high, causing the model to overshoot the optimal solution.\n",
    "\n",
    "Training Duration: We might not be training for long enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
