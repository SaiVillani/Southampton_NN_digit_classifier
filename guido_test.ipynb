{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import string\n",
    "from mnist_skeptic_v9 import skeptic_v9\n",
    "import torch.nn.functional as F\n",
    "from tabulate import tabulate\n",
    "import pandas as pd\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Label: 7\n"
     ]
    }
   ],
   "source": [
    "def load_and_visualize_csv(file_path):\n",
    "    # Extract metadata from filename\n",
    "    filename = os.path.basename(file_path)\n",
    "    subject, digit, run = filename.split('_')[1], filename.split('_')[3], filename.split('_')[5].split('.')[0]\n",
    "    \n",
    "    # Read CSV data\n",
    "    with open(file_path, 'r') as f:\n",
    "        csv_data = f.read().strip().split('\\n')\n",
    "    \n",
    "    # Convert CSV to numpy array\n",
    "    image_data = np.array([list(map(float, row.split(','))) for row in csv_data])\n",
    "    \n",
    "    # Visualize\n",
    "    #plt.figure(figsize=(5, 5))\n",
    "    #plt.imshow(image_data, cmap='gray')\n",
    "    #plt.title(f\"Subject: {subject}, Digit: {digit}, Run: {run}\")\n",
    "    #plt.axis('off')\n",
    "    #plt.show()\n",
    "    \n",
    "    return image_data, int(digit)\n",
    "\n",
    "# Example usage\n",
    "csv_folder = 'CSV_Images'\n",
    "for file in os.listdir(csv_folder):\n",
    "    if file.endswith('.txt'):\n",
    "        file_path = os.path.join(csv_folder, file)\n",
    "        image_data, true_label = load_and_visualize_csv(file_path)\n",
    "        print(f\"True Label: {true_label}\")\n",
    "        break  # Remove this to process all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sai\\Desktop\\NN_digit_classifier\\Southampton_NN_digit_classifier\\env\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sai\\Desktop\\NN_digit_classifier\\Southampton_NN_digit_classifier\\env\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\Sai\\AppData\\Local\\Temp\\ipykernel_13356\\4128932609.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device), strict=False)\n"
     ]
    }
   ],
   "source": [
    "class ResNet50_16x16(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet50_16x16, self).__init__()\n",
    "        self.model = models.resnet50(pretrained=False)\n",
    "        self.model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.model.fc = nn.Linear(self.model.fc.in_features, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "def load_resnet50(model_path):\n",
    "    model = ResNet50_16x16()\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device), strict=False)\n",
    "    model.eval()\n",
    "    return model.to(device)\n",
    "\n",
    "# Uncomment to load the model\n",
    "resnet_model = load_resnet50('resnet50_mnist_experiment.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sai\\AppData\\Local\\Temp\\ipykernel_13356\\4232110767.py:32: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(file_path))  # Load pre-trained weights\n"
     ]
    }
   ],
   "source": [
    "# Function to create LeNet5 model for MNIST-like grayscale images\n",
    "class LeNet5_16x16(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(LeNet5_16x16, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, kernel_size=5),\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(6, 16, kernel_size=5),\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.fc1 = nn.Linear(16 * 1 * 1, 120)  # Adjusted for 16x16 input size\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(out.size(0), -1)  # Flatten the tensor\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "# Function to load LeNet5 model for 16x16 images with pre-trained weights\n",
    "def load_lenet5_model_16x16(file_path=None):\n",
    "    model = LeNet5_16x16(num_classes=10)\n",
    "    if file_path:\n",
    "        model.load_state_dict(torch.load(file_path))  # Load pre-trained weights\n",
    "    return model\n",
    "\n",
    "# Uncomment to load the model\n",
    "lenet_model = load_lenet5_model_16x16('lenet5_trained_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sai\\AppData\\Local\\Temp\\ipykernel_13356\\3498478915.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(checkpoint_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model 1/20: saved_models/skeptic_v10\\skeptic_v10_a_finetuned.pth\n",
      "Loaded model 2/20: saved_models/skeptic_v10\\skeptic_v10_b_finetuned.pth\n",
      "Loaded model 3/20: saved_models/skeptic_v10\\skeptic_v10_c_finetuned.pth\n",
      "Loaded model 4/20: saved_models/skeptic_v10\\skeptic_v10_d_finetuned.pth\n",
      "Loaded model 5/20: saved_models/skeptic_v10\\skeptic_v10_e_finetuned.pth\n",
      "Loaded model 6/20: saved_models/skeptic_v10\\skeptic_v10_f_finetuned.pth\n",
      "Loaded model 7/20: saved_models/skeptic_v10\\skeptic_v10_g_finetuned.pth\n",
      "Loaded model 8/20: saved_models/skeptic_v10\\skeptic_v10_h_finetuned.pth\n",
      "Loaded model 9/20: saved_models/skeptic_v10\\skeptic_v10_i_finetuned.pth\n",
      "Loaded model 10/20: saved_models/skeptic_v10\\skeptic_v10_j_finetuned.pth\n",
      "Loaded model 11/20: saved_models/skeptic_v10\\skeptic_v10_k_finetuned.pth\n",
      "Loaded model 12/20: saved_models/skeptic_v10\\skeptic_v10_l_finetuned.pth\n",
      "Loaded model 13/20: saved_models/skeptic_v10\\skeptic_v10_m_finetuned.pth\n",
      "Loaded model 14/20: saved_models/skeptic_v10\\skeptic_v10_n_finetuned.pth\n",
      "Loaded model 15/20: saved_models/skeptic_v10\\skeptic_v10_o_finetuned.pth\n",
      "Loaded model 16/20: saved_models/skeptic_v10\\skeptic_v10_p_finetuned.pth\n",
      "Loaded model 17/20: saved_models/skeptic_v10\\skeptic_v10_q_finetuned.pth\n",
      "Loaded model 18/20: saved_models/skeptic_v10\\skeptic_v10_r_finetuned.pth\n",
      "Loaded model 19/20: saved_models/skeptic_v10\\skeptic_v10_s_finetuned.pth\n",
      "Loaded model 20/20: saved_models/skeptic_v10\\skeptic_v10_t_finetuned.pth\n",
      "Ensemble loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "class ModelEnsemble:\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "    \n",
    "    def predict(self, x):\n",
    "        predictions = [model(x) for model in self.models]\n",
    "        return torch.stack(predictions).mean(dim=0)\n",
    "    \n",
    "    def eval(self):\n",
    "        for model in self.models:\n",
    "            model.eval()\n",
    "\n",
    "def load_ensemble_models(model_class, folder_path, num_models=20, device='cpu', verbose=True):\n",
    "    models = []\n",
    "    \n",
    "    for idx, letter in enumerate(string.ascii_lowercase[:num_models]):\n",
    "        model = model_class()\n",
    "        checkpoint_path = os.path.join(folder_path, f'skeptic_v10_{letter}_finetuned.pth')\n",
    "        \n",
    "        try:\n",
    "            if not os.path.exists(checkpoint_path):\n",
    "                raise FileNotFoundError(f\"Model checkpoint not found: {checkpoint_path}\")\n",
    "            \n",
    "            state_dict = torch.load(checkpoint_path, map_location=device)\n",
    "            model.load_state_dict(state_dict)\n",
    "            model.to(device)\n",
    "            model.eval()\n",
    "            models.append(model)\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"Loaded model {idx+1}/{num_models}: {checkpoint_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {idx+1}/{num_models}: {str(e)}\")\n",
    "    \n",
    "    if not models:\n",
    "        raise ValueError(\"No models were successfully loaded.\")\n",
    "    \n",
    "    ensemble = ModelEnsemble(models)\n",
    "    ensemble.eval()\n",
    "    return ensemble\n",
    "\n",
    "\n",
    "ensemble = load_ensemble_models(skeptic_v9, 'saved_models/skeptic_v10', num_models=20, device=device)\n",
    "print(\"Ensemble loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_data):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    image = Image.fromarray((image_data * 255).astype(np.uint8), mode='L')\n",
    "    return transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "def evaluate_model(model, image_tensor):\n",
    "    with torch.no_grad():\n",
    "        output = model(image_tensor)\n",
    "        predicted = output.argmax().item()\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sai\\AppData\\Local\\Temp\\ipykernel_13356\\4128932609.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device), strict=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------------+----------------------+-----------------------+-----------------+----------+\n",
      "| True Label | Predicted Label | Predicted Confidence | True Label Confidence | True Label Rank | Distance |\n",
      "+------------+-----------------+----------------------+-----------------------+-----------------+----------+\n",
      "|     7      |        4        |        0.1997        |        0.1048         |        4        |    3     |\n",
      "|     7      |        4        |        0.1962        |        0.1024         |        4        |    3     |\n",
      "|     4      |        4        |        0.2092        |        0.2092         |        1        |    0     |\n",
      "|     4      |        4        |        0.1843        |        0.1843         |        1        |    0     |\n",
      "|     7      |        4        |        0.1955        |        0.0962         |        4        |    3     |\n",
      "|     7      |        4        |        0.1971        |        0.1001         |        4        |    3     |\n",
      "|     6      |        4        |        0.1989        |        0.1117         |        3        |    2     |\n",
      "|     6      |        4        |        0.2124        |        0.1208         |        3        |    2     |\n",
      "|     3      |        4        |        0.1846        |        0.0567         |        9        |    8     |\n",
      "|     3      |        4        |        0.1823        |        0.0593         |        9        |    8     |\n",
      "+------------+-----------------+----------------------+-----------------------+-----------------+----------+\n",
      "\n",
      "Overall Accuracy: 5.56%\n",
      "Top-1 Accuracy (true label is model's top prediction): 5.56%\n",
      "Top-3 Accuracy (true label is within model's top 3 predictions): 11.11%\n"
     ]
    }
   ],
   "source": [
    "def evaluate_resnet50(csv_folder):\n",
    "    resnet_model = load_resnet50('resnet50_mnist_experiment.pth')\n",
    "    results = []\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    top_1_correct = 0\n",
    "    top_3_correct = 0\n",
    "    \n",
    "    for file in os.listdir(csv_folder):\n",
    "        if file.endswith('.txt'):\n",
    "            total += 1\n",
    "            file_path = os.path.join(csv_folder, file)\n",
    "            image_data, true_label = load_and_visualize_csv(file_path)\n",
    "            image_tensor = preprocess_image(image_data)\n",
    "            \n",
    "            if image_tensor.dim() == 5:\n",
    "                image_tensor = image_tensor.squeeze(1)\n",
    "            elif image_tensor.dim() != 4:\n",
    "                raise ValueError(f\"Unexpected tensor shape: {image_tensor.shape}\")\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                output = resnet_model(image_tensor)\n",
    "            \n",
    "            confidence_scores = F.softmax(output, dim=1).squeeze().tolist()\n",
    "            predicted = output.argmax(1).item()\n",
    "            \n",
    "            true_label_confidence = confidence_scores[true_label]\n",
    "            predicted_confidence = confidence_scores[predicted]\n",
    "            \n",
    "            sorted_scores = sorted(enumerate(confidence_scores), key=lambda x: x[1], reverse=True)\n",
    "            true_label_rank = [i for i, (label, _) in enumerate(sorted_scores) if label == true_label][0] + 1\n",
    "            predicted_rank = [i for i, (label, _) in enumerate(sorted_scores) if label == predicted][0] + 1\n",
    "            distance = abs(true_label_rank - predicted_rank)\n",
    "            \n",
    "            if predicted == true_label:\n",
    "                correct += 1\n",
    "            \n",
    "            if true_label_rank == 1:\n",
    "                top_1_correct += 1\n",
    "            \n",
    "            if true_label_rank <= 3:\n",
    "                top_3_correct += 1\n",
    "            \n",
    "            results.append({\n",
    "                'True Label': true_label,\n",
    "                'Predicted Label': predicted,\n",
    "                'Predicted Confidence': f\"{predicted_confidence:.4f}\",\n",
    "                'True Label Confidence': f\"{true_label_confidence:.4f}\",\n",
    "                'True Label Rank': true_label_rank,\n",
    "                'Distance': distance\n",
    "            })\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    accuracy = (correct / total) * 100\n",
    "    top_1_accuracy = (top_1_correct / total) * 100\n",
    "    top_3_accuracy = (top_3_correct / total) * 100\n",
    "    \n",
    "    return results_df, accuracy, top_1_accuracy, top_3_accuracy\n",
    "\n",
    "# Run evaluation\n",
    "evaluation_results, accuracy, top_1_accuracy, top_3_accuracy = evaluate_resnet50('CSV_Images')\n",
    "\n",
    "# Print pretty table (showing only first 10 rows)\n",
    "print(tabulate(evaluation_results.head(10), headers='keys', tablefmt='pretty', showindex=False))\n",
    "\n",
    "# Print overall metrics\n",
    "print(f\"\\nOverall Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"Top-1 Accuracy (true label is model's top prediction): {top_1_accuracy:.2f}%\")\n",
    "print(f\"Top-3 Accuracy (true label is within model's top 3 predictions): {top_3_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------------+----------------------+-----------------------+-----------------+----------+\n",
      "| True Label | Predicted Label | Predicted Confidence | True Label Confidence | True Label Rank | Distance |\n",
      "+------------+-----------------+----------------------+-----------------------+-----------------+----------+\n",
      "|     7      |        8        |        0.5577        |        0.0000         |        8        |    7     |\n",
      "|     7      |        8        |        0.7969        |        0.0000         |        7        |    6     |\n",
      "|     4      |        8        |        0.8689        |        0.0000         |        9        |    8     |\n",
      "|     4      |        8        |        0.4294        |        0.0000         |        7        |    6     |\n",
      "|     7      |        0        |        1.0000        |        0.0000         |        9        |    8     |\n",
      "|     7      |        5        |        0.9627        |        0.0000         |        8        |    7     |\n",
      "|     6      |        3        |        0.6961        |        0.0000         |        5        |    4     |\n",
      "|     6      |        0        |        1.0000        |        0.0000         |        2        |    1     |\n",
      "|     3      |        8        |        0.9940        |        0.0017         |        3        |    2     |\n",
      "|     3      |        0        |        1.0000        |        0.0000         |        5        |    4     |\n",
      "+------------+-----------------+----------------------+-----------------------+-----------------+----------+\n",
      "\n",
      "Overall Accuracy: 2.78%\n",
      "Top-1 Accuracy (true label is model's top prediction): 2.78%\n",
      "Top-3 Accuracy (true label is within model's top 3 predictions): 25.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sai\\AppData\\Local\\Temp\\ipykernel_13356\\4232110767.py:32: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(file_path))  # Load pre-trained weights\n"
     ]
    }
   ],
   "source": [
    "def evaluate_lenet5(csv_folder, model_name):\n",
    "    lenet_model = load_lenet5_model_16x16(model_name)\n",
    "    lenet_model.eval()  # Set the model to evaluation mode\n",
    "    \n",
    "    # Check if CUDA is available and move model to GPU if possible\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    lenet_model = lenet_model.to(device)\n",
    "    \n",
    "    results = []\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    top_1_correct = 0\n",
    "    top_3_correct = 0\n",
    "    \n",
    "    for file in os.listdir(csv_folder):\n",
    "        if file.endswith('.txt'):\n",
    "            total += 1\n",
    "            file_path = os.path.join(csv_folder, file)\n",
    "            image_data, true_label = load_and_visualize_csv(file_path)\n",
    "            image_tensor = preprocess_image(image_data)\n",
    "            \n",
    "            if image_tensor.dim() == 5:\n",
    "                image_tensor = image_tensor.squeeze(1)\n",
    "            elif image_tensor.dim() != 4:\n",
    "                raise ValueError(f\"Unexpected tensor shape: {image_tensor.shape}\")\n",
    "            \n",
    "            # Move input tensor to the same device as the model\n",
    "            image_tensor = image_tensor.to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                output = lenet_model(image_tensor)\n",
    "            \n",
    "            confidence_scores = F.softmax(output, dim=1).squeeze().cpu().tolist()\n",
    "            predicted = output.argmax(1).item()\n",
    "            \n",
    "            true_label_confidence = confidence_scores[true_label]\n",
    "            predicted_confidence = confidence_scores[predicted]\n",
    "            \n",
    "            sorted_scores = sorted(enumerate(confidence_scores), key=lambda x: x[1], reverse=True)\n",
    "            true_label_rank = [i for i, (label, _) in enumerate(sorted_scores) if label == true_label][0] + 1\n",
    "            predicted_rank = [i for i, (label, _) in enumerate(sorted_scores) if label == predicted][0] + 1\n",
    "            distance = abs(true_label_rank - predicted_rank)\n",
    "            \n",
    "            if predicted == true_label:\n",
    "                correct += 1\n",
    "            \n",
    "            if true_label_rank == 1:\n",
    "                top_1_correct += 1\n",
    "            \n",
    "            if true_label_rank <= 3:\n",
    "                top_3_correct += 1\n",
    "            \n",
    "            results.append({\n",
    "                'True Label': true_label,\n",
    "                'Predicted Label': predicted,\n",
    "                'Predicted Confidence': f\"{predicted_confidence:.4f}\",\n",
    "                'True Label Confidence': f\"{true_label_confidence:.4f}\",\n",
    "                'True Label Rank': true_label_rank,\n",
    "                'Distance': distance\n",
    "            })\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    accuracy = (correct / total) * 100\n",
    "    top_1_accuracy = (top_1_correct / total) * 100\n",
    "    top_3_accuracy = (top_3_correct / total) * 100\n",
    "    \n",
    "    return results_df, accuracy, top_1_accuracy, top_3_accuracy\n",
    "\n",
    "# Run evaluation\n",
    "evaluation_results, accuracy, top_1_accuracy, top_3_accuracy = evaluate_lenet5('CSV_Images', 'best_model.pth')\n",
    "\n",
    "# Print pretty table (showing only first 10 rows)\n",
    "print(tabulate(evaluation_results.head(10), headers='keys', tablefmt='pretty', showindex=False))\n",
    "\n",
    "# Print overall metrics\n",
    "print(f\"\\nOverall Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"Top-1 Accuracy (true label is model's top prediction): {top_1_accuracy:.2f}%\")\n",
    "print(f\"Top-3 Accuracy (true label is within model's top 3 predictions): {top_3_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------------+----------------------+-----------------------+-----------------+----------+\n",
      "| True Label | Predicted Label | Predicted Confidence | True Label Confidence | True Label Rank | Distance |\n",
      "+------------+-----------------+----------------------+-----------------------+-----------------+----------+\n",
      "|     7      |        6        |        0.1303        |        0.0843         |        8        |    7     |\n",
      "|     7      |        6        |        0.1434        |        0.0784         |       10        |    9     |\n",
      "|     4      |        1        |        0.1296        |        0.0837         |        9        |    8     |\n",
      "|     4      |        6        |        0.1879        |        0.1429         |        2        |    1     |\n",
      "|     7      |        5        |        0.1557        |        0.0782         |       10        |    9     |\n",
      "|     7      |        5        |        0.1527        |        0.0570         |       10        |    9     |\n",
      "|     6      |        7        |        0.1430        |        0.1295         |        2        |    1     |\n",
      "|     6      |        4        |        0.1936        |        0.1189         |        3        |    2     |\n",
      "|     3      |        4        |        0.1946        |        0.1153         |        3        |    2     |\n",
      "|     3      |        4        |        0.1904        |        0.1253         |        2        |    1     |\n",
      "+------------+-----------------+----------------------+-----------------------+-----------------+----------+\n",
      "\n",
      "Overall Accuracy: 2.78%\n",
      "Top-1 Accuracy (true label is model's top prediction): 2.78%\n",
      "Top-3 Accuracy (true label is within model's top 3 predictions): 30.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sai\\AppData\\Local\\Temp\\ipykernel_13356\\4232110767.py:32: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(file_path))  # Load pre-trained weights\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation\n",
    "evaluation_results, accuracy, top_1_accuracy, top_3_accuracy = evaluate_lenet5('CSV_Images', 'lenet5_trained_model.pth')\n",
    "\n",
    "# Print pretty table (showing only first 10 rows)\n",
    "print(tabulate(evaluation_results.head(10), headers='keys', tablefmt='pretty', showindex=False))\n",
    "\n",
    "# Print overall metrics\n",
    "print(f\"\\nOverall Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"Top-1 Accuracy (true label is model's top prediction): {top_1_accuracy:.2f}%\")\n",
    "print(f\"Top-3 Accuracy (true label is within model's top 3 predictions): {top_3_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------------+----------------------+-----------------------+-----------------+----------+\n",
      "| True Label | Predicted Label | Predicted Confidence | True Label Confidence | True Label Rank | Distance |\n",
      "+------------+-----------------+----------------------+-----------------------+-----------------+----------+\n",
      "|     7      |        4        |        0.1345        |        0.0562         |       10        |    9     |\n",
      "|     7      |        5        |        0.1486        |        0.0782         |        8        |    7     |\n",
      "|     4      |        9        |        0.1448        |        0.0515         |       10        |    9     |\n",
      "|     4      |        6        |        0.1946        |        0.1311         |        3        |    2     |\n",
      "|     7      |        2        |        0.1855        |        0.0885         |        4        |    3     |\n",
      "|     7      |        5        |        0.1718        |        0.0359         |       10        |    9     |\n",
      "|     6      |        7        |        0.2048        |        0.1058         |        4        |    3     |\n",
      "|     6      |        4        |        0.2292        |        0.1042         |        4        |    3     |\n",
      "|     3      |        1        |        0.2017        |        0.1023         |        3        |    2     |\n",
      "|     3      |        4        |        0.2181        |        0.1204         |        3        |    2     |\n",
      "+------------+-----------------+----------------------+-----------------------+-----------------+----------+\n",
      "\n",
      "Overall Accuracy: 2.78%\n",
      "Top-1 Accuracy (true label is model's top prediction): 2.78%\n",
      "Top-3 Accuracy (true label is within model's top 3 predictions): 22.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sai\\AppData\\Local\\Temp\\ipykernel_13356\\4232110767.py:32: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(file_path))  # Load pre-trained weights\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation\n",
    "evaluation_results, accuracy, top_1_accuracy, top_3_accuracy = evaluate_lenet5('CSV_Images', 'lenet5_trained_model2.pth')\n",
    "\n",
    "# Print pretty table (showing only first 10 rows)\n",
    "print(tabulate(evaluation_results.head(10), headers='keys', tablefmt='pretty', showindex=False))\n",
    "\n",
    "# Print overall metrics\n",
    "print(f\"\\nOverall Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"Top-1 Accuracy (true label is model's top prediction): {top_1_accuracy:.2f}%\")\n",
    "print(f\"Top-3 Accuracy (true label is within model's top 3 predictions): {top_3_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------------+----------------------+-----------------------+-----------------+----------+\n",
      "| True Label | Predicted Label | Predicted Confidence | True Label Confidence | True Label Rank | Distance |\n",
      "+------------+-----------------+----------------------+-----------------------+-----------------+----------+\n",
      "|     7      |        2        |        0.2128        |        0.0848         |        5        |    4     |\n",
      "|     7      |        7        |        0.2961        |        0.2961         |        1        |    0     |\n",
      "|     4      |        2        |        0.2747        |        0.0581         |        9        |    8     |\n",
      "|     4      |        4        |        0.1836        |        0.1836         |        1        |    0     |\n",
      "|     7      |        2        |        0.2199        |        0.1835         |        2        |    1     |\n",
      "|     7      |        7        |        0.2507        |        0.2507         |        1        |    0     |\n",
      "|     6      |        4        |        0.1740        |        0.1709         |        2        |    1     |\n",
      "|     6      |        6        |        0.1581        |        0.1581         |        1        |    0     |\n",
      "|     3      |        2        |        0.2508        |        0.1281         |        3        |    2     |\n",
      "|     3      |        6        |        0.2448        |        0.0578         |        8        |    7     |\n",
      "+------------+-----------------+----------------------+-----------------------+-----------------+----------+\n",
      "\n",
      "Overall Accuracy: 19.44%\n",
      "Top-1 Accuracy (true label is model's top prediction): 19.44%\n",
      "Top-3 Accuracy (true label is within model's top 3 predictions): 33.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sai\\AppData\\Local\\Temp\\ipykernel_13356\\4232110767.py:32: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(file_path))  # Load pre-trained weights\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation\n",
    "evaluation_results, accuracy, top_1_accuracy, top_3_accuracy = evaluate_lenet5('CSV_Images', 'lenet5_trained_model_ensemble_1.pth')\n",
    "\n",
    "# Print pretty table (showing only first 10 rows)\n",
    "print(tabulate(evaluation_results.head(10), headers='keys', tablefmt='pretty', showindex=False))\n",
    "\n",
    "# Print overall metrics\n",
    "print(f\"\\nOverall Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"Top-1 Accuracy (true label is model's top prediction): {top_1_accuracy:.2f}%\")\n",
    "print(f\"Top-3 Accuracy (true label is within model's top 3 predictions): {top_3_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sai\\AppData\\Local\\Temp\\ipykernel_13356\\3498478915.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(checkpoint_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model 1/20: saved_models/skeptic_v10\\skeptic_v10_a_finetuned.pth\n",
      "Loaded model 2/20: saved_models/skeptic_v10\\skeptic_v10_b_finetuned.pth\n",
      "Loaded model 3/20: saved_models/skeptic_v10\\skeptic_v10_c_finetuned.pth\n",
      "Loaded model 4/20: saved_models/skeptic_v10\\skeptic_v10_d_finetuned.pth\n",
      "Loaded model 5/20: saved_models/skeptic_v10\\skeptic_v10_e_finetuned.pth\n",
      "Loaded model 6/20: saved_models/skeptic_v10\\skeptic_v10_f_finetuned.pth\n",
      "Loaded model 7/20: saved_models/skeptic_v10\\skeptic_v10_g_finetuned.pth\n",
      "Loaded model 8/20: saved_models/skeptic_v10\\skeptic_v10_h_finetuned.pth\n",
      "Loaded model 9/20: saved_models/skeptic_v10\\skeptic_v10_i_finetuned.pth\n",
      "Loaded model 10/20: saved_models/skeptic_v10\\skeptic_v10_j_finetuned.pth\n",
      "Loaded model 11/20: saved_models/skeptic_v10\\skeptic_v10_k_finetuned.pth\n",
      "Loaded model 12/20: saved_models/skeptic_v10\\skeptic_v10_l_finetuned.pth\n",
      "Loaded model 13/20: saved_models/skeptic_v10\\skeptic_v10_m_finetuned.pth\n",
      "Loaded model 14/20: saved_models/skeptic_v10\\skeptic_v10_n_finetuned.pth\n",
      "Loaded model 15/20: saved_models/skeptic_v10\\skeptic_v10_o_finetuned.pth\n",
      "Loaded model 16/20: saved_models/skeptic_v10\\skeptic_v10_p_finetuned.pth\n",
      "Loaded model 17/20: saved_models/skeptic_v10\\skeptic_v10_q_finetuned.pth\n",
      "Loaded model 18/20: saved_models/skeptic_v10\\skeptic_v10_r_finetuned.pth\n",
      "Loaded model 19/20: saved_models/skeptic_v10\\skeptic_v10_s_finetuned.pth\n",
      "Loaded model 20/20: saved_models/skeptic_v10\\skeptic_v10_t_finetuned.pth\n",
      "+------------+-----------------+----------------------+-----------------------+-----------------+----------+\n",
      "| True Label | Predicted Label | Predicted Confidence | True Label Confidence | True Label Rank | Distance |\n",
      "+------------+-----------------+----------------------+-----------------------+-----------------+----------+\n",
      "|     7      |        9        |        0.1240        |        0.0706         |        9        |    8     |\n",
      "|     7      |        0        |        0.1135        |        0.0953         |        8        |    7     |\n",
      "|     4      |        9        |        0.1176        |        0.0802         |       10        |    9     |\n",
      "|     4      |        6        |        0.1424        |        0.1008         |        6        |    5     |\n",
      "|     7      |        5        |        0.1276        |        0.0956         |        8        |    7     |\n",
      "|     7      |        9        |        0.1572        |        0.0572         |       10        |    9     |\n",
      "|     6      |        8        |        0.1343        |        0.1246         |        3        |    2     |\n",
      "|     6      |        4        |        0.1966        |        0.0853         |        5        |    4     |\n",
      "|     3      |        1        |        0.1181        |        0.1068         |        3        |    2     |\n",
      "|     3      |        8        |        0.1196        |        0.1010         |        6        |    5     |\n",
      "+------------+-----------------+----------------------+-----------------------+-----------------+----------+\n",
      "\n",
      "Overall Accuracy: 2.78%\n",
      "Top-1 Accuracy (true label is model's top prediction): 2.78%\n",
      "Top-3 Accuracy (true label is within model's top 3 predictions): 25.00%\n"
     ]
    }
   ],
   "source": [
    "def evaluate_ensemble(csv_folder, ensemble_path):\n",
    "    ensemble_model = load_ensemble_models(skeptic_v9, ensemble_path, num_models=20, device=device)\n",
    "    ensemble_model.eval()\n",
    "    \n",
    "    results = []\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    top_1_correct = 0\n",
    "    top_3_correct = 0\n",
    "    \n",
    "    for file in os.listdir(csv_folder):\n",
    "        if file.endswith('.txt'):\n",
    "            total += 1\n",
    "            file_path = os.path.join(csv_folder, file)\n",
    "            image_data, true_label = load_and_visualize_csv(file_path)\n",
    "            image_tensor = preprocess_image(image_data)\n",
    "            \n",
    "            if image_tensor.dim() == 5:\n",
    "                image_tensor = image_tensor.squeeze(1)\n",
    "            elif image_tensor.dim() != 4:\n",
    "                raise ValueError(f\"Unexpected tensor shape: {image_tensor.shape}\")\n",
    "            \n",
    "            # Ensure the input tensor is on the same device as the model\n",
    "            image_tensor = image_tensor.to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                output = ensemble_model.predict(image_tensor)\n",
    "            \n",
    "            confidence_scores = F.softmax(output, dim=1).squeeze().cpu().tolist()\n",
    "            predicted = output.argmax(1).item()\n",
    "            \n",
    "            true_label_confidence = confidence_scores[true_label]\n",
    "            predicted_confidence = confidence_scores[predicted]\n",
    "            \n",
    "            sorted_scores = sorted(enumerate(confidence_scores), key=lambda x: x[1], reverse=True)\n",
    "            true_label_rank = [i for i, (label, _) in enumerate(sorted_scores) if label == true_label][0] + 1\n",
    "            predicted_rank = [i for i, (label, _) in enumerate(sorted_scores) if label == predicted][0] + 1\n",
    "            distance = abs(true_label_rank - predicted_rank)\n",
    "            \n",
    "            if predicted == true_label:\n",
    "                correct += 1\n",
    "            \n",
    "            if true_label_rank == 1:\n",
    "                top_1_correct += 1\n",
    "            \n",
    "            if true_label_rank <= 3:\n",
    "                top_3_correct += 1\n",
    "            \n",
    "            results.append({\n",
    "                'True Label': true_label,\n",
    "                'Predicted Label': predicted,\n",
    "                'Predicted Confidence': f\"{predicted_confidence:.4f}\",\n",
    "                'True Label Confidence': f\"{true_label_confidence:.4f}\",\n",
    "                'True Label Rank': true_label_rank,\n",
    "                'Distance': distance\n",
    "            })\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    accuracy = (correct / total) * 100\n",
    "    top_1_accuracy = (top_1_correct / total) * 100\n",
    "    top_3_accuracy = (top_3_correct / total) * 100\n",
    "    \n",
    "    return results_df, accuracy, top_1_accuracy, top_3_accuracy\n",
    "\n",
    "# Run evaluation\n",
    "ensemble_path = 'saved_models/skeptic_v10'  # Adjust this path as needed\n",
    "evaluation_results, accuracy, top_1_accuracy, top_3_accuracy = evaluate_ensemble('CSV_Images', ensemble_path)\n",
    "\n",
    "# Print pretty table (showing only first 10 rows)\n",
    "print(tabulate(evaluation_results.head(10), headers='keys', tablefmt='pretty', showindex=False))\n",
    "\n",
    "# Print overall metrics\n",
    "print(f\"\\nOverall Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"Top-1 Accuracy (true label is model's top prediction): {top_1_accuracy:.2f}%\")\n",
    "print(f\"Top-3 Accuracy (true label is within model's top 3 predictions): {top_3_accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
