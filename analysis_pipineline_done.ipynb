{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import required libraries\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Data extraction (updated)\n",
    "def extract_valid_results(jrzip_path):\n",
    "    \"\"\"Robust extraction with duplicate prevention\"\"\"\n",
    "    seen_pids = set()\n",
    "    valid_results = []\n",
    "    \n",
    "    with zipfile.ZipFile(jrzip_path, 'r') as zip_ref:\n",
    "        for file_info in zip_ref.infolist():\n",
    "            if not file_info.filename.endswith('.txt'):\n",
    "                continue\n",
    "                \n",
    "            with zip_ref.open(file_info) as f:\n",
    "                for line in f:\n",
    "                    try:\n",
    "                        data = json.loads(line.decode('utf-8').strip())\n",
    "                        pid = str(data['metadata']['participant_id']).strip().lower()  # Normalize IDs\n",
    "                        \n",
    "                        if pid in seen_pids:\n",
    "                            print(f\"Duplicate ID skipped: {pid} in {file_info.filename}\")\n",
    "                            continue\n",
    "                            \n",
    "                        seen_pids.add(pid)\n",
    "                        valid_results.append(data)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"Invalid entry: {str(e)}\")\n",
    "    \n",
    "    print(f\"Loaded {len(valid_results)} unique participants\")\n",
    "    return valid_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: DataFrame creation functions (updated)\n",
    "def create_participant_df(valid_results):\n",
    "    \"\"\"Participant metadata with comprehensive validation\"\"\"\n",
    "    participant_info = []\n",
    "    \n",
    "    for result in valid_results:\n",
    "        try:\n",
    "            meta = result['metadata']\n",
    "            params = meta.get('parameters', {}).get('filter_parameters', {})\n",
    "            \n",
    "            entry = {\n",
    "                'participant_id': meta['participant_id'],\n",
    "                'timestamp': pd.to_datetime(meta['timestamp'], errors='coerce'),\n",
    "                'filter_threshold': float(params.get('threshold', 0)),\n",
    "                'preservation_factor': float(params.get('preservation_factor', 0)),\n",
    "                'noise_reduction': float(params.get('noise_reduction_factor', 0)),\n",
    "                'has_vviq': 'vviq_data' in meta,\n",
    "                'has_caps': 'caps_data' in meta,\n",
    "                'has_training': 'training_phase' in result\n",
    "            }\n",
    "            \n",
    "            if entry['timestamp'] is pd.NaT:\n",
    "                print(f\"Invalid timestamp for {entry['participant_id']}\")\n",
    "                continue\n",
    "                \n",
    "            participant_info.append(entry)\n",
    "            \n",
    "        except KeyError as e:\n",
    "            print(f\"Missing key {e} in participant metadata\")\n",
    "            continue\n",
    "            \n",
    "    df = pd.DataFrame(participant_info).set_index('participant_id')\n",
    "    print(f\"Participant DF: {df.shape[0]} entries\")\n",
    "    return df\n",
    "\n",
    "def create_training_df(valid_results):\n",
    "    \"\"\"Training data with robust type handling\"\"\"\n",
    "    training_data = []\n",
    "    \n",
    "    for result in valid_results:\n",
    "        try:\n",
    "            training = result.get('training_phase', {})\n",
    "            summary = training.get('summary', {})\n",
    "            \n",
    "            acc = str(summary.get('accuracy_percentage', '0%')).rstrip('%')\n",
    "            \n",
    "            entry = {\n",
    "                'participant_id': result['metadata']['participant_id'],\n",
    "                'total_trials': int(summary.get('total_trials', 0)),\n",
    "                'correct_trials': int(summary.get('correct_trials', 0)),\n",
    "                'accuracy': float(acc),\n",
    "                'mean_rt': float(summary.get('mean_rt', 0)),\n",
    "                'completed': bool(training.get('completed', False))\n",
    "            }\n",
    "            training_data.append(entry)\n",
    "            \n",
    "        except Exception as e:\n",
    "            pid = result['metadata'].get('participant_id', 'unknown')\n",
    "            print(f\"Invalid training data for {pid}: {str(e)}\")\n",
    "            continue\n",
    "            \n",
    "    df = pd.DataFrame(training_data)\n",
    "    if not df.empty:\n",
    "        df = df.set_index('participant_id')\n",
    "    else:\n",
    "        df = pd.DataFrame(index=[r['metadata']['participant_id'] for r in valid_results])\n",
    "    print(f\"Training DF: {len(df)} entries\")\n",
    "    return df\n",
    "\n",
    "def create_vviq_df(valid_results):\n",
    "    \"\"\"VVIQ processing with missing data handling\"\"\"\n",
    "    vviq_data = []\n",
    "    seen_pids = set()\n",
    "\n",
    "    \n",
    "    for result in valid_results:\n",
    "        pid = str(result['metadata']['participant_id']).strip().lower()\n",
    "        if pid in seen_pids:\n",
    "            continue\n",
    "        seen_pids.add(pid)\n",
    "        try:\n",
    "            meta = result['metadata']\n",
    "            pid = meta['participant_id']\n",
    "            vviq_meta = meta['vviq_data']\n",
    "            \n",
    "            for condition in ['eyes_open', 'eyes_closed']:\n",
    "                responses = vviq_meta.get(condition, [])\n",
    "                for response in responses:\n",
    "                    q_idx = response['question_index']\n",
    "                    items = response.get('responses', {})\n",
    "                    for item, rating in items.items():\n",
    "                        entry = {\n",
    "                            'participant_id': pid,\n",
    "                            'condition': condition,\n",
    "                            'question_index': q_idx,\n",
    "                            'item': item,\n",
    "                            'rating': int(rating),\n",
    "                            'is_attention_check': q_idx == 3 and item == 'item_5',\n",
    "                            'attention_passed': vviq_meta['attention_check_passed'][condition]\n",
    "                        }\n",
    "                        vviq_data.append(entry)\n",
    "        except Exception as e:\n",
    "            pid = meta.get('participant_id', 'unknown')\n",
    "            print(f\"Skipping VVIQ for {pid}: {str(e)}\")\n",
    "            continue\n",
    "            \n",
    "    df = pd.DataFrame(vviq_data)\n",
    "    if not df.empty:\n",
    "        df = df.set_index('participant_id')\n",
    "    else:\n",
    "        df = pd.DataFrame(index=[r['metadata']['participant_id'] for r in valid_results])\n",
    "    print(f\"VVIQ DF: {len(df)} items\")\n",
    "    df = pd.DataFrame(vviq_data).set_index('participant_id')\n",
    "    return df[~df.index.duplicated(keep='first')]\n",
    "\n",
    "\n",
    "def create_caps_df(valid_results):\n",
    "    \"\"\"Handle yes/no responses properly\"\"\"\n",
    "    caps_data = []\n",
    "    seen_pids = set()\n",
    "    for result in valid_results:\n",
    "        pid = str(result['metadata']['participant_id']).strip().lower()\n",
    "        if pid in seen_pids:\n",
    "            continue\n",
    "        seen_pids.add(pid)\n",
    "        participant_id = result['metadata']['participant_id']\n",
    "        caps_responses = result['metadata'].get('caps_data', [])\n",
    "        \n",
    "        for response in caps_responses:\n",
    "            try:\n",
    "                # Convert yes/no to numeric\n",
    "                raw_response = response.get('response', '').lower()\n",
    "                numeric_response = 1 if 'yes' in raw_response else 0 if 'no' in raw_response else None\n",
    "                \n",
    "                caps_data.append({\n",
    "                    'participant_id': participant_id,\n",
    "                    'question_index': response['question_index'],\n",
    "                    'subscale': response['subscale'],\n",
    "                    'response': numeric_response,\n",
    "                    'raw_response': raw_response  # Keep original for validation\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping CAPS for {participant_id}: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    df = pd.DataFrame(caps_data).set_index('participant_id')\n",
    "    return df[~df.index.duplicated(keep='first')]\n",
    "\n",
    "\n",
    "\n",
    "def create_evolution_df(valid_results):\n",
    "    \"\"\"Evolution data with validation\"\"\"\n",
    "    evolution_data = []\n",
    "    \n",
    "    for result in valid_results:\n",
    "        try:\n",
    "            pid = result['metadata']['participant_id']\n",
    "            for session in result['evolution_summary']:\n",
    "                entry = {\n",
    "                    'participant_id': pid,\n",
    "                    'session': session['session'],\n",
    "                    'generation': session['generation'],\n",
    "                    'n_selections': len(session['selected_parents']),\n",
    "                    'duration': float(session.get('duration_ms', 0)),\n",
    "                    'session_type': session.get('type', 'unknown')\n",
    "                }\n",
    "                evolution_data.append(entry)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping evolution data for {pid}: {str(e)}\")\n",
    "            continue\n",
    "            \n",
    "    df = pd.DataFrame(evolution_data)\n",
    "    if not df.empty:\n",
    "        df = df.set_index('participant_id')\n",
    "    else:\n",
    "        df = pd.DataFrame(index=[r['metadata']['participant_id'] for r in valid_results])\n",
    "    print(f\"Evolution DF: {len(df)} entries\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Analysis functions (updated)\n",
    "def analyze_attention_checks(vviq_df):\n",
    "    \"\"\"Robust attention check analysis\"\"\"\n",
    "    if vviq_df.empty:\n",
    "        print(\"⚠️ Empty VVIQ DataFrame\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    required_cols = ['attention_passed', 'is_attention_check']\n",
    "    if not all(col in vviq_df.columns for col in required_cols):\n",
    "        missing = set(required_cols) - set(vviq_df.columns)\n",
    "        print(f\"Missing columns: {missing}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    analysis = (\n",
    "        vviq_df\n",
    "        .groupby('participant_id', observed=True)\n",
    "        .agg(\n",
    "            total_checks=('is_attention_check', 'size'),\n",
    "            passed_checks=('is_attention_check', 'sum'),\n",
    "            all_passed=('attention_passed', 'first')\n",
    "        )\n",
    "        .assign(valid_participant=lambda x: (x['total_checks'] >= 2) & x['all_passed'])\n",
    "    )\n",
    "    \n",
    "    print(f\"Valid participants: {analysis['valid_participant'].sum()}/{len(analysis)}\")\n",
    "    return analysis\n",
    "\n",
    "def analyze_training_performance(training_df):\n",
    "    \"\"\"Training analysis with empty DF handling\"\"\"\n",
    "    if training_df.empty:\n",
    "        print(\"⚠️ Empty Training DataFrame\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    stats = {\n",
    "        'accuracy': ['mean', 'std', 'min', 'max', lambda x: x.quantile(0.25), lambda x: x.quantile(0.75)],\n",
    "        'mean_rt': ['mean', 'std', 'min', 'max'],\n",
    "        'completed': ['mean']\n",
    "    }\n",
    "    \n",
    "    return (\n",
    "        training_df\n",
    "        .agg(stats)\n",
    "        .rename(columns={'<lambda_0>': 'q25', '<lambda_1>': 'q75'})\n",
    "        .round(2)\n",
    "        .T\n",
    "    )\n",
    "\n",
    "def validate_and_align_dataframes(dfs):\n",
    "    \"\"\"Ensure consistent index across all DataFrames\"\"\"\n",
    "    base_index = dfs['participants'].index.unique()\n",
    "    \n",
    "    for name in ['training', 'vviq', 'caps', 'evolution']:\n",
    "        # Preserve data but align indices\n",
    "        dfs[name] = dfs[name].reindex(base_index)\n",
    "        \n",
    "        # Add missing participant flag\n",
    "        if name not in ['participants']:\n",
    "            dfs[name]['data_present'] = ~dfs[name].index.duplicated(keep='first')\n",
    "    \n",
    "    # Check for duplicates\n",
    "    for name, df in dfs.items():\n",
    "        duplicates = df.index.duplicated()\n",
    "        if duplicates.any():\n",
    "            print(f\"⚠️ Removing {duplicates.sum()} duplicates from {name}\")\n",
    "            dfs[name] = df[~duplicates]\n",
    "    \n",
    "    return dfs\n",
    "\n",
    "def validate_indices(dfs):\n",
    "    \"\"\"Ensure unique indices across all dataframes\"\"\"\n",
    "    base_index = dfs['participants'].index.unique()\n",
    "    \n",
    "    for name, df in dfs.items():\n",
    "        # Remove duplicate indices\n",
    "        dfs[name] = df[~df.index.duplicated(keep='first')]\n",
    "        \n",
    "        # Align to participant index\n",
    "        dfs[name] = dfs[name].reindex(base_index)\n",
    "        \n",
    "        # Add existence flag for debugging\n",
    "        if name != 'participants':\n",
    "            dfs[name]['exists'] = dfs[name].index.isin(base_index)\n",
    "    \n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Final pipeline (updated)\n",
    "def main_analysis_pipeline(jrzip_path):\n",
    "    try:\n",
    "        # Data extraction\n",
    "        valid_results = extract_valid_results(jrzip_path)\n",
    "        \n",
    "        # Create base dataframes\n",
    "        dfs = {\n",
    "            'participants': create_participant_df(valid_results),\n",
    "            'training': create_training_df(valid_results),\n",
    "            'vviq': create_vviq_df(valid_results),\n",
    "            'caps': create_caps_df(valid_results),\n",
    "            'evolution': create_evolution_df(valid_results)\n",
    "        }\n",
    "        \n",
    "        # Validate and align indices\n",
    "        dfs = validate_indices(dfs)\n",
    "        \n",
    "        # Analysis\n",
    "        attention_report = analyze_attention_checks(dfs['vviq'])\n",
    "        training_report = analyze_training_performance(dfs['training'])\n",
    "        \n",
    "        return {\n",
    "            'data': dfs,\n",
    "            'reports': {\n",
    "                'attention': attention_report,\n",
    "                'training': training_report\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Pipeline failed: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 valid participant records\n",
      "\n",
      "Participant Summary:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "\n",
      "CAPS Responses:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "\n",
      "Trial Data:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import required libraries\n",
    "import base64\n",
    "from io import BytesIO\n",
    "import zipfile\n",
    "import json\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Cell 2: Enhanced data extraction\n",
    "def extract_valid_results(jrzip_path):\n",
    "    \"\"\"Extract and validate results from JRZIP file with improved validation\"\"\"\n",
    "    def validate_participant(data):\n",
    "        required_fields = {\n",
    "            'metadata': ['participant_id', 'sona_id', 'timestamp', 'demographics'],\n",
    "            'caps_data': list,\n",
    "            'parameters': ['filter_parameters'],\n",
    "            'training_phase': ['trials'],\n",
    "            'sessions': list\n",
    "        }\n",
    "        try:\n",
    "            for section, fields in required_fields.items():\n",
    "                if section not in data:\n",
    "                    return False\n",
    "                if isinstance(fields, list):\n",
    "                    for field in fields:\n",
    "                        if field not in data[section]:\n",
    "                            return False\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    valid_results = []\n",
    "    with zipfile.ZipFile(jrzip_path, 'r') as zip_ref:\n",
    "        for file_name in zip_ref.namelist():\n",
    "            if file_name.endswith('.txt'):\n",
    "                with zip_ref.open(file_name) as f:\n",
    "                    for line in f:\n",
    "                        try:\n",
    "                            decoded_line = line.decode('utf-8').strip()\n",
    "                            if decoded_line:\n",
    "                                data = json.loads(decoded_line)\n",
    "                                if validate_participant(data):\n",
    "                                    valid_results.append(data)\n",
    "                        except (json.JSONDecodeError, UnicodeDecodeError) as e:\n",
    "                            continue\n",
    "    print(f\"Found {len(valid_results)} valid participant records\")\n",
    "    return valid_results\n",
    "\n",
    "# Cell 3: Enhanced DataFrame creation\n",
    "def create_participant_df(valid_results):\n",
    "    \"\"\"Create comprehensive participant metadata DataFrame\"\"\"\n",
    "    participant_data = []\n",
    "    \n",
    "    for result in valid_results:\n",
    "        meta = result['metadata']\n",
    "        params = result['parameters']\n",
    "        filter_params = params['filter_parameters']\n",
    "        \n",
    "        participant_data.append({\n",
    "            'participant_id': meta['participant_id'],\n",
    "            'sona_id': meta['sona_id'],\n",
    "            'timestamp': meta['timestamp'],\n",
    "            'age': meta['demographics']['age'],\n",
    "            'gender': meta['demographics']['gender'],\n",
    "            'filter_threshold': filter_params['threshold'],\n",
    "            'preservation_factor': filter_params['preservation_factor'],\n",
    "            'noise_reduction': filter_params['noise_reduction_factor'],\n",
    "            'total_trials': result['training_phase']['summary']['total_trials'],\n",
    "            'accuracy': result['training_phase']['summary']['accuracy_percentage'],\n",
    "            'mean_rt': result['training_phase']['summary']['mean_rt']\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(participant_data)\n",
    "\n",
    "# Cell 4: Enhanced CAPS data processing\n",
    "def create_caps_df(valid_results):\n",
    "    \"\"\"Create DataFrame with all CAPS responses\"\"\"\n",
    "    caps_data = []\n",
    "    \n",
    "    for result in valid_results:\n",
    "        pid = result['metadata']['participant_id']\n",
    "        for item in result['caps_data']:\n",
    "            record = {\n",
    "                'participant_id': pid,\n",
    "                'question_index': item['question_index'],\n",
    "                'question_text': item['question_text'],\n",
    "                'response': item['response']\n",
    "            }\n",
    "            if item['subscale']:\n",
    "                record.update({\n",
    "                    'distressing': item['subscale']['distressing'],\n",
    "                    'distracting': item['subscale']['distracting'],\n",
    "                    'frequency': item['subscale']['frequency']\n",
    "                })\n",
    "            caps_data.append(record)\n",
    "    \n",
    "    return pd.DataFrame(caps_data)\n",
    "\n",
    "# Cell 5: Enhanced trial processing\n",
    "def create_trial_df(valid_results):\n",
    "    \"\"\"Create comprehensive trial-level DataFrame\"\"\"\n",
    "    trial_data = []\n",
    "    \n",
    "    for result in valid_results:\n",
    "        pid = result['metadata']['participant_id']\n",
    "        \n",
    "        # Process training phase trials\n",
    "        for trial in result['training_phase']['trials']:\n",
    "            trial_data.append({\n",
    "                'participant_id': pid,\n",
    "                'phase': 'training',\n",
    "                'trial_number': trial['trial_number'],\n",
    "                'target_index': trial['target_index'],\n",
    "                'reaction_time': trial.get('reaction_time_ms'),\n",
    "                'selected_id': trial.get('selected_id'),\n",
    "                'correct': trial.get('correct')\n",
    "            })\n",
    "        \n",
    "        # Process session trials\n",
    "        for session in result['sessions']:\n",
    "            for gen in session['generations']:\n",
    "                for trial in gen['trials']:\n",
    "                    if 'participant_selection' in trial:\n",
    "                        sel = trial['participant_selection']\n",
    "                        trial_data.append({\n",
    "                            'participant_id': pid,\n",
    "                            'phase': 'session',\n",
    "                            'trial_number': trial['trial_number'],\n",
    "                            'target_index': trial['target_index'],\n",
    "                            'reaction_time': sel.get('reaction_time_ms'),\n",
    "                            'selected_id': sel.get('stimulus_number'),\n",
    "                            'correct': sel.get('correct')\n",
    "                        })\n",
    "    \n",
    "    return pd.DataFrame(trial_data)\n",
    "\n",
    "# Cell 6: Image processing (unchanged but verified)\n",
    "def process_image_data(valid_results):\n",
    "    \"\"\"Process and display image stimuli\"\"\"\n",
    "    for result in valid_results[:1]:  # Just first participant for demo\n",
    "        for session in result['sessions']:\n",
    "            for gen in session['generations']:\n",
    "                for trial in gen['trials']:\n",
    "                    if 'displayed_stimuli' in trial:\n",
    "                        print(f\"Processing trial {trial['trial_number']}\")\n",
    "                        for stim in trial['displayed_stimuli']:\n",
    "                            img_data = stim['image_data'].split(\",\")[1]\n",
    "                            img = Image.open(BytesIO(base64.b64decode(img_data)))\n",
    "                            plt.imshow(img)\n",
    "                            plt.show()\n",
    "                        break\n",
    "                    break\n",
    "                break\n",
    "            break\n",
    "\n",
    "# Usage example:\n",
    "jrzip_path = \"jatos_results_20250207155153.jrzip\"\n",
    "results = extract_valid_results(jrzip_path)\n",
    "\n",
    "participant_df = create_participant_df(results)\n",
    "caps_df = create_caps_df(results)\n",
    "trial_df = create_trial_df(results)\n",
    "\n",
    "print(\"\\nParticipant Summary:\")\n",
    "print(participant_df.head())\n",
    "\n",
    "print(\"\\nCAPS Responses:\")\n",
    "print(caps_df.head())\n",
    "\n",
    "print(\"\\nTrial Data:\")\n",
    "print(trial_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
