{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from torchvision import models, datasets, transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import zipfile\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load experimental data\n",
    "def load_all_experimental_data(test_digits_folder):\n",
    "    train_images = []\n",
    "    train_labels = []\n",
    "    test_images = []\n",
    "    test_labels = []\n",
    "    participant_data = {}\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((16, 16)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "    for filename in os.listdir(test_digits_folder):\n",
    "        if filename.endswith('.zip') and filename.startswith('experiment_results_participant'):\n",
    "            participant_number = int(filename.split('participant')[1].split('.')[0])\n",
    "            zip_filepath = os.path.join(test_digits_folder, filename)\n",
    "\n",
    "            participant_train_images = []\n",
    "            participant_train_labels = []\n",
    "            participant_test_images = []\n",
    "            participant_test_labels = []\n",
    "\n",
    "            with zipfile.ZipFile(zip_filepath, 'r') as zip_ref:\n",
    "                for img_filename in zip_ref.namelist():\n",
    "                    if img_filename.endswith('.png'):\n",
    "                        with zip_ref.open(img_filename) as file:\n",
    "                            img = Image.open(file).convert('L')\n",
    "                            img_tensor = transform(img)\n",
    "                            \n",
    "                            digit = int(img_filename.split('_')[0])\n",
    "                            \n",
    "                            if 'composite' in img_filename:\n",
    "                                test_images.append(img_tensor)\n",
    "                                test_labels.append(digit)\n",
    "                                participant_test_images.append(img_tensor)\n",
    "                                participant_test_labels.append(digit)\n",
    "                            else:\n",
    "                                train_images.append(img_tensor)\n",
    "                                train_labels.append(digit)\n",
    "                                participant_train_images.append(img_tensor)\n",
    "                                participant_train_labels.append(digit)\n",
    "\n",
    "            participant_data[participant_number] = {\n",
    "                'train': (torch.stack(participant_train_images), torch.tensor(participant_train_labels)),\n",
    "                'test': (torch.stack(participant_test_images), torch.tensor(participant_test_labels))\n",
    "            }\n",
    "\n",
    "    return (torch.stack(train_images), torch.tensor(train_labels), \n",
    "            torch.stack(test_images), torch.tensor(test_labels),\n",
    "            participant_data)\n",
    "\n",
    "# Load experimental data\n",
    "exp_train_images, exp_train_labels, exp_test_images, exp_test_labels, participant_data = load_all_experimental_data('test_digits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the test data into validation and test sets\n",
    "val_images, final_test_images, val_labels, final_test_labels = train_test_split(\n",
    "    exp_test_images, exp_test_labels, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "# Assuming exp_train_images and exp_train_labels are already loaded\n",
    "train_dataset = TensorDataset(exp_train_images, exp_train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Create DataLoader for validation set\n",
    "val_dataset = torch.utils.data.TensorDataset(val_images, val_labels)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Create DataLoader for final test set\n",
    "test_dataset = torch.utils.data.TensorDataset(final_test_images, final_test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Define LeNet5 architecture modified for 16x16 images\n",
    "class LeNet5_16x16(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(LeNet5_16x16, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, kernel_size=5),\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(6, 16, kernel_size=5),\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.fc1 = nn.Linear(16 * 1 * 1, 120)  # Adjusted for 16x16 input size\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(out.size(0), -1)  # Flatten the tensor\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "# Load LeNet5 model for 16x16 images\n",
    "def load_lenet5_model_16x16():\n",
    "    model = LeNet5_16x16(num_classes=10)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 2408.0627\n",
      "Validation Accuracy: 68.42%\n",
      "Epoch [2/15], Loss: 2377.3934\n",
      "Validation Accuracy: 73.68%\n",
      "Epoch [3/15], Loss: 2365.0198\n",
      "Validation Accuracy: 75.79%\n",
      "Epoch [4/15], Loss: 2358.5327\n",
      "Validation Accuracy: 85.26%\n",
      "Epoch [5/15], Loss: 2354.6844\n",
      "Validation Accuracy: 90.53%\n",
      "Epoch [6/15], Loss: 2351.0160\n",
      "Validation Accuracy: 90.53%\n",
      "Epoch [7/15], Loss: 2346.9722\n",
      "Validation Accuracy: 74.74%\n",
      "Epoch [8/15], Loss: 2344.3531\n",
      "Validation Accuracy: 86.32%\n",
      "Epoch [9/15], Loss: 2342.2535\n",
      "Validation Accuracy: 88.42%\n",
      "Epoch [10/15], Loss: 2341.1659\n",
      "Validation Accuracy: 96.84%\n",
      "Epoch [11/15], Loss: 2339.2536\n",
      "Validation Accuracy: 94.74%\n",
      "Epoch [12/15], Loss: 2337.6042\n",
      "Validation Accuracy: 95.79%\n",
      "Epoch [13/15], Loss: 2336.2158\n",
      "Validation Accuracy: 89.47%\n",
      "Epoch [14/15], Loss: 2334.8472\n",
      "Validation Accuracy: 88.42%\n",
      "Epoch [15/15], Loss: 2334.1785\n",
      "Validation Accuracy: 93.68%\n",
      "Model saved to lenet5_trained_model.pth\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuning function for LeNet5 on your dataset\n",
    "def fine_tune_lenet(model, train_loader, val_loader=None, num_epochs=15, save_path=\"lenet5_model.pth\"):\n",
    "    criterion = nn.CrossEntropyLoss()  # Loss function for classification tasks\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer\n",
    "\n",
    "    model.train()  # Set model to training mode\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        # Training loop\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()  # Zero the parameter gradients\n",
    "            \n",
    "            outputs = model(images)  # Forward pass\n",
    "            loss = criterion(outputs, labels)  # Compute loss\n",
    "            \n",
    "            loss.backward()  # Backward pass (compute gradients)\n",
    "            optimizer.step()  # Update weights\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss:.4f}')\n",
    "        \n",
    "        # Validation loop (if validation set is provided)\n",
    "        if val_loader:\n",
    "            correct_val = 0\n",
    "            total_val = 0\n",
    "            model.eval()  # Set model to evaluation mode during validation\n",
    "            \n",
    "            with torch.no_grad():  # Disable gradient computation during validation\n",
    "                for val_images_batch, val_labels_batch in val_loader:\n",
    "                    outputs_val = model(val_images_batch)\n",
    "                    _, predicted_val = torch.max(outputs_val.data, 1)   # Get predicted class\n",
    "                    \n",
    "                    total_val += val_labels_batch.size(0)\n",
    "                    correct_val += (predicted_val == val_labels_batch).sum().item()\n",
    "            \n",
    "            print(f'Validation Accuracy: {100 * correct_val / total_val:.2f}%')\n",
    "            \n",
    "            model.train()  # Switch back to training mode after validation\n",
    "\n",
    "    # Save the trained model after training is complete\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"Model saved to {save_path}\")\n",
    "\n",
    "# Example usage:\n",
    "lenet_model_16x16 = load_lenet5_model_16x16()\n",
    "fine_tune_lenet(lenet_model_16x16, train_loader=train_loader, val_loader=val_loader, save_path=\"lenet5_trained_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on final test images: 91.58%\n"
     ]
    }
   ],
   "source": [
    "def evaluate_lenet(model, test_loader):\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    \n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation for inference\n",
    "        for images_batch_test, labels_batch_test in test_loader:\n",
    "            outputs_test = model(images_batch_test)\n",
    "            _, predicted_test_classifications = torch.max(outputs_test.data, 1)   # Get predicted class\n",
    "            \n",
    "            total_test += labels_batch_test.size(0)\n",
    "            correct_test += (predicted_test_classifications == labels_batch_test).sum().item()\n",
    "    \n",
    "    print(f'Accuracy of the network on final test images: {100 * correct_test / total_test:.2f}%')\n",
    "\n",
    "# Evaluate on final test set after training is complete\n",
    "evaluate_lenet(lenet_model_16x16, test_loader=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train Loss: 1.6164, Val Loss: 0.6012, Val Accuracy: 81.60%\n",
      "Model saved to lenet5_trained_model1.pth\n",
      "Epoch [2/50], Train Loss: 1.3542, Val Loss: 0.4199, Val Accuracy: 87.36%\n",
      "Model saved to lenet5_trained_model1.pth\n",
      "Epoch [3/50], Train Loss: 1.2854, Val Loss: 0.3513, Val Accuracy: 89.23%\n",
      "Model saved to lenet5_trained_model1.pth\n",
      "Epoch [4/50], Train Loss: 1.2551, Val Loss: 0.3177, Val Accuracy: 90.09%\n",
      "Model saved to lenet5_trained_model1.pth\n",
      "Epoch [5/50], Train Loss: 1.2307, Val Loss: 0.2901, Val Accuracy: 91.12%\n",
      "Model saved to lenet5_trained_model1.pth\n",
      "Epoch [6/50], Train Loss: 1.2169, Val Loss: 0.2750, Val Accuracy: 91.65%\n",
      "Model saved to lenet5_trained_model1.pth\n",
      "Epoch [7/50], Train Loss: 1.2057, Val Loss: 0.2802, Val Accuracy: 91.27%\n",
      "Epoch [8/50], Train Loss: 1.1966, Val Loss: 0.2614, Val Accuracy: 91.98%\n",
      "Model saved to lenet5_trained_model1.pth\n",
      "Epoch [9/50], Train Loss: 1.1919, Val Loss: 0.2509, Val Accuracy: 92.34%\n",
      "Model saved to lenet5_trained_model1.pth\n",
      "Epoch [10/50], Train Loss: 1.1856, Val Loss: 0.2465, Val Accuracy: 92.27%\n",
      "Model saved to lenet5_trained_model1.pth\n",
      "Epoch [11/50], Train Loss: 1.1846, Val Loss: 0.2502, Val Accuracy: 92.07%\n",
      "Epoch [12/50], Train Loss: 1.1774, Val Loss: 0.2442, Val Accuracy: 92.36%\n",
      "Model saved to lenet5_trained_model1.pth\n",
      "Epoch [13/50], Train Loss: 1.1733, Val Loss: 0.2361, Val Accuracy: 92.67%\n",
      "Model saved to lenet5_trained_model1.pth\n",
      "Epoch [14/50], Train Loss: 1.1713, Val Loss: 0.2390, Val Accuracy: 92.53%\n",
      "Epoch [15/50], Train Loss: 1.1656, Val Loss: 0.2310, Val Accuracy: 92.78%\n",
      "Model saved to lenet5_trained_model1.pth\n",
      "Epoch [16/50], Train Loss: 1.1630, Val Loss: 0.2315, Val Accuracy: 92.93%\n",
      "Epoch [17/50], Train Loss: 1.1625, Val Loss: 0.2258, Val Accuracy: 92.81%\n",
      "Model saved to lenet5_trained_model1.pth\n",
      "Epoch [18/50], Train Loss: 1.1571, Val Loss: 0.2316, Val Accuracy: 92.72%\n",
      "Epoch [19/50], Train Loss: 1.1616, Val Loss: 0.2283, Val Accuracy: 92.74%\n",
      "Epoch [20/50], Train Loss: 1.1544, Val Loss: 0.2253, Val Accuracy: 92.79%\n",
      "Model saved to lenet5_trained_model1.pth\n",
      "Epoch [21/50], Train Loss: 1.1534, Val Loss: 0.2201, Val Accuracy: 92.99%\n",
      "Model saved to lenet5_trained_model1.pth\n",
      "Epoch [22/50], Train Loss: 1.1534, Val Loss: 0.2265, Val Accuracy: 93.08%\n",
      "Epoch [23/50], Train Loss: 1.1499, Val Loss: 0.2241, Val Accuracy: 92.87%\n",
      "Epoch [24/50], Train Loss: 1.1492, Val Loss: 0.2234, Val Accuracy: 93.10%\n",
      "Epoch [25/50], Train Loss: 1.1469, Val Loss: 0.2117, Val Accuracy: 93.37%\n",
      "Model saved to lenet5_trained_model1.pth\n",
      "Epoch [26/50], Train Loss: 1.1485, Val Loss: 0.2230, Val Accuracy: 93.16%\n",
      "Epoch [27/50], Train Loss: 1.1483, Val Loss: 0.2221, Val Accuracy: 93.00%\n",
      "Epoch [28/50], Train Loss: 1.1460, Val Loss: 0.2160, Val Accuracy: 93.31%\n",
      "Epoch [29/50], Train Loss: 1.1455, Val Loss: 0.2156, Val Accuracy: 93.27%\n",
      "Epoch [30/50], Train Loss: 1.1440, Val Loss: 0.2189, Val Accuracy: 93.05%\n",
      "Epoch [31/50], Train Loss: 1.1421, Val Loss: 0.2163, Val Accuracy: 93.10%\n",
      "Epoch [32/50], Train Loss: 1.1341, Val Loss: 0.2083, Val Accuracy: 93.47%\n",
      "Model saved to lenet5_trained_model1.pth\n",
      "Epoch [33/50], Train Loss: 1.1283, Val Loss: 0.2023, Val Accuracy: 93.68%\n",
      "Model saved to lenet5_trained_model1.pth\n",
      "Epoch [34/50], Train Loss: 1.1282, Val Loss: 0.2014, Val Accuracy: 93.75%\n",
      "Model saved to lenet5_trained_model1.pth\n",
      "Epoch [35/50], Train Loss: 1.1282, Val Loss: 0.1994, Val Accuracy: 93.64%\n",
      "Model saved to lenet5_trained_model1.pth\n",
      "Epoch [36/50], Train Loss: 1.1295, Val Loss: 0.2005, Val Accuracy: 93.82%\n",
      "Epoch [37/50], Train Loss: 1.1291, Val Loss: 0.2019, Val Accuracy: 93.42%\n",
      "Epoch [38/50], Train Loss: 1.1231, Val Loss: 0.1967, Val Accuracy: 93.71%\n",
      "Model saved to lenet5_trained_model1.pth\n",
      "Epoch [39/50], Train Loss: 1.1262, Val Loss: 0.1982, Val Accuracy: 93.72%\n",
      "Epoch [40/50], Train Loss: 1.1221, Val Loss: 0.1994, Val Accuracy: 93.73%\n",
      "Epoch [41/50], Train Loss: 1.1208, Val Loss: 0.1970, Val Accuracy: 93.59%\n",
      "Epoch [42/50], Train Loss: 1.1233, Val Loss: 0.1958, Val Accuracy: 93.50%\n",
      "Model saved to lenet5_trained_model1.pth\n",
      "Epoch [43/50], Train Loss: 1.1240, Val Loss: 0.1981, Val Accuracy: 93.81%\n",
      "Epoch [44/50], Train Loss: 1.1244, Val Loss: 0.1948, Val Accuracy: 93.89%\n",
      "Model saved to lenet5_trained_model1.pth\n",
      "Epoch [45/50], Train Loss: 1.1234, Val Loss: 0.1937, Val Accuracy: 93.87%\n",
      "Model saved to lenet5_trained_model1.pth\n",
      "Epoch [46/50], Train Loss: 1.1237, Val Loss: 0.1927, Val Accuracy: 93.93%\n",
      "Model saved to lenet5_trained_model1.pth\n",
      "Epoch [47/50], Train Loss: 1.1184, Val Loss: 0.1936, Val Accuracy: 93.85%\n",
      "Epoch [48/50], Train Loss: 1.1214, Val Loss: 0.1973, Val Accuracy: 93.70%\n",
      "Epoch [49/50], Train Loss: 1.1249, Val Loss: 0.1977, Val Accuracy: 93.63%\n",
      "Epoch [50/50], Train Loss: 1.1239, Val Loss: 0.1941, Val Accuracy: 93.83%\n",
      "Accuracy of the network on final test images: 6.32%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, ConcatDataset\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "\n",
    "# Define LeNet5 architecture modified for 16x16 images with dropout\n",
    "class LeNet5_16x16(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(LeNet5_16x16, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, kernel_size=5),\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool2d(kernel_size=2),\n",
    "            nn.Dropout(0.25)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(6, 16, kernel_size=5),\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool2d(kernel_size=2),\n",
    "            nn.Dropout(0.25)\n",
    "        )\n",
    "        self.fc1 = nn.Linear(16 * 1 * 1, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = torch.tanh(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "# Data augmentation and normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((16, 16)),\n",
    "    transforms.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Load MNIST dataset\n",
    "mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "mnist_test = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Convert MNIST datasets to TensorDataset\n",
    "mnist_train_data = torch.stack([img for img, _ in mnist_train])\n",
    "mnist_train_labels = torch.tensor([label for _, label in mnist_train])\n",
    "mnist_train_tensor = TensorDataset(mnist_train_data, mnist_train_labels)\n",
    "\n",
    "mnist_test_data = torch.stack([img for img, _ in mnist_test])\n",
    "mnist_test_labels = torch.tensor([label for _, label in mnist_test])\n",
    "mnist_test_tensor = TensorDataset(mnist_test_data, mnist_test_labels)\n",
    "\n",
    "# Combine MNIST and experimental data\n",
    "exp_train_dataset = TensorDataset(exp_train_images, exp_train_labels)\n",
    "val_dataset = TensorDataset(val_images, val_labels)\n",
    "\n",
    "combined_train = ConcatDataset([mnist_train_tensor, exp_train_dataset])\n",
    "combined_val = ConcatDataset([mnist_test_tensor, val_dataset])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(combined_train, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(combined_val, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Calculate class weights\n",
    "def calculate_class_weights(combined_dataset):\n",
    "    all_labels = []\n",
    "    for dataset in combined_dataset.datasets:\n",
    "        if isinstance(dataset, TensorDataset):\n",
    "            all_labels.extend(dataset.tensors[1].tolist())\n",
    "        else:\n",
    "            all_labels.extend([label for _, label in dataset])\n",
    "    \n",
    "    labels = torch.tensor(all_labels)\n",
    "    class_counts = torch.bincount(labels)\n",
    "    class_weights = 1. / class_counts.float()\n",
    "    return class_weights / class_weights.sum()\n",
    "\n",
    "class_weights = calculate_class_weights(combined_train)\n",
    "\n",
    "\n",
    "# Fine-tuning function for LeNet5\n",
    "def fine_tune_lenet(model, train_loader, val_loader, num_epochs=50, save_path=\"lenet5_trained_model1.pth\"):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience = 10\n",
    "    counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for val_images, val_labels in val_loader:\n",
    "                val_images, val_labels = val_images.to(device), val_labels.to(device)\n",
    "                outputs_val = model(val_images)\n",
    "                loss_val = criterion(outputs_val, val_labels)\n",
    "                val_loss += loss_val.item()\n",
    "                \n",
    "                _, predicted_val = torch.max(outputs_val.data, 1)\n",
    "                total_val += val_labels.size(0)\n",
    "                correct_val += (predicted_val == val_labels).sum().item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = 100 * correct_val / total_val\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {running_loss/len(train_loader):.4f}, '\n",
    "              f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            counter = 0\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"Model saved to {save_path}\")\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Train the model\n",
    "lenet_model_16x16 = LeNet5_16x16()\n",
    "trained_model = fine_tune_lenet(lenet_model_16x16, train_loader, val_loader)\n",
    "\n",
    "# Evaluate on test set\n",
    "def evaluate_lenet(model, test_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images_batch_test, labels_batch_test in test_loader:\n",
    "            images_batch_test, labels_batch_test = images_batch_test.to(device), labels_batch_test.to(device)\n",
    "            outputs_test = model(images_batch_test)\n",
    "            _, predicted_test_classifications = torch.max(outputs_test.data, 1)\n",
    "            \n",
    "            total_test += labels_batch_test.size(0)\n",
    "            correct_test += (predicted_test_classifications == labels_batch_test).sum().item()\n",
    "    \n",
    "    print(f'Accuracy of the network on final test images: {100 * correct_test / total_test:.2f}%')\n",
    "\n",
    "# Evaluate on final test set\n",
    "evaluate_lenet(trained_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "--------------------------------\n",
      "Epoch [1/30], Train Loss: 1.5198, Val Loss: 1.1170, Val Accuracy: 60.23%\n",
      "Epoch [2/30], Train Loss: 1.2720, Val Loss: 1.0247, Val Accuracy: 62.53%\n",
      "Epoch [3/30], Train Loss: 1.2170, Val Loss: 0.9742, Val Accuracy: 64.31%\n",
      "Epoch [4/30], Train Loss: 1.1824, Val Loss: 0.9545, Val Accuracy: 64.88%\n",
      "Epoch [5/30], Train Loss: 1.1670, Val Loss: 0.9389, Val Accuracy: 65.12%\n",
      "Epoch [6/30], Train Loss: 1.1515, Val Loss: 0.9304, Val Accuracy: 65.46%\n",
      "Epoch [7/30], Train Loss: 1.1402, Val Loss: 0.9324, Val Accuracy: 65.49%\n",
      "Epoch [8/30], Train Loss: 1.1349, Val Loss: 0.9224, Val Accuracy: 65.50%\n",
      "Epoch [9/30], Train Loss: 1.1291, Val Loss: 0.9100, Val Accuracy: 65.85%\n",
      "Epoch [10/30], Train Loss: 1.1224, Val Loss: 0.9133, Val Accuracy: 65.55%\n",
      "Epoch [11/30], Train Loss: 1.1169, Val Loss: 0.9139, Val Accuracy: 65.90%\n",
      "Epoch [12/30], Train Loss: 1.1123, Val Loss: 0.9043, Val Accuracy: 66.27%\n",
      "Epoch [13/30], Train Loss: 1.1086, Val Loss: 0.8936, Val Accuracy: 66.60%\n",
      "Epoch [14/30], Train Loss: 1.1045, Val Loss: 0.9042, Val Accuracy: 66.35%\n",
      "Epoch [15/30], Train Loss: 1.1047, Val Loss: 0.8990, Val Accuracy: 66.35%\n",
      "Epoch [16/30], Train Loss: 1.1021, Val Loss: 0.8934, Val Accuracy: 66.39%\n",
      "Epoch [17/30], Train Loss: 1.1048, Val Loss: 0.8979, Val Accuracy: 66.40%\n",
      "Epoch [18/30], Train Loss: 1.0964, Val Loss: 0.8948, Val Accuracy: 66.46%\n",
      "Epoch [19/30], Train Loss: 1.0943, Val Loss: 0.8957, Val Accuracy: 66.30%\n",
      "Epoch [20/30], Train Loss: 1.0938, Val Loss: 0.8858, Val Accuracy: 66.51%\n",
      "Epoch [21/30], Train Loss: 1.0928, Val Loss: 0.8871, Val Accuracy: 66.35%\n",
      "Epoch [22/30], Train Loss: 1.0877, Val Loss: 0.8867, Val Accuracy: 66.78%\n",
      "Epoch [23/30], Train Loss: 1.0879, Val Loss: 0.8852, Val Accuracy: 66.73%\n",
      "Epoch [24/30], Train Loss: 1.0889, Val Loss: 0.8865, Val Accuracy: 66.44%\n",
      "Epoch [25/30], Train Loss: 1.0854, Val Loss: 0.8826, Val Accuracy: 66.57%\n",
      "Epoch [26/30], Train Loss: 1.0855, Val Loss: 0.8800, Val Accuracy: 66.71%\n",
      "Epoch [27/30], Train Loss: 1.0854, Val Loss: 0.8875, Val Accuracy: 66.72%\n",
      "Epoch [28/30], Train Loss: 1.0805, Val Loss: 0.8871, Val Accuracy: 66.89%\n",
      "Epoch [29/30], Train Loss: 1.0832, Val Loss: 0.8791, Val Accuracy: 66.73%\n",
      "Epoch [30/30], Train Loss: 1.0800, Val Loss: 0.8847, Val Accuracy: 66.71%\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Epoch [1/30], Train Loss: 1.5510, Val Loss: 1.1703, Val Accuracy: 57.08%\n",
      "Epoch [2/30], Train Loss: 1.2616, Val Loss: 1.0196, Val Accuracy: 62.37%\n",
      "Epoch [3/30], Train Loss: 1.2002, Val Loss: 0.9981, Val Accuracy: 63.07%\n",
      "Epoch [4/30], Train Loss: 1.1678, Val Loss: 0.9599, Val Accuracy: 64.19%\n",
      "Epoch [5/30], Train Loss: 1.1535, Val Loss: 0.9571, Val Accuracy: 64.17%\n",
      "Epoch [6/30], Train Loss: 1.1442, Val Loss: 0.9399, Val Accuracy: 64.84%\n",
      "Epoch [7/30], Train Loss: 1.1340, Val Loss: 0.9355, Val Accuracy: 65.02%\n",
      "Epoch [8/30], Train Loss: 1.1292, Val Loss: 0.9316, Val Accuracy: 64.96%\n",
      "Epoch [9/30], Train Loss: 1.1233, Val Loss: 0.9226, Val Accuracy: 65.32%\n",
      "Epoch [10/30], Train Loss: 1.1146, Val Loss: 0.9281, Val Accuracy: 65.12%\n",
      "Epoch [11/30], Train Loss: 1.1123, Val Loss: 0.9188, Val Accuracy: 65.26%\n",
      "Epoch [12/30], Train Loss: 1.1099, Val Loss: 0.9185, Val Accuracy: 65.49%\n",
      "Epoch [13/30], Train Loss: 1.1064, Val Loss: 0.9100, Val Accuracy: 65.61%\n",
      "Epoch [14/30], Train Loss: 1.1044, Val Loss: 0.9219, Val Accuracy: 65.23%\n",
      "Epoch [15/30], Train Loss: 1.0989, Val Loss: 0.9098, Val Accuracy: 65.71%\n",
      "Epoch [16/30], Train Loss: 1.0992, Val Loss: 0.9207, Val Accuracy: 65.53%\n",
      "Epoch [17/30], Train Loss: 1.0948, Val Loss: 0.9082, Val Accuracy: 65.86%\n",
      "Epoch [18/30], Train Loss: 1.0940, Val Loss: 0.9059, Val Accuracy: 65.79%\n",
      "Epoch [19/30], Train Loss: 1.0927, Val Loss: 0.9089, Val Accuracy: 65.76%\n",
      "Epoch [20/30], Train Loss: 1.0896, Val Loss: 0.9054, Val Accuracy: 65.79%\n",
      "Epoch [21/30], Train Loss: 1.0860, Val Loss: 0.9073, Val Accuracy: 65.88%\n",
      "Epoch [22/30], Train Loss: 1.0875, Val Loss: 0.9001, Val Accuracy: 65.93%\n",
      "Epoch [23/30], Train Loss: 1.0870, Val Loss: 0.8997, Val Accuracy: 66.16%\n",
      "Epoch [24/30], Train Loss: 1.0830, Val Loss: 0.9033, Val Accuracy: 66.10%\n",
      "Epoch [25/30], Train Loss: 1.0844, Val Loss: 0.9025, Val Accuracy: 65.98%\n",
      "Epoch [26/30], Train Loss: 1.0807, Val Loss: 0.8942, Val Accuracy: 66.21%\n",
      "Epoch [27/30], Train Loss: 1.0828, Val Loss: 0.8978, Val Accuracy: 66.16%\n",
      "Epoch [28/30], Train Loss: 1.0821, Val Loss: 0.9069, Val Accuracy: 66.06%\n",
      "Epoch [29/30], Train Loss: 1.0783, Val Loss: 0.9084, Val Accuracy: 65.77%\n",
      "Epoch [30/30], Train Loss: 1.0780, Val Loss: 0.8993, Val Accuracy: 66.01%\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Epoch [1/30], Train Loss: 1.4957, Val Loss: 1.1041, Val Accuracy: 59.65%\n",
      "Epoch [2/30], Train Loss: 1.2423, Val Loss: 1.0095, Val Accuracy: 62.89%\n",
      "Epoch [3/30], Train Loss: 1.1856, Val Loss: 0.9919, Val Accuracy: 63.20%\n",
      "Epoch [4/30], Train Loss: 1.1626, Val Loss: 0.9675, Val Accuracy: 64.05%\n",
      "Epoch [5/30], Train Loss: 1.1446, Val Loss: 0.9659, Val Accuracy: 64.18%\n",
      "Epoch [6/30], Train Loss: 1.1281, Val Loss: 0.9559, Val Accuracy: 64.30%\n",
      "Epoch [7/30], Train Loss: 1.1275, Val Loss: 0.9504, Val Accuracy: 64.48%\n",
      "Epoch [8/30], Train Loss: 1.1190, Val Loss: 0.9401, Val Accuracy: 64.65%\n",
      "Epoch [9/30], Train Loss: 1.1117, Val Loss: 0.9319, Val Accuracy: 65.20%\n",
      "Epoch [10/30], Train Loss: 1.1076, Val Loss: 0.9314, Val Accuracy: 65.08%\n",
      "Epoch [11/30], Train Loss: 1.1012, Val Loss: 0.9298, Val Accuracy: 64.97%\n",
      "Epoch [12/30], Train Loss: 1.1018, Val Loss: 0.9232, Val Accuracy: 65.25%\n",
      "Epoch [13/30], Train Loss: 1.0977, Val Loss: 0.9228, Val Accuracy: 65.11%\n",
      "Epoch [14/30], Train Loss: 1.0975, Val Loss: 0.9212, Val Accuracy: 65.30%\n",
      "Epoch [15/30], Train Loss: 1.0923, Val Loss: 0.9199, Val Accuracy: 65.42%\n",
      "Epoch [16/30], Train Loss: 1.0922, Val Loss: 0.9202, Val Accuracy: 65.48%\n",
      "Epoch [17/30], Train Loss: 1.0864, Val Loss: 0.9190, Val Accuracy: 65.43%\n",
      "Epoch [18/30], Train Loss: 1.0832, Val Loss: 0.9255, Val Accuracy: 65.35%\n",
      "Epoch [19/30], Train Loss: 1.0864, Val Loss: 0.9136, Val Accuracy: 65.58%\n",
      "Epoch [20/30], Train Loss: 1.0818, Val Loss: 0.9244, Val Accuracy: 65.51%\n",
      "Epoch [21/30], Train Loss: 1.0824, Val Loss: 0.9101, Val Accuracy: 65.79%\n",
      "Epoch [22/30], Train Loss: 1.0816, Val Loss: 0.9114, Val Accuracy: 65.82%\n",
      "Epoch [23/30], Train Loss: 1.0765, Val Loss: 0.9150, Val Accuracy: 65.53%\n",
      "Epoch [24/30], Train Loss: 1.0776, Val Loss: 0.9180, Val Accuracy: 65.58%\n",
      "Epoch [25/30], Train Loss: 1.0732, Val Loss: 0.9199, Val Accuracy: 65.42%\n",
      "Epoch [26/30], Train Loss: 1.0564, Val Loss: 0.8972, Val Accuracy: 65.82%\n",
      "Epoch [27/30], Train Loss: 1.0531, Val Loss: 0.8961, Val Accuracy: 66.01%\n",
      "Epoch [28/30], Train Loss: 1.0537, Val Loss: 0.8960, Val Accuracy: 66.04%\n",
      "Epoch [29/30], Train Loss: 1.0505, Val Loss: 0.8960, Val Accuracy: 66.03%\n",
      "Epoch [30/30], Train Loss: 1.0464, Val Loss: 0.8947, Val Accuracy: 66.07%\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "Epoch [1/30], Train Loss: 1.5813, Val Loss: 1.1223, Val Accuracy: 59.90%\n",
      "Epoch [2/30], Train Loss: 1.2764, Val Loss: 1.0202, Val Accuracy: 62.70%\n",
      "Epoch [3/30], Train Loss: 1.2082, Val Loss: 0.9758, Val Accuracy: 63.66%\n",
      "Epoch [4/30], Train Loss: 1.1730, Val Loss: 0.9550, Val Accuracy: 64.62%\n",
      "Epoch [5/30], Train Loss: 1.1553, Val Loss: 0.9503, Val Accuracy: 64.71%\n",
      "Epoch [6/30], Train Loss: 1.1421, Val Loss: 0.9347, Val Accuracy: 64.95%\n",
      "Epoch [7/30], Train Loss: 1.1336, Val Loss: 0.9315, Val Accuracy: 65.09%\n",
      "Epoch [8/30], Train Loss: 1.1229, Val Loss: 0.9214, Val Accuracy: 65.38%\n",
      "Epoch [9/30], Train Loss: 1.1209, Val Loss: 0.9190, Val Accuracy: 65.77%\n",
      "Epoch [10/30], Train Loss: 1.1138, Val Loss: 0.9180, Val Accuracy: 65.81%\n",
      "Epoch [11/30], Train Loss: 1.1138, Val Loss: 0.9139, Val Accuracy: 65.63%\n",
      "Epoch [12/30], Train Loss: 1.1048, Val Loss: 0.9176, Val Accuracy: 65.81%\n",
      "Epoch [13/30], Train Loss: 1.1034, Val Loss: 0.9081, Val Accuracy: 65.75%\n",
      "Epoch [14/30], Train Loss: 1.1017, Val Loss: 0.9101, Val Accuracy: 65.50%\n",
      "Epoch [15/30], Train Loss: 1.0968, Val Loss: 0.9093, Val Accuracy: 65.92%\n",
      "Epoch [16/30], Train Loss: 1.0984, Val Loss: 0.9124, Val Accuracy: 65.83%\n",
      "Epoch [17/30], Train Loss: 1.0990, Val Loss: 0.9122, Val Accuracy: 65.88%\n",
      "Epoch [18/30], Train Loss: 1.0700, Val Loss: 0.8904, Val Accuracy: 66.09%\n",
      "Epoch [19/30], Train Loss: 1.0712, Val Loss: 0.8907, Val Accuracy: 66.24%\n",
      "Epoch [20/30], Train Loss: 1.0642, Val Loss: 0.8906, Val Accuracy: 66.21%\n",
      "Epoch [21/30], Train Loss: 1.0634, Val Loss: 0.8892, Val Accuracy: 66.13%\n",
      "Epoch [22/30], Train Loss: 1.0648, Val Loss: 0.8907, Val Accuracy: 66.32%\n",
      "Epoch [23/30], Train Loss: 1.0673, Val Loss: 0.8877, Val Accuracy: 66.36%\n",
      "Epoch [24/30], Train Loss: 1.0646, Val Loss: 0.8883, Val Accuracy: 66.20%\n",
      "Epoch [25/30], Train Loss: 1.0613, Val Loss: 0.8867, Val Accuracy: 66.26%\n",
      "Epoch [26/30], Train Loss: 1.0632, Val Loss: 0.8872, Val Accuracy: 66.27%\n",
      "Epoch [27/30], Train Loss: 1.0645, Val Loss: 0.8865, Val Accuracy: 66.40%\n",
      "Epoch [28/30], Train Loss: 1.0610, Val Loss: 0.8851, Val Accuracy: 66.22%\n",
      "Epoch [29/30], Train Loss: 1.0652, Val Loss: 0.8862, Val Accuracy: 66.42%\n",
      "Epoch [30/30], Train Loss: 1.0620, Val Loss: 0.8864, Val Accuracy: 66.40%\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "Epoch [1/30], Train Loss: 1.5348, Val Loss: 1.0949, Val Accuracy: 60.26%\n",
      "Epoch [2/30], Train Loss: 1.2672, Val Loss: 1.0214, Val Accuracy: 62.63%\n",
      "Epoch [3/30], Train Loss: 1.2144, Val Loss: 0.9836, Val Accuracy: 63.71%\n",
      "Epoch [4/30], Train Loss: 1.1814, Val Loss: 0.9515, Val Accuracy: 64.72%\n",
      "Epoch [5/30], Train Loss: 1.1610, Val Loss: 0.9514, Val Accuracy: 64.51%\n",
      "Epoch [6/30], Train Loss: 1.1437, Val Loss: 0.9401, Val Accuracy: 64.80%\n",
      "Epoch [7/30], Train Loss: 1.1340, Val Loss: 0.9327, Val Accuracy: 65.08%\n",
      "Epoch [8/30], Train Loss: 1.1287, Val Loss: 0.9370, Val Accuracy: 65.23%\n",
      "Epoch [9/30], Train Loss: 1.1239, Val Loss: 0.9373, Val Accuracy: 64.76%\n",
      "Epoch [10/30], Train Loss: 1.1125, Val Loss: 0.9225, Val Accuracy: 65.44%\n",
      "Epoch [11/30], Train Loss: 1.1134, Val Loss: 0.9331, Val Accuracy: 65.14%\n",
      "Epoch [12/30], Train Loss: 1.1103, Val Loss: 0.9142, Val Accuracy: 65.63%\n",
      "Epoch [13/30], Train Loss: 1.1079, Val Loss: 0.9197, Val Accuracy: 65.43%\n",
      "Epoch [14/30], Train Loss: 1.1027, Val Loss: 0.9102, Val Accuracy: 65.89%\n",
      "Epoch [15/30], Train Loss: 1.1002, Val Loss: 0.9109, Val Accuracy: 65.73%\n",
      "Epoch [16/30], Train Loss: 1.0974, Val Loss: 0.9126, Val Accuracy: 65.76%\n",
      "Epoch [17/30], Train Loss: 1.0954, Val Loss: 0.9080, Val Accuracy: 65.70%\n",
      "Epoch [18/30], Train Loss: 1.0969, Val Loss: 0.9072, Val Accuracy: 66.06%\n",
      "Epoch [19/30], Train Loss: 1.0939, Val Loss: 0.9090, Val Accuracy: 65.74%\n",
      "Epoch [20/30], Train Loss: 1.0877, Val Loss: 0.9013, Val Accuracy: 65.94%\n",
      "Epoch [21/30], Train Loss: 1.0904, Val Loss: 0.9075, Val Accuracy: 65.92%\n",
      "Epoch [22/30], Train Loss: 1.0869, Val Loss: 0.8990, Val Accuracy: 66.31%\n",
      "Epoch [23/30], Train Loss: 1.0879, Val Loss: 0.8977, Val Accuracy: 66.26%\n",
      "Epoch [24/30], Train Loss: 1.0869, Val Loss: 0.8977, Val Accuracy: 66.27%\n",
      "Epoch [25/30], Train Loss: 1.0884, Val Loss: 0.9001, Val Accuracy: 66.19%\n",
      "Epoch [26/30], Train Loss: 1.0830, Val Loss: 0.8977, Val Accuracy: 66.18%\n",
      "Epoch [27/30], Train Loss: 1.0823, Val Loss: 0.8921, Val Accuracy: 66.40%\n",
      "Epoch [28/30], Train Loss: 1.0823, Val Loss: 0.9047, Val Accuracy: 66.26%\n",
      "Epoch [29/30], Train Loss: 1.0809, Val Loss: 0.8953, Val Accuracy: 66.24%\n",
      "Epoch [30/30], Train Loss: 1.0829, Val Loss: 0.9026, Val Accuracy: 66.17%\n",
      "Accuracy of the network on final test images: 12.63%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sai\\AppData\\Local\\Temp\\ipykernel_50692\\4031194117.py:90: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  best_model.load_state_dict(torch.load(\"best_model.pth\"))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Data augmentation and normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((16, 16)),\n",
    "    transforms.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Combine all data\n",
    "all_data = ConcatDataset([mnist_train_tensor, mnist_test_tensor, exp_train_dataset, val_dataset, test_dataset])\n",
    "\n",
    "# K-Fold Cross-validation\n",
    "k_folds = 5\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, num_epochs=30):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)  # Added L2 regularization\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.1)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for val_images, val_labels in val_loader:\n",
    "                val_images, val_labels = val_images.to(device), val_labels.to(device)\n",
    "                outputs_val = model(val_images)\n",
    "                loss_val = criterion(outputs_val, val_labels)\n",
    "                val_loss += loss_val.item()\n",
    "                \n",
    "                _, predicted_val = torch.max(outputs_val.data, 1)\n",
    "                total_val += val_labels.size(0)\n",
    "                correct_val += (predicted_val == val_labels).sum().item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = 100 * correct_val / total_val\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {running_loss/len(train_loader):.4f}, '\n",
    "              f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# K-Fold Cross-validation training\n",
    "for fold, (train_ids, val_ids) in enumerate(kfold.split(all_data)):\n",
    "    print(f'FOLD {fold}')\n",
    "    print('--------------------------------')\n",
    "    \n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "    val_subsampler = torch.utils.data.SubsetRandomSampler(val_ids)\n",
    "    \n",
    "    train_loader = DataLoader(all_data, batch_size=32, sampler=train_subsampler)\n",
    "    val_loader = DataLoader(all_data, batch_size=32, sampler=val_subsampler)\n",
    "    \n",
    "    model = LeNet5_16x16()\n",
    "    train_model(model, train_loader, val_loader)\n",
    "\n",
    "# Load the best model and evaluate on the test set\n",
    "best_model = LeNet5_16x16()\n",
    "best_model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "evaluate_lenet(best_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5_16x16(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(LeNet5_16x16, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, kernel_size=5, padding=2),\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(6, 16, kernel_size=5),\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = torch.tanh(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 1\n",
      "--------------------------------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 2 in argument 0, but got int",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 211\u001b[0m\n\u001b[0;32m    208\u001b[0m     val_loader \u001b[38;5;241m=\u001b[39m DataLoader(all_train_data, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, sampler\u001b[38;5;241m=\u001b[39mval_subsampler)\n\u001b[0;32m    210\u001b[0m     model \u001b[38;5;241m=\u001b[39m LeNet5_16x16()\n\u001b[1;32m--> 211\u001b[0m     \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;66;03m# Load the best model and evaluate on the final validation set\u001b[39;00m\n\u001b[0;32m    214\u001b[0m best_model \u001b[38;5;241m=\u001b[39m LeNet5_16x16()\n",
      "Cell \u001b[1;32mIn[11], line 151\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, num_epochs, patience)\u001b[0m\n\u001b[0;32m    148\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m    149\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m--> 151\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m    152\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    154\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\Sai\\Desktop\\NN_digit_classifier\\Southampton_NN_digit_classifier\\env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Sai\\Desktop\\NN_digit_classifier\\Southampton_NN_digit_classifier\\env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Sai\\Desktop\\NN_digit_classifier\\Southampton_NN_digit_classifier\\env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Sai\\Desktop\\NN_digit_classifier\\Southampton_NN_digit_classifier\\env\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:317\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    257\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[0;32m    259\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Sai\\Desktop\\NN_digit_classifier\\Southampton_NN_digit_classifier\\env\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:174\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    171\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Sai\\Desktop\\NN_digit_classifier\\Southampton_NN_digit_classifier\\env\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:174\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    171\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Sai\\Desktop\\NN_digit_classifier\\Southampton_NN_digit_classifier\\env\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 142\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32mc:\\Users\\Sai\\Desktop\\NN_digit_classifier\\Southampton_NN_digit_classifier\\env\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:214\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    212\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    213\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected Tensor as element 2 in argument 0, but got int"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, ConcatDataset, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "# Data augmentation and normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((16, 16)),\n",
    "    transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Transform for PIL Images\n",
    "pil_transform = transforms.Compose([\n",
    "    transforms.Resize((16, 16)),\n",
    "    transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Transform for tensors\n",
    "tensor_transform = transforms.Compose([\n",
    "    transforms.Resize((16, 16)),\n",
    "    transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "class CompositeDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert('L')\n",
    "        image = transform(image)\n",
    "        return image, self.labels[idx]\n",
    "\n",
    "def load_composite_data(base_folder):\n",
    "    standard_images, standard_labels = [], []\n",
    "    rotated_images, rotated_labels = [], []\n",
    "    cursive_images, cursive_labels = [], []\n",
    "\n",
    "    for folder in os.listdir(base_folder):\n",
    "        if folder.startswith('composite_images_'):\n",
    "            folder_path = os.path.join(base_folder, folder)\n",
    "            for filename in os.listdir(folder_path):\n",
    "                if filename.endswith('.png'):\n",
    "                    file_path = os.path.join(folder_path, filename)\n",
    "                    digit = int(filename.split('_')[2].split('.')[0])\n",
    "                    \n",
    "                    if 'cursive' in filename:\n",
    "                        cursive_images.append(file_path)\n",
    "                        cursive_labels.append(digit)\n",
    "                    elif 'angle' in filename:\n",
    "                        rotated_images.append(file_path)\n",
    "                        rotated_labels.append(digit)\n",
    "                    else:\n",
    "                        standard_images.append(file_path)\n",
    "                        standard_labels.append(digit)\n",
    "\n",
    "    return (\n",
    "        CompositeDataset(standard_images, standard_labels),\n",
    "        CompositeDataset(rotated_images, rotated_labels),\n",
    "        CompositeDataset(cursive_images, cursive_labels)\n",
    "    )\n",
    "\n",
    "# Modify MNIST loading\n",
    "def load_and_invert_mnist():\n",
    "    mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=pil_transform)\n",
    "    mnist_test = datasets.MNIST(root='./data', train=False, download=True, transform=pil_transform)\n",
    "    \n",
    "    # Invert half of the MNIST data\n",
    "    for dataset in [mnist_train, mnist_test]:\n",
    "        for idx in range(len(dataset.data) // 2):\n",
    "            dataset.data[idx] = 255 - dataset.data[idx]\n",
    "    \n",
    "    return mnist_train, mnist_test\n",
    "\n",
    "# Load MNIST data\n",
    "mnist_train, mnist_test = load_and_invert_mnist()\n",
    "\n",
    "# Ensure experimental data is also [1, 16, 16]\n",
    "exp_train_images = tensor_transform(exp_train_images)\n",
    "val_images = tensor_transform(val_images)\n",
    "\n",
    "# Modify the CompositeDataset __getitem__ method\n",
    "class CompositeDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert('L')\n",
    "        image = pil_transform(image)\n",
    "        return image, self.labels[idx]\n",
    "\n",
    "# Load composite image data\n",
    "composite_standard, composite_rotated, composite_cursive = load_composite_data('composite_image_folders')\n",
    "\n",
    "# Combine all data\n",
    "all_train_data = ConcatDataset([\n",
    "    mnist_train,\n",
    "    mnist_test,\n",
    "    TensorDataset(exp_train_images, exp_train_labels),\n",
    "    TensorDataset(val_images, val_labels),\n",
    "    test_dataset\n",
    "])\n",
    "\n",
    "mnist_train, mnist_test = load_and_invert_mnist()\n",
    "\n",
    "# Load experimental data (keep your existing code for loading exp_train_images, exp_train_labels, etc.)\n",
    "\n",
    "# Load composite image data\n",
    "composite_standard, composite_rotated, composite_cursive = load_composite_data('composite_image_folders')\n",
    "\n",
    "# Use composite images as final validation set\n",
    "final_val_data = ConcatDataset([composite_standard, composite_rotated, composite_cursive])\n",
    "\n",
    "# K-Fold Cross-validation\n",
    "k_folds = 5\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Training function with early stopping\n",
    "def train_model(model, train_loader, val_loader, num_epochs=100, patience=10):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.1)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for val_images, val_labels in val_loader:\n",
    "                val_images, val_labels = val_images.to(device), val_labels.to(device)\n",
    "                outputs_val = model(val_images)\n",
    "                loss_val = criterion(outputs_val, val_labels)\n",
    "                val_loss += loss_val.item()\n",
    "                \n",
    "                _, predicted_val = torch.max(outputs_val.data, 1)\n",
    "                total_val += val_labels.size(0)\n",
    "                correct_val += (predicted_val == val_labels).sum().item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = 100 * correct_val / total_val\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {running_loss/len(train_loader):.4f}, '\n",
    "              f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), \"best_model_early.pth\")\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "    return model\n",
    "\n",
    "# K-Fold Cross-validation training\n",
    "for fold, (train_ids, val_ids) in enumerate(kfold.split(all_train_data)):\n",
    "    print(f'FOLD {fold+1}')\n",
    "    print('--------------------------------')\n",
    "    \n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "    val_subsampler = torch.utils.data.SubsetRandomSampler(val_ids)\n",
    "    \n",
    "    train_loader = DataLoader(all_train_data, batch_size=64, sampler=train_subsampler)\n",
    "    val_loader = DataLoader(all_train_data, batch_size=64, sampler=val_subsampler)\n",
    "    \n",
    "    model = LeNet5_16x16()\n",
    "    train_model(model, train_loader, val_loader)\n",
    "\n",
    "# Load the best model and evaluate on the final validation set\n",
    "best_model = LeNet5_16x16()\n",
    "best_model.load_state_dict(torch.load(\"best_model_done.pth\"))\n",
    "final_val_loader = DataLoader(final_val_data, batch_size=64, shuffle=False)\n",
    "evaluate_lenet(best_model, final_val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
