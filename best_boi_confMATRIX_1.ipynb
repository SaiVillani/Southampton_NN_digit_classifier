{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n",
    "import os\n",
    "import zipfile\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from mnist_skeptic_v9 import skeptic_v9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sai\\AppData\\Local\\Temp\\ipykernel_43612\\3300162512.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(path))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EnsembleModel(\n",
       "  (models): ModuleList(\n",
       "    (0-19): 20 x skeptic_v9(\n",
       "      (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (batchnorm1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (batchnorm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (fc1): Linear(in_features=512, out_features=128, bias=True)\n",
       "      (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 1: Ensemble Model Creation and Prediction\n",
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self, model_paths):\n",
    "        super(EnsembleModel, self).__init__()\n",
    "        self.models = nn.ModuleList([skeptic_v9() for _ in range(len(model_paths))])\n",
    "        for model, path in zip(self.models, model_paths):\n",
    "            model.load_state_dict(torch.load(path))\n",
    "            model.eval()\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = [model(x) for model in self.models]\n",
    "        return torch.stack(outputs).mean(dim=0)\n",
    "\n",
    "def create_ensemble(model_dir='best_boi_models'):\n",
    "    model_paths = [os.path.join(model_dir, f) for f in os.listdir(model_dir) if f.endswith('.pth')]\n",
    "    return EnsembleModel(model_paths)\n",
    "\n",
    "ensemble_model = create_ensemble()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ensemble_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Data Loading Functions\n",
    "\n",
    "def load_mnist_test_data():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((16, 16)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    test_dataset = MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "    return DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "def load_all_experimental_data(test_digits_folder):\n",
    "    train_images = []\n",
    "    train_labels = []\n",
    "    test_images = []\n",
    "    test_labels = []\n",
    "    participant_data = {}\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((16, 16)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,)) #PLEASE CHANGE THIS TO THE TRAINING NORMALIZATION VALUES (0.1307,), (0.3081,) IF YOU WANT TO USE THE TRAINING NORMALIZATION VALUES - ELSE (0.5,), (0.5,) WILL BE USED\n",
    "    ])\n",
    "\n",
    "    for filename in os.listdir(test_digits_folder):\n",
    "        if filename.endswith('.zip') and filename.startswith('experiment_results_participant'):\n",
    "            participant_number = int(filename.split('participant')[1].split('.')[0])\n",
    "            zip_filepath = os.path.join(test_digits_folder, filename)\n",
    "\n",
    "            participant_train_images = []\n",
    "            participant_train_labels = []\n",
    "            participant_test_images = []\n",
    "            participant_test_labels = []\n",
    "\n",
    "            with zipfile.ZipFile(zip_filepath, 'r') as zip_ref:\n",
    "                for img_filename in zip_ref.namelist():\n",
    "                    if img_filename.endswith('.png'):\n",
    "                        with zip_ref.open(img_filename) as file:\n",
    "                            img = Image.open(file).convert('L')  # Convert to grayscale\n",
    "                            img_tensor = transform(img)\n",
    "                            \n",
    "                            digit = int(img_filename.split('_')[0])\n",
    "                            \n",
    "                            if 'composite' in img_filename:\n",
    "                                test_images.append(img_tensor)\n",
    "                                test_labels.append(digit)\n",
    "                                participant_test_images.append(img_tensor)\n",
    "                                participant_test_labels.append(digit)\n",
    "                            else:\n",
    "                                train_images.append(img_tensor)\n",
    "                                train_labels.append(digit)\n",
    "                                participant_train_images.append(img_tensor)\n",
    "                                participant_train_labels.append(digit)\n",
    "\n",
    "            participant_data[participant_number] = {\n",
    "                'train': (torch.stack(participant_train_images), torch.tensor(participant_train_labels)),\n",
    "                'test': (torch.stack(participant_test_images), torch.tensor(participant_test_labels))\n",
    "            }\n",
    "\n",
    "    print(f\"Total training images: {len(train_images)}\")\n",
    "    print(f\"Total test images: {len(test_images)}\")\n",
    "    \n",
    "    for participant, data in participant_data.items():\n",
    "        print(f\"Participant {participant}:\")\n",
    "        print(f\"  Training images: {len(data['train'][0])}\")\n",
    "        print(f\"  Test images: {len(data['test'][0])}\")\n",
    "\n",
    "    return (torch.stack(train_images), torch.tensor(train_labels), \n",
    "            torch.stack(test_images), torch.tensor(test_labels),\n",
    "            participant_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Evaluation Functions\n",
    "\n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    return np.array(all_preds), np.array(all_labels)\n",
    "\n",
    "def plot_confusion_matrix(true_labels, pred_labels, title):\n",
    "    cm = confusion_matrix(true_labels, pred_labels)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.savefig(f'{title.lower().replace(\" \", \"_\")}.png')\n",
    "    plt.close()\n",
    "\n",
    "def analyze_confusion(cm):\n",
    "    n_classes = cm.shape[0]\n",
    "    \n",
    "    # Most confusable pairs\n",
    "    confusable_pairs = []\n",
    "    for i in range(n_classes):\n",
    "        for j in range(i+1, n_classes):\n",
    "            if i != j:\n",
    "                confusion_score = cm[i, j] + cm[j, i]\n",
    "                confusable_pairs.append((i, j, confusion_score))\n",
    "    \n",
    "    confusable_pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    # Most discriminable digits\n",
    "    discriminability = np.diag(cm) / cm.sum(axis=1)\n",
    "    most_discriminable = np.argsort(discriminability)[::-1]\n",
    "    \n",
    "    # Digits the model always guesses (overfitting)\n",
    "    guess_bias = cm.sum(axis=0) / cm.sum()\n",
    "    most_guessed = np.argsort(guess_bias)[::-1]\n",
    "    \n",
    "    return confusable_pairs[:5], most_discriminable[:5], most_guessed[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Main Evaluation Script\n",
    "\n",
    "def main_evaluation():\n",
    "    # Evaluate on MNIST\n",
    "    mnist_loader = load_mnist_test_data()\n",
    "    mnist_preds, mnist_labels = evaluate_model(ensemble_model, mnist_loader)\n",
    "    plot_confusion_matrix(mnist_labels, mnist_preds, \"MNIST Confusion Matrix\")\n",
    "    \n",
    "    # Analyze MNIST results\n",
    "    mnist_cm = confusion_matrix(mnist_labels, mnist_preds)\n",
    "    mnist_confusable, mnist_discriminable, mnist_guessed = analyze_confusion(mnist_cm)\n",
    "    \n",
    "    print(\"MNIST Analysis:\")\n",
    "    print(\"Most confusable pairs:\", mnist_confusable)\n",
    "    print(\"Most discriminable digits:\", mnist_discriminable)\n",
    "    print(\"Most frequently guessed digits:\", mnist_guessed)\n",
    "    \n",
    "    # Evaluate on Experimental Data\n",
    "    exp_data = load_all_experimental_data('path_to_your_experimental_data_folder')\n",
    "    exp_train_images, exp_train_labels, exp_test_images, exp_test_labels, participant_data = exp_data\n",
    "    \n",
    "    exp_dataset = ExperimentalDataset(exp_test_images, exp_test_labels)\n",
    "    exp_loader = DataLoader(exp_dataset, batch_size=64, shuffle=False)\n",
    "    \n",
    "    exp_preds, exp_labels = evaluate_model(ensemble_model, exp_loader)\n",
    "    plot_confusion_matrix(exp_labels, exp_preds, \"Experimental Data Confusion Matrix\")\n",
    "    \n",
    "    # Analyze Experimental results\n",
    "    exp_cm = confusion_matrix(exp_labels, exp_preds)\n",
    "    exp_confusable, exp_discriminable, exp_guessed = analyze_confusion(exp_cm)\n",
    "    \n",
    "    print(\"\\nExperimental Data Analysis:\")\n",
    "    print(\"Most confusable pairs:\", exp_confusable)\n",
    "    print(\"Most discriminable digits:\", exp_discriminable)\n",
    "    print(\"Most frequently guessed digits:\", exp_guessed)\n",
    "    \n",
    "    # Evaluate by participant\n",
    "    participant_accuracies = evaluate_by_participant(ensemble_model, participant_data, device)\n",
    "    print(\"\\nParticipant Accuracies:\", participant_accuracies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST Analysis:\n",
      "Most confusable pairs: [(4, 9, 70), (7, 9, 51), (8, 9, 40), (1, 4, 39), (3, 5, 35)]\n",
      "Most discriminable digits: [0 6 5 1 4]\n",
      "Most frequently guessed digits: [1 9 0 4 7]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'path_to_your_experimental_data_folder'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Run the main evaluation\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmain_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 19\u001b[0m, in \u001b[0;36mmain_evaluation\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMost frequently guessed digits:\u001b[39m\u001b[38;5;124m\"\u001b[39m, mnist_guessed)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Evaluate on Experimental Data\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m exp_data \u001b[38;5;241m=\u001b[39m \u001b[43mload_all_experimental_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpath_to_your_experimental_data_folder\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m exp_train_images, exp_train_labels, exp_test_images, exp_test_labels, participant_data \u001b[38;5;241m=\u001b[39m exp_data\n\u001b[0;32m     22\u001b[0m exp_dataset \u001b[38;5;241m=\u001b[39m ExperimentalDataset(exp_test_images, exp_test_labels)\n",
      "Cell \u001b[1;32mIn[3], line 25\u001b[0m, in \u001b[0;36mload_all_experimental_data\u001b[1;34m(test_digits_folder)\u001b[0m\n\u001b[0;32m     17\u001b[0m participant_data \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     19\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m     20\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m16\u001b[39m)),\n\u001b[0;32m     21\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[0;32m     22\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize((\u001b[38;5;241m0.1307\u001b[39m,), (\u001b[38;5;241m0.3081\u001b[39m,)) \u001b[38;5;66;03m#PLEASE CHANGE THIS TO THE TRAINING NORMALIZATION VALUES (0.1307,), (0.3081,) IF YOU WANT TO USE THE TRAINING NORMALIZATION VALUES - ELSE (0.5,), (0.5,) WILL BE USED\u001b[39;00m\n\u001b[0;32m     23\u001b[0m ])\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_digits_folder\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.zip\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexperiment_results_participant\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     27\u001b[0m         participant_number \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(filename\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparticipant\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'path_to_your_experimental_data_folder'"
     ]
    }
   ],
   "source": [
    "# Run the main evaluation\n",
    "main_evaluation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
