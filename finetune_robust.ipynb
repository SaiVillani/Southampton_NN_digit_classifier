{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import zipfile\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from mnist_skeptic_v9 import skeptic_v9\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompositeDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = []\n",
    "        for participant_data in data:\n",
    "            for item in participant_data:\n",
    "                if isinstance(item, list) and len(item) > 0:\n",
    "                    self.data.extend(item)\n",
    "                else:\n",
    "                    self.data.append(item)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        image = torch.tensor(item['composite'], dtype=torch.float32).view(1, 16, 16)\n",
    "        label = torch.tensor(int(item['true_digit']), dtype=torch.long)\n",
    "        return image, label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        image = torch.tensor(item['composite'], dtype=torch.float32).view(1, 16, 16)\n",
    "        label = torch.tensor(int(item['true_digit']), dtype=torch.long)\n",
    "        return image, label\n",
    "\n",
    "def load_data(zip_path):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        train_data = []\n",
    "        test_data = []\n",
    "        for file in zip_ref.namelist():\n",
    "            if file.startswith('training_data/training_set/'):\n",
    "                with zip_ref.open(file) as f:\n",
    "                    train_data.append(json.load(f))\n",
    "            elif file.startswith('training_data/test_set/'):\n",
    "                with zip_ref.open(file) as f:\n",
    "                    test_data.append(json.load(f))\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self, models):\n",
    "        super(EnsembleModel, self).__init__()\n",
    "        self.models = nn.ModuleList(models)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        outputs = [model(x) for model in self.models]\n",
    "        return torch.mean(torch.stack(outputs), dim=0)\n",
    "\n",
    "def train_epoch(ensemble, dataloader, criterion, optimizer, device):\n",
    "    ensemble.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = ensemble(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    return total_loss / len(dataloader), 100 * correct / total\n",
    "\n",
    "def evaluate(ensemble, dataloader, criterion, device, show_matrix=False):\n",
    "    ensemble.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = ensemble(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    if show_matrix:\n",
    "        cm = confusion_matrix(all_labels, all_predictions)\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.savefig('confusion_matrix_ensemble.png')\n",
    "        plt.close()\n",
    "    \n",
    "    return total_loss / len(dataloader), accuracy, np.array(all_labels), np.array(all_predictions)\n",
    "\n",
    "def analyze_confusion_matrix(cm):\n",
    "    most_confusable_digits = {}\n",
    "    most_discriminable_digits = {}\n",
    "    \n",
    "    for i in range(cm.shape[0]):\n",
    "        row_sum = cm[i].sum()\n",
    "        \n",
    "        if row_sum > 0:\n",
    "            confusions = cm[i] / row_sum\n",
    "            \n",
    "            # Most confusable digit (highest off-diagonal value)\n",
    "            most_confusable_index = np.argmax(confusions[np.arange(len(confusions)) != i])\n",
    "            most_confusable_digits[i] = most_confusable_index\n",
    "            \n",
    "            # Most discriminable digit (lowest off-diagonal value)\n",
    "            most_discriminable_index = np.argmin(confusions[np.arange(len(confusions)) != i])\n",
    "            most_discriminable_digits[i] = most_discriminable_index\n",
    "    \n",
    "    return most_confusable_digits, most_discriminable_digits\n",
    "\n",
    "\n",
    "def finetune_ensemble(ensemble, train_loader, val_loader, num_epochs=50, patience=5):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    ensemble.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(ensemble.parameters(), lr=0.0001)\n",
    "    \n",
    "    best_val_acc = 0\n",
    "    epochs_no_improve = 0\n",
    "    \n",
    "    # Create a new directory for finetuned models\n",
    "    os.makedirs('finetuned_models', exist_ok=True)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = train_epoch(ensemble, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_acc, _, _ = evaluate(ensemble, val_loader, criterion, device)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(ensemble.state_dict(), os.path.join('finetuned_models', f'best_finetuned_ensemble_epoch_{epoch+1}.pth'))\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        \n",
    "        if epochs_no_improve == patience:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "    \n",
    "    # Final evaluation\n",
    "    best_model_path = os.path.join('finetuned_models', f'best_finetuned_ensemble_epoch_{epoch-epochs_no_improve+1}.pth')\n",
    "    ensemble.load_state_dict(torch.load(best_model_path))\n",
    "    _, final_accuracy, all_labels, all_predictions = evaluate(ensemble, val_loader, criterion, device, show_matrix=True)\n",
    "    \n",
    "    cm = confusion_matrix(all_labels, all_predictions)\n",
    "    most_confusable_digits, most_discriminable_digits = analyze_confusion_matrix(cm)\n",
    "    \n",
    "    print(f'Final Ensemble Accuracy: {final_accuracy:.2f}%')\n",
    "    print(f'Most Confusable Digits: {most_confusable_digits}')\n",
    "    print(f'Most Discriminable Digits: {most_discriminable_digits}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sai\\AppData\\Local\\Temp\\ipykernel_55960\\2415699396.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f'saved_models/skeptic_v10/{model_name}.pth'))\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'composite'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinetuning completed. Best model saved as \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_finetuned_ensemble.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 27\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 22\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m ensemble \u001b[38;5;241m=\u001b[39m EnsembleModel(models)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Finetune the ensemble\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[43mfinetune_ensemble\u001b[49m\u001b[43m(\u001b[49m\u001b[43mensemble\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinetuning completed. Best model saved as \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_finetuned_ensemble.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 96\u001b[0m, in \u001b[0;36mfinetune_ensemble\u001b[1;34m(ensemble, train_loader, val_loader, num_epochs, patience)\u001b[0m\n\u001b[0;32m     93\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinetuned_models\u001b[39m\u001b[38;5;124m'\u001b[39m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m---> 96\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mensemble\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m     val_loss, val_acc, _, _ \u001b[38;5;241m=\u001b[39m evaluate(ensemble, val_loader, criterion, device)\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 15\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(ensemble, dataloader, criterion, optimizer, device)\u001b[0m\n\u001b[0;32m     13\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     14\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m     16\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     17\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\Sai\\Desktop\\NN_digit_classifier\\Southampton_NN_digit_classifier\\env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Sai\\Desktop\\NN_digit_classifier\\Southampton_NN_digit_classifier\\env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Sai\\Desktop\\NN_digit_classifier\\Southampton_NN_digit_classifier\\env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Sai\\Desktop\\NN_digit_classifier\\Southampton_NN_digit_classifier\\env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[7], line 25\u001b[0m, in \u001b[0;36mCompositeDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m     24\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[idx]\n\u001b[1;32m---> 25\u001b[0m     image \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcomposite\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m16\u001b[39m)\n\u001b[0;32m     26\u001b[0m     label \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mint\u001b[39m(item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrue_digit\u001b[39m\u001b[38;5;124m'\u001b[39m]), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m image, label\n",
      "\u001b[1;31mKeyError\u001b[0m: 'composite'"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Load data\n",
    "    train_data, test_data = load_data('training_data.zip')\n",
    "    \n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = CompositeDataset(train_data)\n",
    "    test_dataset = CompositeDataset(test_data)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    # Load ensemble model\n",
    "    models = []\n",
    "    for i in range(ord('a'), ord('u')):\n",
    "        model_name = f'skeptic_v10_{chr(i)}'\n",
    "        model = skeptic_v9()\n",
    "        model.load_state_dict(torch.load(f'saved_models/skeptic_v10/{model_name}.pth'))\n",
    "        models.append(model)\n",
    "    \n",
    "    ensemble = EnsembleModel(models)\n",
    "    \n",
    "    # Finetune the ensemble\n",
    "    finetune_ensemble(ensemble, train_loader, test_loader, num_epochs=20)\n",
    "    \n",
    "    print(\"Finetuning completed. Best model saved as 'best_finetuned_ensemble.pth'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sai\\AppData\\Local\\Temp\\ipykernel_55960\\2051336337.py:213: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f'saved_models/skeptic_v10/{model_name}.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "Train Loss: 2.2668, Train Acc: 14.07%\n",
      "Val Loss: 2.2819, Val Acc: 16.51%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import zipfile\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from mnist_skeptic_v9 import skeptic_v9\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import base64\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "class CompositeDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = []\n",
    "        for participant_data in data:\n",
    "            self.data.extend(participant_data)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        image = self.base64_to_image(item['composite'])\n",
    "        label = torch.tensor(int(item['true_digit']), dtype=torch.long)\n",
    "        return image, label\n",
    "    \n",
    "    def base64_to_image(self, base64_string):\n",
    "        img_data = base64.b64decode(base64_string)\n",
    "        img = Image.open(io.BytesIO(img_data))\n",
    "        img_array = np.array(img, dtype=np.float32) / 255.0  # Normalize to [0, 1]\n",
    "        return torch.tensor(img_array).unsqueeze(0)  # Add channel dimension\n",
    "\n",
    "class SelectionDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = []\n",
    "        for participant_data in data:\n",
    "            for trial_data in participant_data:\n",
    "                self.data.extend(trial_data)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        image = self.base64_to_image(item['selected_image'])\n",
    "        label = torch.tensor(int(item['true_digit']), dtype=torch.long)\n",
    "        return image, label\n",
    "    \n",
    "    def base64_to_image(self, base64_string):\n",
    "        img_data = base64.b64decode(base64_string)\n",
    "        img = Image.open(io.BytesIO(img_data))\n",
    "        img_array = np.array(img, dtype=np.float32) / 255.0  # Normalize to [0, 1]\n",
    "        return torch.tensor(img_array).unsqueeze(0)  # Add channel dimension\n",
    "\n",
    "def load_data(zip_path):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        train_data = []\n",
    "        test_data = []\n",
    "        for file in zip_ref.namelist():\n",
    "            if file.startswith('training_data/training_set/'):\n",
    "                with zip_ref.open(file) as f:\n",
    "                    train_data.append(json.load(f))\n",
    "            elif file.startswith('training_data/test_set/'):\n",
    "                with zip_ref.open(file) as f:\n",
    "                    test_data.append(json.load(f))\n",
    "    return train_data, test_data\n",
    "\n",
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self, models):\n",
    "        super(EnsembleModel, self).__init__()\n",
    "        self.models = nn.ModuleList(models)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        outputs = [model(x) for model in self.models]\n",
    "        return torch.mean(torch.stack(outputs), dim=0)\n",
    "\n",
    "def train_epoch(ensemble, dataloader, criterion, optimizer, device):\n",
    "    ensemble.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = ensemble(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    return total_loss / len(dataloader), 100 * correct / total\n",
    "\n",
    "def evaluate(ensemble, dataloader, criterion, device, show_matrix=False):\n",
    "    ensemble.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = ensemble(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    if show_matrix:\n",
    "        cm = confusion_matrix(all_labels, all_predictions)\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.savefig('confusion_matrix_ensemble.png')\n",
    "        plt.close()\n",
    "    \n",
    "    return total_loss / len(dataloader), accuracy, np.array(all_labels), np.array(all_predictions)\n",
    "\n",
    "def analyze_confusion_matrix(cm):\n",
    "    most_confusable_digits = {}\n",
    "    most_discriminable_digits = {}\n",
    "    \n",
    "    for i in range(cm.shape[0]):\n",
    "        row_sum = cm[i].sum()\n",
    "        \n",
    "        if row_sum > 0:\n",
    "            confusions = cm[i] / row_sum\n",
    "            \n",
    "            # Most confusable digit (highest off-diagonal value)\n",
    "            most_confusable_index = np.argmax(confusions[np.arange(len(confusions)) != i])\n",
    "            most_confusable_digits[i] = most_confusable_index\n",
    "            \n",
    "            # Most discriminable digit (lowest off-diagonal value)\n",
    "            most_discriminable_index = np.argmin(confusions[np.arange(len(confusions)) != i])\n",
    "            most_discriminable_digits[i] = most_discriminable_index\n",
    "    \n",
    "    return most_confusable_digits, most_discriminable_digits\n",
    "\n",
    "def finetune_ensemble(ensemble, train_loader, val_loader, num_epochs=50, patience=5):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    ensemble.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(ensemble.parameters(), lr=0.0001)\n",
    "    \n",
    "    best_val_acc = 0\n",
    "    epochs_no_improve = 0\n",
    "    \n",
    "    # Create a new directory for finetuned models\n",
    "    os.makedirs('finetuned_models', exist_ok=True)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = train_epoch(ensemble, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_acc, _, _ = evaluate(ensemble, val_loader, criterion, device)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(ensemble.state_dict(), os.path.join('finetuned_models', f'best_finetuned_ensemble_epoch_{epoch+1}.pth'))\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        \n",
    "        if epochs_no_improve == patience:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "    \n",
    "    # Final evaluation\n",
    "    best_model_path = os.path.join('finetuned_models', f'best_finetuned_ensemble_epoch_{epoch-epochs_no_improve+1}.pth')\n",
    "    ensemble.load_state_dict(torch.load(best_model_path))\n",
    "    _, final_accuracy, all_labels, all_predictions = evaluate(ensemble, val_loader, criterion, device, show_matrix=True)\n",
    "    \n",
    "    cm = confusion_matrix(all_labels, all_predictions)\n",
    "    most_confusable_digits, most_discriminable_digits = analyze_confusion_matrix(cm)\n",
    "    \n",
    "    print(f'Final Ensemble Accuracy: {final_accuracy:.2f}%')\n",
    "    print(f'Most Confusable Digits: {most_confusable_digits}')\n",
    "    print(f'Most Discriminable Digits: {most_discriminable_digits}')\n",
    "\n",
    "def main():\n",
    "    # Load data\n",
    "    train_data, test_data = load_data('training_data.zip')\n",
    "    \n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = SelectionDataset(train_data)\n",
    "    test_dataset = CompositeDataset(test_data)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    # Load ensemble model\n",
    "    models = []\n",
    "    for i in range(ord('a'), ord('u')):\n",
    "        model_name = f'skeptic_v10_{chr(i)}'\n",
    "        model = skeptic_v9()\n",
    "        model.load_state_dict(torch.load(f'saved_models/skeptic_v10/{model_name}.pth'))\n",
    "        models.append(model)\n",
    "    \n",
    "    ensemble = EnsembleModel(models)\n",
    "    \n",
    "    # Finetune the ensemble\n",
    "    finetune_ensemble(ensemble, train_loader, test_loader, num_epochs=20)\n",
    "    \n",
    "    print(\"Finetuning completed. Best model saved in 'finetuned_models' directory.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
