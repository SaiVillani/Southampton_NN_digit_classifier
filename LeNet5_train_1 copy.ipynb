{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset, ConcatDataset\n",
    "from torchvision import transforms, datasets\n",
    "from PIL import Image\n",
    "import os\n",
    "import zipfile\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "import random\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixedDataset(Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if isinstance(self.dataset, TensorDataset):\n",
    "            x, y = self.dataset[index]\n",
    "        else:\n",
    "            x, y = self.dataset[index]\n",
    "        \n",
    "        # Ensure x is a tensor\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x)\n",
    "        \n",
    "        # Ensure y is a tensor\n",
    "        if not isinstance(y, torch.Tensor):\n",
    "            y = torch.tensor(y)\n",
    "        \n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "def add_noise(image, noise_factor=0.5):\n",
    "    noise = torch.randn_like(image) * noise_factor\n",
    "    noisy_image = image + noise\n",
    "    return torch.clamp(noisy_image, 0., 1.)\n",
    "\n",
    "# Load experimental data\n",
    "def load_all_experimental_data(test_digits_folder):\n",
    "    train_images = []\n",
    "    train_labels = []\n",
    "    test_images = []\n",
    "    test_labels = []\n",
    "    participant_data = {}\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((16, 16)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "\n",
    "    for filename in os.listdir(test_digits_folder):\n",
    "        if filename.endswith('.zip') and filename.startswith('experiment_results_participant'):\n",
    "            participant_number = int(filename.split('participant')[1].split('.')[0])\n",
    "            zip_filepath = os.path.join(test_digits_folder, filename)\n",
    "\n",
    "            participant_train_images = []\n",
    "            participant_train_labels = []\n",
    "            participant_test_images = []\n",
    "            participant_test_labels = []\n",
    "\n",
    "            with zipfile.ZipFile(zip_filepath, 'r') as zip_ref:\n",
    "                for img_filename in zip_ref.namelist():\n",
    "                    if img_filename.endswith('.png'):\n",
    "                        with zip_ref.open(img_filename) as file:\n",
    "                            img = Image.open(file).convert('L')\n",
    "                            img_tensor = transform(img)\n",
    "                            \n",
    "                            digit = int(img_filename.split('_')[0])\n",
    "                            \n",
    "                            if 'composite' in img_filename:\n",
    "                                test_images.append(img_tensor)\n",
    "                                test_labels.append(digit)\n",
    "                                participant_test_images.append(img_tensor)\n",
    "                                participant_test_labels.append(digit)\n",
    "                            else:\n",
    "                                train_images.append(img_tensor)\n",
    "                                train_labels.append(digit)\n",
    "                                participant_train_images.append(img_tensor)\n",
    "                                participant_train_labels.append(digit)\n",
    "\n",
    "            participant_data[participant_number] = {\n",
    "                'train': (torch.stack(participant_train_images), torch.tensor(participant_train_labels)),\n",
    "                'test': (torch.stack(participant_test_images), torch.tensor(participant_test_labels))\n",
    "            }\n",
    "\n",
    "    return (torch.stack(train_images), torch.tensor(train_labels), \n",
    "            torch.stack(test_images), torch.tensor(test_labels),\n",
    "            participant_data)\n",
    "\n",
    "# Load and augment MNIST data\n",
    "def load_augmented_mnist():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((16, 16)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    mnist_test = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "    noisy_transform = transforms.Compose([\n",
    "        transforms.Lambda(lambda x: add_noise(x, noise_factor=0.5)),\n",
    "    ])\n",
    "    invert_transform = transforms.Compose([\n",
    "        transforms.Lambda(lambda x: 1 - x),\n",
    "    ])\n",
    "\n",
    "    noisy_mnist_train = MixedDataset(mnist_train, noisy_transform)\n",
    "    noisy_mnist_test = MixedDataset(mnist_test, noisy_transform)\n",
    "    inverted_mnist_train = MixedDataset(mnist_train, invert_transform)\n",
    "    inverted_mnist_test = MixedDataset(mnist_test, invert_transform)\n",
    "\n",
    "    combined_train = ConcatDataset([mnist_train, noisy_mnist_train, inverted_mnist_train])\n",
    "    combined_test = ConcatDataset([mnist_test, noisy_mnist_test, inverted_mnist_test])\n",
    "\n",
    "    return combined_train, combined_test\n",
    "\n",
    "# Model Definition\n",
    "class LeNet5_16x16(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(LeNet5_16x16, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, kernel_size=5),\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(6, 16, kernel_size=5),\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.fc1 = nn.Linear(16 * 1 * 1, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "# Training Function\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, patience):\n",
    "    model = model.to(device)\n",
    "    best_val_acc = 0\n",
    "    epochs_no_improve = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss:.4f}')\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "        with torch.no_grad():\n",
    "            for val_images, val_labels in val_loader:\n",
    "                val_images, val_labels = val_images.to(device), val_labels.to(device)\n",
    "                outputs_val = model(val_images)\n",
    "                _, predicted_val = torch.max(outputs_val.data, 1)\n",
    "                total_val += val_labels.size(0)\n",
    "                correct_val += (predicted_val == val_labels).sum().item()\n",
    "        \n",
    "        val_acc = 100 * correct_val / total_val\n",
    "        print(f'Validation Accuracy: {val_acc:.2f}%')\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve == patience:\n",
    "                print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                break\n",
    "    \n",
    "    return model\n",
    "\n",
    "# K-Fold Cross Validation\n",
    "def k_fold_cross_validation(dataset, num_folds=5, num_epochs=50, patience=5):\n",
    "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "    fold_results = []\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(range(len(dataset)))):\n",
    "        print(f\"Fold {fold + 1}/{num_folds}\")\n",
    "        train_subset = torch.utils.data.Subset(dataset, train_index)\n",
    "        val_subset = torch.utils.data.Subset(dataset, val_index)\n",
    "\n",
    "        train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
    "        val_loader = DataLoader(val_subset, batch_size=32, shuffle=False)\n",
    "\n",
    "        model = LeNet5_16x16(num_classes=10).to(device)\n",
    "        \n",
    "        # Calculate class weights for weighted cross-entropy\n",
    "        labels = torch.tensor([dataset[i][1] for i in train_index])\n",
    "        class_counts = torch.bincount(labels)\n",
    "        class_weights = 1. / class_counts.float()\n",
    "        class_weights = class_weights / class_weights.sum()\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
    "        \n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "        model = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, patience)\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        model.eval()\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "        with torch.no_grad():\n",
    "            for val_images, val_labels in val_loader:\n",
    "                val_images, val_labels = val_images.to(device), val_labels.to(device)\n",
    "                outputs_val = model(val_images)\n",
    "                _, predicted_val = torch.max(outputs_val.data, 1)\n",
    "                total_val += val_labels.size(0)\n",
    "                correct_val += (predicted_val == val_labels).sum().item()\n",
    "        \n",
    "        val_accuracy = 100 * correct_val / total_val\n",
    "        fold_results.append(val_accuracy)\n",
    "        print(f'Fold {fold + 1} Validation Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "    return fold_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n",
      "Epoch [1/50], Loss: 6526.6644\n",
      "Validation Accuracy: 70.36%\n",
      "Epoch [2/50], Loss: 4811.8453\n",
      "Validation Accuracy: 72.54%\n",
      "Epoch [3/50], Loss: 4482.2102\n",
      "Validation Accuracy: 73.67%\n",
      "Epoch [4/50], Loss: 4261.4427\n",
      "Validation Accuracy: 74.01%\n",
      "Epoch [5/50], Loss: 4113.2235\n",
      "Validation Accuracy: 74.63%\n",
      "Epoch [6/50], Loss: 4006.0122\n",
      "Validation Accuracy: 74.87%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Load experimental data\n",
    "    exp_train_images, exp_train_labels, exp_test_images, exp_test_labels, participant_data = load_all_experimental_data('test_digits')\n",
    "    exp_dataset = TensorDataset(exp_train_images, exp_train_labels)\n",
    "\n",
    "    # Load augmented MNIST data\n",
    "    mnist_train, mnist_test = load_augmented_mnist()\n",
    "\n",
    "    # Combine experimental data with augmented MNIST data\n",
    "    combined_dataset = ConcatDataset([MixedDataset(exp_dataset), MixedDataset(mnist_train)])\n",
    "\n",
    "    # Perform K-Fold Cross Validation\n",
    "    fold_results = k_fold_cross_validation(combined_dataset, num_folds=5, num_epochs=50, patience=5)\n",
    "    print(f\"Average Validation Accuracy across folds: {np.mean(fold_results):.2f}%\")\n",
    "\n",
    "    # Train final model on all data\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    final_train_loader = DataLoader(combined_dataset, batch_size=32, shuffle=True)\n",
    "    final_val_loader = DataLoader(MixedDataset(mnist_test), batch_size=32, shuffle=False)\n",
    "\n",
    "    final_model = LeNet5_16x16(num_classes=10).to(device)\n",
    "    \n",
    "    # Calculate class weights for the final model\n",
    "    all_labels = []\n",
    "    for dataset in combined_dataset.datasets:\n",
    "        if isinstance(dataset, TensorDataset):\n",
    "            all_labels.extend(dataset.tensors[1].tolist())\n",
    "        else:\n",
    "            all_labels.extend([label for _, label in dataset])\n",
    "    \n",
    "    labels = torch.tensor(all_labels)\n",
    "    class_counts = torch.bincount(labels)\n",
    "    class_weights = 1. / class_counts.float()\n",
    "    class_weights = class_weights / class_weights.sum()\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
    "    optimizer = optim.Adam(final_model.parameters(), lr=0.001)\n",
    "\n",
    "    final_model = train_model(final_model, final_train_loader, final_val_loader, criterion, optimizer, num_epochs=50, patience=5)\n",
    "\n",
    "    # Save the final model\n",
    "    torch.save(final_model.state_dict(), \"lenet5_trained_model_heavy.pth\")\n",
    "    print(\"Final model saved as lenet5_trained_model_heavy.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
