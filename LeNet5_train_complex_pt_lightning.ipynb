{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset, ConcatDataset\n",
    "from torchvision import transforms, datasets\n",
    "from PIL import Image\n",
    "import os\n",
    "import zipfile\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional data augmentation techniques\n",
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0., std=1.):\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "        \n",
    "    def __call__(self, tensor):\n",
    "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load experimental data\n",
    "def load_all_experimental_data(test_digits_folder):\n",
    "    train_images = []\n",
    "    train_labels = []\n",
    "    test_images = []\n",
    "    test_labels = []\n",
    "    participant_data = {}\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((16, 16)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "\n",
    "    for filename in os.listdir(test_digits_folder):\n",
    "        if filename.endswith('.zip') and filename.startswith('experiment_results_participant'):\n",
    "            participant_number = int(filename.split('participant')[1].split('.')[0])\n",
    "            zip_filepath = os.path.join(test_digits_folder, filename)\n",
    "\n",
    "            participant_train_images = []\n",
    "            participant_train_labels = []\n",
    "            participant_test_images = []\n",
    "            participant_test_labels = []\n",
    "\n",
    "            with zipfile.ZipFile(zip_filepath, 'r') as zip_ref:\n",
    "                for img_filename in zip_ref.namelist():\n",
    "                    if img_filename.endswith('.png'):\n",
    "                        with zip_ref.open(img_filename) as file:\n",
    "                            img = Image.open(file).convert('L')\n",
    "                            img_tensor = transform(img)\n",
    "                            \n",
    "                            digit = int(img_filename.split('_')[0])\n",
    "                            \n",
    "                            if 'composite' in img_filename:\n",
    "                                test_images.append(img_tensor)\n",
    "                                test_labels.append(digit)\n",
    "                                participant_test_images.append(img_tensor)\n",
    "                                participant_test_labels.append(digit)\n",
    "                            else:\n",
    "                                train_images.append(img_tensor)\n",
    "                                train_labels.append(digit)\n",
    "                                participant_train_images.append(img_tensor)\n",
    "                                participant_train_labels.append(digit)\n",
    "\n",
    "            participant_data[participant_number] = {\n",
    "                'train': (torch.stack(participant_train_images), torch.tensor(participant_train_labels)),\n",
    "                'test': (torch.stack(participant_test_images), torch.tensor(participant_test_labels))\n",
    "            }\n",
    "\n",
    "    return (torch.stack(train_images), torch.tensor(train_labels), \n",
    "            torch.stack(test_images), torch.tensor(test_labels),\n",
    "            participant_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_augmented_mnist():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((16, 16)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.RandomApply([\n",
    "            transforms.RandomRotation(10),\n",
    "            transforms.RandomAffine(0, translate=(0.1, 0.1)),\n",
    "            AddGaussianNoise(0., 0.05),\n",
    "        ], p=0.5),\n",
    "    ])\n",
    "    \n",
    "    invert_transform = transforms.Compose([\n",
    "        transform,\n",
    "        transforms.Lambda(lambda x: 1 - x),\n",
    "    ])\n",
    "    \n",
    "    mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    mnist_test = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "    \n",
    "    inverted_mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=invert_transform)\n",
    "    inverted_mnist_test = datasets.MNIST(root='./data', train=False, download=True, transform=invert_transform)\n",
    "    \n",
    "    combined_train = ConcatDataset([mnist_train, inverted_mnist_train])\n",
    "    combined_test = ConcatDataset([mnist_test, inverted_mnist_test])\n",
    "    \n",
    "    return combined_train, combined_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixedDataset(Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if isinstance(self.dataset, TensorDataset):\n",
    "            x, y = self.dataset[index]\n",
    "        else:\n",
    "            x, y = self.dataset[index]\n",
    "        \n",
    "        # Ensure x is a tensor\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x)\n",
    "        \n",
    "        # Ensure y is a tensor\n",
    "        if not isinstance(y, torch.Tensor):\n",
    "            y = torch.tensor(y)\n",
    "        \n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5_16x16(pl.LightningModule):\n",
    "    def __init__(self, num_classes=10, learning_rate=0.001):\n",
    "        super(LeNet5_16x16, self).__init__()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, kernel_size=5),\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(6, 16, kernel_size=5),\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.fc1 = nn.Linear(16 * 1 * 1, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.CrossEntropyLoss()(y_hat, y)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.CrossEntropyLoss()(y_hat, y)\n",
    "        acc = (y_hat.argmax(dim=1) == y).float().mean()\n",
    "        self.log('val_loss', loss)\n",
    "        self.log('val_acc', acc)\n",
    "        return {'val_loss': loss, 'val_acc': acc}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, exp_data_path, batch_size=64):\n",
    "        super().__init__()\n",
    "        self.exp_data_path = exp_data_path\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        exp_train_images, exp_train_labels, self.exp_test_images, self.exp_test_labels, _ = load_all_experimental_data(self.exp_data_path)\n",
    "        exp_dataset = TensorDataset(exp_train_images, exp_train_labels)\n",
    "        mnist_train, mnist_test = load_augmented_mnist()\n",
    "        self.combined_dataset = ConcatDataset([MixedDataset(exp_dataset), MixedDataset(mnist_train)])\n",
    "        self.mnist_test = MixedDataset(mnist_test)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.combined_dataset, batch_size=self.batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.mnist_test, batch_size=self.batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        exp_test_dataset = TensorDataset(self.exp_test_images, self.exp_test_labels)\n",
    "        return DataLoader(exp_test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "def train_ensemble(num_models=25, num_epochs=30, patience=5):\n",
    "    ensemble = []\n",
    "    data_module = EnsembleDataModule('test_digits')\n",
    "\n",
    "    for i in range(num_models):\n",
    "        print(f\"Training model {i+1}/{num_models}\")\n",
    "\n",
    "        model = LeNet5_16x16()\n",
    "        early_stop_callback = EarlyStopping(monitor='val_loss', patience=patience)\n",
    "        trainer = pl.Trainer(max_epochs=num_epochs, callbacks=[early_stop_callback], gpus=1 if torch.cuda.is_available() else 0)\n",
    "        \n",
    "        trainer.fit(model, data_module)\n",
    "        \n",
    "        ensemble.append(model)\n",
    "        \n",
    "        # Save the model\n",
    "        torch.save(model.state_dict(), f\"lenet5_trained_model_ensemble_{i+1}.pth\")\n",
    "        print(f\"Model {i+1} saved as lenet5_trained_model_ensemble_{i+1}.pth\")\n",
    "\n",
    "    return ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, patience):\n",
    "    model = model.to(device)\n",
    "    best_val_acc = 0\n",
    "    epochs_no_improve = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss:.4f}')\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "        with torch.no_grad():\n",
    "            for val_images, val_labels in val_loader:\n",
    "                val_images, val_labels = val_images.to(device), val_labels.to(device)\n",
    "                outputs_val = model(val_images)\n",
    "                _, predicted_val = torch.max(outputs_val.data, 1)\n",
    "                total_val += val_labels.size(0)\n",
    "                correct_val += (predicted_val == val_labels).sum().item()\n",
    "        \n",
    "        val_acc = 100 * correct_val / total_val\n",
    "        print(f'Validation Accuracy: {val_acc:.2f}%')\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve == patience:\n",
    "                print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                break\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold Cross Validation\n",
    "def k_fold_cross_validation(dataset, num_folds=5, num_epochs=50, patience=5):\n",
    "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "    fold_results = []\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(range(len(dataset)))):\n",
    "        print(f\"Fold {fold + 1}/{num_folds}\")\n",
    "        train_subset = torch.utils.data.Subset(dataset, train_index)\n",
    "        val_subset = torch.utils.data.Subset(dataset, val_index)\n",
    "\n",
    "        train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
    "        val_loader = DataLoader(val_subset, batch_size=32, shuffle=False)\n",
    "\n",
    "        model = LeNet5_16x16(num_classes=10).to(device)\n",
    "        \n",
    "        # Calculate class weights for weighted cross-entropy\n",
    "        labels = torch.tensor([dataset[i][1] for i in train_index])\n",
    "        class_counts = torch.bincount(labels)\n",
    "        class_weights = 1. / class_counts.float()\n",
    "        class_weights = class_weights / class_weights.sum()\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
    "        \n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "        model = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, patience)\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        model.eval()\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "        with torch.no_grad():\n",
    "            for val_images, val_labels in val_loader:\n",
    "                val_images, val_labels = val_images.to(device), val_labels.to(device)\n",
    "                outputs_val = model(val_images)\n",
    "                _, predicted_val = torch.max(outputs_val.data, 1)\n",
    "                total_val += val_labels.size(0)\n",
    "                correct_val += (predicted_val == val_labels).sum().item()\n",
    "        \n",
    "        val_accuracy = 100 * correct_val / total_val\n",
    "        fold_results.append(val_accuracy)\n",
    "        print(f'Fold {fold + 1} Validation Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "    return fold_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 1/25\n",
      "Fold 1/3\n",
      "Epoch [1/30], Loss: 5878.8590\n",
      "Validation Accuracy: 58.77%\n",
      "Epoch [2/30], Loss: 3571.6743\n",
      "Validation Accuracy: 68.27%\n",
      "Epoch [3/30], Loss: 3150.5706\n",
      "Validation Accuracy: 69.76%\n",
      "Epoch [4/30], Loss: 3000.6706\n",
      "Validation Accuracy: 71.15%\n",
      "Epoch [5/30], Loss: 2892.7960\n",
      "Validation Accuracy: 71.96%\n",
      "Epoch [6/30], Loss: 2820.2248\n",
      "Validation Accuracy: 72.67%\n",
      "Epoch [7/30], Loss: 2764.6476\n",
      "Validation Accuracy: 72.27%\n",
      "Epoch [8/30], Loss: 2719.3309\n",
      "Validation Accuracy: 72.80%\n",
      "Epoch [9/30], Loss: 2671.9581\n",
      "Validation Accuracy: 72.93%\n",
      "Epoch [10/30], Loss: 2638.2961\n",
      "Validation Accuracy: 73.25%\n",
      "Epoch [11/30], Loss: 2615.6605\n",
      "Validation Accuracy: 73.64%\n",
      "Epoch [12/30], Loss: 2585.4855\n",
      "Validation Accuracy: 73.33%\n",
      "Epoch [13/30], Loss: 2556.4389\n",
      "Validation Accuracy: 73.23%\n",
      "Epoch [14/30], Loss: 2550.7736\n",
      "Validation Accuracy: 74.05%\n",
      "Epoch [15/30], Loss: 2518.8984\n",
      "Validation Accuracy: 73.77%\n",
      "Epoch [16/30], Loss: 2501.6597\n",
      "Validation Accuracy: 74.07%\n",
      "Epoch [17/30], Loss: 2494.4732\n",
      "Validation Accuracy: 73.75%\n",
      "Epoch [18/30], Loss: 2482.9537\n",
      "Validation Accuracy: 73.99%\n",
      "Epoch [19/30], Loss: 2472.7466\n",
      "Validation Accuracy: 74.80%\n",
      "Epoch [20/30], Loss: 2458.1131\n",
      "Validation Accuracy: 74.89%\n",
      "Epoch [21/30], Loss: 2450.1190\n",
      "Validation Accuracy: 74.36%\n",
      "Epoch [22/30], Loss: 2435.0286\n",
      "Validation Accuracy: 74.70%\n",
      "Epoch [23/30], Loss: 2428.2398\n",
      "Validation Accuracy: 74.65%\n",
      "Epoch [24/30], Loss: 2422.1685\n",
      "Validation Accuracy: 74.85%\n",
      "Epoch [25/30], Loss: 2416.6299\n",
      "Validation Accuracy: 74.76%\n",
      "Early stopping triggered after 25 epochs\n",
      "Fold 1 Validation Accuracy: 74.80%\n",
      "Fold 2/3\n",
      "Epoch [1/30], Loss: 5354.0244\n",
      "Validation Accuracy: 55.97%\n",
      "Epoch [2/30], Loss: 3743.9521\n",
      "Validation Accuracy: 66.24%\n",
      "Epoch [3/30], Loss: 3204.3146\n",
      "Validation Accuracy: 66.59%\n",
      "Epoch [4/30], Loss: 2968.5545\n",
      "Validation Accuracy: 70.04%\n",
      "Epoch [5/30], Loss: 2831.7064\n",
      "Validation Accuracy: 71.37%\n",
      "Epoch [6/30], Loss: 2742.4061\n",
      "Validation Accuracy: 71.51%\n",
      "Epoch [7/30], Loss: 2661.9692\n",
      "Validation Accuracy: 71.93%\n",
      "Epoch [8/30], Loss: 2610.4814\n",
      "Validation Accuracy: 72.70%\n",
      "Epoch [9/30], Loss: 2553.5603\n",
      "Validation Accuracy: 72.85%\n",
      "Epoch [10/30], Loss: 2518.2079\n",
      "Validation Accuracy: 72.56%\n",
      "Epoch [11/30], Loss: 2483.8494\n",
      "Validation Accuracy: 73.05%\n",
      "Epoch [12/30], Loss: 2445.5396\n",
      "Validation Accuracy: 73.81%\n",
      "Epoch [13/30], Loss: 2419.1500\n",
      "Validation Accuracy: 73.30%\n",
      "Epoch [14/30], Loss: 2398.1287\n",
      "Validation Accuracy: 73.14%\n",
      "Epoch [15/30], Loss: 2377.5730\n",
      "Validation Accuracy: 73.75%\n",
      "Epoch [16/30], Loss: 2358.1873\n",
      "Validation Accuracy: 73.41%\n",
      "Epoch [17/30], Loss: 2338.4428\n",
      "Validation Accuracy: 74.74%\n",
      "Epoch [18/30], Loss: 2331.4221\n",
      "Validation Accuracy: 74.46%\n",
      "Epoch [19/30], Loss: 2316.2241\n",
      "Validation Accuracy: 75.05%\n",
      "Epoch [20/30], Loss: 2303.7161\n",
      "Validation Accuracy: 75.16%\n",
      "Epoch [21/30], Loss: 2299.7627\n",
      "Validation Accuracy: 74.68%\n",
      "Epoch [22/30], Loss: 2288.0019\n",
      "Validation Accuracy: 74.70%\n",
      "Epoch [23/30], Loss: 2278.7404\n",
      "Validation Accuracy: 74.93%\n",
      "Epoch [24/30], Loss: 2263.0673\n",
      "Validation Accuracy: 75.06%\n",
      "Epoch [25/30], Loss: 2259.3757\n",
      "Validation Accuracy: 75.00%\n",
      "Early stopping triggered after 25 epochs\n",
      "Fold 2 Validation Accuracy: 75.02%\n",
      "Fold 3/3\n",
      "Epoch [1/30], Loss: 7267.9345\n",
      "Validation Accuracy: 43.30%\n",
      "Epoch [2/30], Loss: 3884.6032\n",
      "Validation Accuracy: 66.13%\n",
      "Epoch [3/30], Loss: 3222.0144\n",
      "Validation Accuracy: 69.18%\n",
      "Epoch [4/30], Loss: 3063.2419\n",
      "Validation Accuracy: 69.14%\n",
      "Epoch [5/30], Loss: 2964.7220\n",
      "Validation Accuracy: 70.83%\n",
      "Epoch [6/30], Loss: 2871.4947\n",
      "Validation Accuracy: 71.72%\n",
      "Epoch [7/30], Loss: 2802.6324\n",
      "Validation Accuracy: 71.99%\n",
      "Epoch [8/30], Loss: 2733.7622\n",
      "Validation Accuracy: 72.62%\n",
      "Epoch [9/30], Loss: 2663.3578\n",
      "Validation Accuracy: 71.33%\n",
      "Epoch [10/30], Loss: 2625.8345\n",
      "Validation Accuracy: 73.26%\n",
      "Epoch [11/30], Loss: 2572.0441\n",
      "Validation Accuracy: 72.90%\n",
      "Epoch [12/30], Loss: 2543.2093\n",
      "Validation Accuracy: 73.15%\n",
      "Epoch [13/30], Loss: 2503.3920\n",
      "Validation Accuracy: 73.40%\n",
      "Epoch [14/30], Loss: 2480.4147\n",
      "Validation Accuracy: 73.54%\n",
      "Epoch [15/30], Loss: 2458.7682\n",
      "Validation Accuracy: 74.18%\n",
      "Epoch [16/30], Loss: 2445.8182\n",
      "Validation Accuracy: 74.14%\n",
      "Epoch [17/30], Loss: 2427.9056\n",
      "Validation Accuracy: 73.86%\n",
      "Epoch [18/30], Loss: 2404.5938\n",
      "Validation Accuracy: 74.16%\n",
      "Epoch [19/30], Loss: 2388.7291\n",
      "Validation Accuracy: 73.53%\n",
      "Epoch [20/30], Loss: 2376.2370\n",
      "Validation Accuracy: 74.31%\n",
      "Epoch [21/30], Loss: 2370.0444\n",
      "Validation Accuracy: 74.45%\n",
      "Epoch [22/30], Loss: 2361.6658\n",
      "Validation Accuracy: 73.88%\n",
      "Epoch [23/30], Loss: 2351.4186\n",
      "Validation Accuracy: 74.91%\n",
      "Epoch [24/30], Loss: 2336.2270\n",
      "Validation Accuracy: 74.84%\n",
      "Epoch [25/30], Loss: 2334.1250\n",
      "Validation Accuracy: 74.81%\n",
      "Epoch [26/30], Loss: 2315.9380\n",
      "Validation Accuracy: 73.54%\n",
      "Epoch [27/30], Loss: 2317.8621\n",
      "Validation Accuracy: 74.55%\n",
      "Epoch [28/30], Loss: 2310.6297\n",
      "Validation Accuracy: 74.80%\n",
      "Early stopping triggered after 28 epochs\n",
      "Fold 3 Validation Accuracy: 74.73%\n",
      "Model 1 - Average Validation Accuracy across folds: 74.85%\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Can't pickle local object 'load_augmented_mnist.<locals>.<lambda>'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 64\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39margmax(avg_prediction, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 64\u001b[0m     ensemble \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_ensemble\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_models\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;66;03m# Test the ensemble on experimental data\u001b[39;00m\n\u001b[0;32m     67\u001b[0m     data_module \u001b[38;5;241m=\u001b[39m EnsembleDataModule(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_digits\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[18], line 43\u001b[0m, in \u001b[0;36mtrain_ensemble\u001b[1;34m(num_models, num_folds, num_epochs, patience)\u001b[0m\n\u001b[0;32m     40\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(weight\u001b[38;5;241m=\u001b[39mclass_weights\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[0;32m     41\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m---> 43\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_train_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_val_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatience\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m ensemble\u001b[38;5;241m.\u001b[39mappend(model)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[16], line 11\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, criterion, optimizer, num_epochs, patience)\u001b[0m\n\u001b[0;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m      9\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     12\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     13\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\Sai\\Desktop\\NN_digit_classifier\\Southampton_NN_digit_classifier\\env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:440\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    438\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Sai\\Desktop\\NN_digit_classifier\\Southampton_NN_digit_classifier\\env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:388\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    387\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Sai\\Desktop\\NN_digit_classifier\\Southampton_NN_digit_classifier\\env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1038\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1031\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1032\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m   1033\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m-> 1038\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[0;32m   1040\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2800.0_x64__qbz5n2kfra8p0\\lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2800.0_x64__qbz5n2kfra8p0\\lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2800.0_x64__qbz5n2kfra8p0\\lib\\multiprocessing\\context.py:327\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_win32\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[1;32m--> 327\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2800.0_x64__qbz5n2kfra8p0\\lib\\multiprocessing\\popen_spawn_win32.py:93\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 93\u001b[0m     \u001b[43mreduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_child\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2800.0_x64__qbz5n2kfra8p0\\lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdump\u001b[39m(obj, file, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     \u001b[43mForkingPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: Can't pickle local object 'load_augmented_mnist.<locals>.<lambda>'"
     ]
    }
   ],
   "source": [
    "def train_ensemble(num_models=25, num_folds=3, num_epochs=30, patience=3):\n",
    "    ensemble = []\n",
    "    \n",
    "    for i in range(num_models):\n",
    "        print(f\"Training model {i+1}/{num_models}\")\n",
    "        \n",
    "        # Load experimental data\n",
    "        exp_train_images, exp_train_labels, exp_test_images, exp_test_labels, participant_data = load_all_experimental_data('test_digits')\n",
    "        exp_dataset = TensorDataset(exp_train_images, exp_train_labels)\n",
    "\n",
    "        # Load augmented MNIST data\n",
    "        mnist_train, mnist_test = load_augmented_mnist()\n",
    "\n",
    "        # Combine experimental data with augmented MNIST data\n",
    "        combined_dataset = ConcatDataset([MixedDataset(exp_dataset), MixedDataset(mnist_train)])\n",
    "\n",
    "        # Perform K-Fold Cross Validation\n",
    "        fold_results = k_fold_cross_validation(combined_dataset, num_folds=num_folds, num_epochs=num_epochs, patience=patience)\n",
    "        print(f\"Model {i+1} - Average Validation Accuracy across folds: {np.mean(fold_results):.2f}%\")\n",
    "\n",
    "        # Train final model on all data\n",
    "        final_train_loader = DataLoader(combined_dataset, batch_size=64, num_workers=4, pin_memory=True,shuffle=True)\n",
    "        final_val_loader = DataLoader(MixedDataset(mnist_test), batch_size=64, num_workers=4, pin_memory=True,shuffle=False)\n",
    "\n",
    "        model = LeNet5_16x16(num_classes=10).to(device)\n",
    "        \n",
    "        # Calculate class weights for the final model\n",
    "        all_labels = []\n",
    "        for dataset in combined_dataset.datasets:\n",
    "            if isinstance(dataset, TensorDataset):\n",
    "                all_labels.extend(dataset.tensors[1].tolist())\n",
    "            else:\n",
    "                all_labels.extend([label for _, label in dataset])\n",
    "        \n",
    "        labels = torch.tensor(all_labels)\n",
    "        class_counts = torch.bincount(labels)\n",
    "        class_weights = 1. / class_counts.float()\n",
    "        class_weights = class_weights / class_weights.sum()\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "        model = train_model(model, final_train_loader, final_val_loader, criterion, optimizer, num_epochs=num_epochs, patience=patience)\n",
    "        \n",
    "        ensemble.append(model)\n",
    "        \n",
    "        # Save the model\n",
    "        torch.save(model.state_dict(), f\"lenet5_trained_model_ensemble_{i+1}.pth\")\n",
    "        print(f\"Model {i+1} saved as lenet5_trained_model_ensemble_{i+1}.pth\")\n",
    "    \n",
    "    return ensemble\n",
    "\n",
    "def ensemble_predict(ensemble, input_tensor):\n",
    "    predictions = []\n",
    "    for model in ensemble:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "            predictions.append(torch.softmax(output, dim=1))\n",
    "    avg_prediction = torch.mean(torch.stack(predictions), dim=0)\n",
    "    return torch.argmax(avg_prediction, dim=1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ensemble = train_ensemble(num_models=25, num_epochs=30, patience=5)\n",
    "\n",
    "    # Test the ensemble on experimental data\n",
    "    data_module = EnsembleDataModule('test_digits')\n",
    "    data_module.setup()\n",
    "    test_loader = data_module.test_dataloader()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        predictions = ensemble_predict(ensemble, images)\n",
    "        total += labels.size(0)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Ensemble accuracy on experimental data: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
