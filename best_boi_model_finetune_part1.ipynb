{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the information you've shared, I suggest we focus on the test_digits dataset first, as it's already prepared and more manageable in size. Here's a proposed approach:\n",
    "\n",
    "1. Use the test_digits dataset:\n",
    "   - 31,000 judgements for training\n",
    "   - 140 composites for initial testing\n",
    "\n",
    "2. Data preparation:\n",
    "   a. Create more composites to balance the classes. Aim for about 200-300 composites per digit.\n",
    "   b. Split the data:\n",
    "      - 80% of judgements for training\n",
    "      - 10% of judgements for validation\n",
    "      - 10% of judgements + all composites for testing\n",
    "\n",
    "3. Transfer learning approach:\n",
    "   Instead of fine-tuning the entire model, which might lead to overfitting given the relatively small dataset, I suggest using transfer learning:\n",
    "   \n",
    "   a. Freeze the earlier layers of the ensemble model.\n",
    "   b. Replace the final classification layer with a new one.\n",
    "   c. Train only the new layer(s) on the test_digits training set.\n",
    "\n",
    "4. Training process:\n",
    "   a. Use the validation set to monitor performance and prevent overfitting.\n",
    "   b. Implement early stopping.\n",
    "   c. Use data augmentation to increase the effective size of the training set.\n",
    "\n",
    "5. Evaluation:\n",
    "   a. Use the held-out test set (10% of judgements + composites) for final evaluation.\n",
    "   b. Compare performance on judgements vs. composites.\n",
    "\n",
    "6. Iterative improvement:\n",
    "   If the results are still not satisfactory, we can gradually unfreeze more layers of the original model and continue training.\n",
    "\n",
    "This approach allows us to leverage the pre-trained model's knowledge while adapting it to the specific characteristics of the test_digits dataset. It's also computationally efficient compared to training from scratch or using the larger JSON dataset.\n",
    "\n",
    "Regarding the JSON dataset, we can keep it as a backup option. If the transfer learning approach with the test_digits dataset doesn't yield satisfactory results, we can explore using the JSON data to create a larger, more diverse training set. However, this would require more preprocessing and computational resources.\n",
    "\n",
    "To implement this plan, we'll need to modify your existing code to:\n",
    "1. Split the test_digits data as described.\n",
    "2. Implement the transfer learning approach (freezing layers, replacing the classification layer).\n",
    "3. Add data augmentation.\n",
    "4. Implement early stopping and validation checks during training.\n",
    "5. Create a new evaluation function for the final test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mnist_skeptic_v9 import skeptic_v9\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sai\\AppData\\Local\\Temp\\ipykernel_31088\\3300162512.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(path))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EnsembleModel(\n",
       "  (models): ModuleList(\n",
       "    (0-19): 20 x skeptic_v9(\n",
       "      (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (batchnorm1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (batchnorm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (fc1): Linear(in_features=512, out_features=128, bias=True)\n",
       "      (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 1: Ensemble Model Creation and Prediction\n",
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self, model_paths):\n",
    "        super(EnsembleModel, self).__init__()\n",
    "        self.models = nn.ModuleList([skeptic_v9() for _ in range(len(model_paths))])\n",
    "        for model, path in zip(self.models, model_paths):\n",
    "            model.load_state_dict(torch.load(path))\n",
    "            model.eval()\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = [model(x) for model in self.models]\n",
    "        return torch.stack(outputs).mean(dim=0)\n",
    "\n",
    "def create_ensemble(model_dir='best_boi_models'):\n",
    "    model_paths = [os.path.join(model_dir, f) for f in os.listdir(model_dir) if f.endswith('.pth')]\n",
    "    return EnsembleModel(model_paths)\n",
    "\n",
    "ensemble_model = create_ensemble()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ensemble_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransferEnsembleModel(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super(TransferEnsembleModel, self).__init__()\n",
    "        self.base_model = base_model\n",
    "        \n",
    "        # Freeze the base model parameters\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Inspect the base model structure\n",
    "        last_layer = list(self.base_model.models[0].children())[-1]\n",
    "        if isinstance(last_layer, nn.Linear):\n",
    "            num_features = last_layer.in_features\n",
    "        else:\n",
    "            # If the last layer is not Linear, we need to flatten the output\n",
    "            # and determine the number of features\n",
    "            dummy_input = torch.randn(1, 1, 16, 16)  # Assuming 16x16 input size\n",
    "            with torch.no_grad():\n",
    "                dummy_output = self.base_model.models[0](dummy_input)\n",
    "            num_features = dummy_output.numel()\n",
    "        \n",
    "        # Replace the final layer\n",
    "        self.new_fc = nn.Linear(num_features, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            features = self.base_model(x)\n",
    "        # If features is not already flattened, flatten it\n",
    "        if features.dim() > 2:\n",
    "            features = features.view(features.size(0), -1)\n",
    "        return self.new_fc(features)\n",
    "\n",
    "def load_and_prepare_data(test_digits_folder):\n",
    "    train_images = []\n",
    "    train_labels = []\n",
    "    test_images = []\n",
    "    test_labels = []\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((16, 16)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "    for filename in os.listdir(test_digits_folder):\n",
    "        if filename.endswith('.zip') and filename.startswith('experiment_results_participant'):\n",
    "            zip_filepath = os.path.join(test_digits_folder, filename)\n",
    "\n",
    "            with zipfile.ZipFile(zip_filepath, 'r') as zip_ref:\n",
    "                for img_filename in zip_ref.namelist():\n",
    "                    if img_filename.endswith('.png'):\n",
    "                        with zip_ref.open(img_filename) as file:\n",
    "                            img = Image.open(file).convert('L')  # Convert to grayscale\n",
    "                            img_tensor = transform(img)\n",
    "                            \n",
    "                            digit = int(img_filename.split('_')[0])\n",
    "                            \n",
    "                            if 'composite' in img_filename:\n",
    "                                test_images.append(img_tensor)\n",
    "                                test_labels.append(digit)\n",
    "                            else:\n",
    "                                train_images.append(img_tensor)\n",
    "                                train_labels.append(digit)\n",
    "\n",
    "    train_images = torch.stack(train_images)\n",
    "    train_labels = torch.tensor(train_labels)\n",
    "    test_images = torch.stack(test_images)\n",
    "    test_labels = torch.tensor(test_labels)\n",
    "\n",
    "    # Split train data into train and validation\n",
    "    train_images, val_images, train_labels, val_labels = train_test_split(\n",
    "        train_images, train_labels, test_size=0.1, stratify=train_labels, random_state=42\n",
    "    )\n",
    "\n",
    "    return (train_images, train_labels), (val_images, val_labels), (test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device, save_dir):\n",
    "    best_val_acc = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            if (batch_idx + 1) % 10 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "        train_loss = train_loss / len(train_loader.dataset)\n",
    "        train_acc = train_correct / train_total\n",
    "        epoch_time = time.time() - start_time\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_acc = val_correct / val_total\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Time: {epoch_time:.2f}s')\n",
    "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
    "        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), os.path.join(save_dir, f'best_transfer_model_epoch_{epoch+1}.pth'))\n",
    "            print(f'New best model saved at epoch {epoch+1}')\n",
    "\n",
    "    print(f'Best validation accuracy: {best_val_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sai\\AppData\\Local\\Temp\\ipykernel_31088\\3300162512.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base ensemble structure:\n",
      "EnsembleModel(\n",
      "  (models): ModuleList(\n",
      "    (0-19): 20 x skeptic_v9(\n",
      "      (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (batchnorm1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (batchnorm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (fc1): Linear(in_features=512, out_features=128, bias=True)\n",
      "      (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      "      (dropout): Dropout(p=0.5, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Transfer model structure:\n",
      "TransferEnsembleModel(\n",
      "  (base_model): EnsembleModel(\n",
      "    (models): ModuleList(\n",
      "      (0-19): 20 x skeptic_v9(\n",
      "        (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (batchnorm1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (batchnorm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (fc1): Linear(in_features=512, out_features=128, bias=True)\n",
      "        (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (new_fc): Linear(in_features=10, out_features=10, bias=True)\n",
      ")\n",
      "Epoch [1/50], Step [10/436], Loss: 2.6027\n",
      "Epoch [1/50], Step [20/436], Loss: 2.4431\n",
      "Epoch [1/50], Step [30/436], Loss: 2.4938\n",
      "Epoch [1/50], Step [40/436], Loss: 2.4150\n",
      "Epoch [1/50], Step [50/436], Loss: 2.4278\n",
      "Epoch [1/50], Step [60/436], Loss: 2.3566\n",
      "Epoch [1/50], Step [70/436], Loss: 2.3511\n",
      "Epoch [1/50], Step [80/436], Loss: 2.4516\n",
      "Epoch [1/50], Step [90/436], Loss: 2.3720\n",
      "Epoch [1/50], Step [100/436], Loss: 2.3786\n",
      "Epoch [1/50], Step [110/436], Loss: 2.2994\n",
      "Epoch [1/50], Step [120/436], Loss: 2.2996\n",
      "Epoch [1/50], Step [130/436], Loss: 2.2980\n",
      "Epoch [1/50], Step [140/436], Loss: 2.3850\n",
      "Epoch [1/50], Step [150/436], Loss: 2.3870\n",
      "Epoch [1/50], Step [160/436], Loss: 2.3090\n",
      "Epoch [1/50], Step [170/436], Loss: 2.3468\n",
      "Epoch [1/50], Step [180/436], Loss: 2.3266\n",
      "Epoch [1/50], Step [190/436], Loss: 2.2974\n",
      "Epoch [1/50], Step [200/436], Loss: 2.3433\n",
      "Epoch [1/50], Step [210/436], Loss: 2.2830\n",
      "Epoch [1/50], Step [220/436], Loss: 2.3198\n",
      "Epoch [1/50], Step [230/436], Loss: 2.3598\n",
      "Epoch [1/50], Step [240/436], Loss: 2.3506\n",
      "Epoch [1/50], Step [250/436], Loss: 2.3129\n",
      "Epoch [1/50], Step [260/436], Loss: 2.3275\n",
      "Epoch [1/50], Step [270/436], Loss: 2.3674\n",
      "Epoch [1/50], Step [280/436], Loss: 2.2802\n",
      "Epoch [1/50], Step [290/436], Loss: 2.3567\n",
      "Epoch [1/50], Step [300/436], Loss: 2.3264\n",
      "Epoch [1/50], Step [310/436], Loss: 2.2624\n",
      "Epoch [1/50], Step [320/436], Loss: 2.3081\n",
      "Epoch [1/50], Step [330/436], Loss: 2.3187\n",
      "Epoch [1/50], Step [340/436], Loss: 2.3209\n",
      "Epoch [1/50], Step [350/436], Loss: 2.3394\n",
      "Epoch [1/50], Step [360/436], Loss: 2.3350\n",
      "Epoch [1/50], Step [370/436], Loss: 2.3215\n",
      "Epoch [1/50], Step [380/436], Loss: 2.3393\n",
      "Epoch [1/50], Step [390/436], Loss: 2.3200\n",
      "Epoch [1/50], Step [400/436], Loss: 2.3221\n",
      "Epoch [1/50], Step [410/436], Loss: 2.3029\n",
      "Epoch [1/50], Step [420/436], Loss: 2.2611\n",
      "Epoch [1/50], Step [430/436], Loss: 2.3047\n",
      "Epoch [1/50], Time: 33.13s\n",
      "Train Loss: 2.3530, Train Acc: 0.1032\n",
      "Val Loss: 2.3219, Val Acc: 0.1035\n",
      "New best model saved at epoch 1\n",
      "Epoch [2/50], Step [10/436], Loss: 2.3228\n",
      "Epoch [2/50], Step [20/436], Loss: 2.3190\n",
      "Epoch [2/50], Step [30/436], Loss: 2.3122\n",
      "Epoch [2/50], Step [40/436], Loss: 2.3100\n",
      "Epoch [2/50], Step [50/436], Loss: 2.2581\n",
      "Epoch [2/50], Step [60/436], Loss: 2.3145\n",
      "Epoch [2/50], Step [70/436], Loss: 2.2984\n",
      "Epoch [2/50], Step [80/436], Loss: 2.2791\n",
      "Epoch [2/50], Step [90/436], Loss: 2.2981\n",
      "Epoch [2/50], Step [100/436], Loss: 2.3027\n",
      "Epoch [2/50], Step [110/436], Loss: 2.3398\n",
      "Epoch [2/50], Step [120/436], Loss: 2.3192\n",
      "Epoch [2/50], Step [130/436], Loss: 2.3088\n",
      "Epoch [2/50], Step [140/436], Loss: 2.3060\n",
      "Epoch [2/50], Step [150/436], Loss: 2.3014\n",
      "Epoch [2/50], Step [160/436], Loss: 2.3356\n",
      "Epoch [2/50], Step [170/436], Loss: 2.3248\n",
      "Epoch [2/50], Step [180/436], Loss: 2.3192\n",
      "Epoch [2/50], Step [190/436], Loss: 2.3158\n",
      "Epoch [2/50], Step [200/436], Loss: 2.3024\n",
      "Epoch [2/50], Step [210/436], Loss: 2.2971\n",
      "Epoch [2/50], Step [220/436], Loss: 2.2902\n",
      "Epoch [2/50], Step [230/436], Loss: 2.2928\n",
      "Epoch [2/50], Step [240/436], Loss: 2.2902\n",
      "Epoch [2/50], Step [250/436], Loss: 2.2848\n",
      "Epoch [2/50], Step [260/436], Loss: 2.3167\n",
      "Epoch [2/50], Step [270/436], Loss: 2.2906\n",
      "Epoch [2/50], Step [280/436], Loss: 2.3181\n",
      "Epoch [2/50], Step [290/436], Loss: 2.3161\n",
      "Epoch [2/50], Step [300/436], Loss: 2.3028\n",
      "Epoch [2/50], Step [310/436], Loss: 2.3238\n",
      "Epoch [2/50], Step [320/436], Loss: 2.3227\n",
      "Epoch [2/50], Step [330/436], Loss: 2.3357\n",
      "Epoch [2/50], Step [340/436], Loss: 2.3336\n",
      "Epoch [2/50], Step [350/436], Loss: 2.2914\n",
      "Epoch [2/50], Step [360/436], Loss: 2.3262\n",
      "Epoch [2/50], Step [370/436], Loss: 2.3153\n",
      "Epoch [2/50], Step [380/436], Loss: 2.2984\n",
      "Epoch [2/50], Step [390/436], Loss: 2.2993\n",
      "Epoch [2/50], Step [400/436], Loss: 2.3089\n",
      "Epoch [2/50], Step [410/436], Loss: 2.2997\n",
      "Epoch [2/50], Step [420/436], Loss: 2.3226\n",
      "Epoch [2/50], Step [430/436], Loss: 2.2943\n",
      "Epoch [2/50], Time: 32.89s\n",
      "Train Loss: 2.3087, Train Acc: 0.1059\n",
      "Val Loss: 2.3109, Val Acc: 0.1000\n",
      "Epoch [3/50], Step [10/436], Loss: 2.3294\n",
      "Epoch [3/50], Step [20/436], Loss: 2.2871\n",
      "Epoch [3/50], Step [30/436], Loss: 2.2979\n",
      "Epoch [3/50], Step [40/436], Loss: 2.3105\n",
      "Epoch [3/50], Step [50/436], Loss: 2.2994\n",
      "Epoch [3/50], Step [60/436], Loss: 2.2807\n",
      "Epoch [3/50], Step [70/436], Loss: 2.3028\n",
      "Epoch [3/50], Step [80/436], Loss: 2.2696\n",
      "Epoch [3/50], Step [90/436], Loss: 2.3026\n",
      "Epoch [3/50], Step [100/436], Loss: 2.3191\n",
      "Epoch [3/50], Step [110/436], Loss: 2.3303\n",
      "Epoch [3/50], Step [120/436], Loss: 2.3007\n",
      "Epoch [3/50], Step [130/436], Loss: 2.3065\n",
      "Epoch [3/50], Step [140/436], Loss: 2.3022\n",
      "Epoch [3/50], Step [150/436], Loss: 2.3118\n",
      "Epoch [3/50], Step [160/436], Loss: 2.3202\n",
      "Epoch [3/50], Step [170/436], Loss: 2.2955\n",
      "Epoch [3/50], Step [180/436], Loss: 2.3041\n",
      "Epoch [3/50], Step [190/436], Loss: 2.3018\n",
      "Epoch [3/50], Step [200/436], Loss: 2.2653\n",
      "Epoch [3/50], Step [210/436], Loss: 2.2919\n",
      "Epoch [3/50], Step [220/436], Loss: 2.2941\n",
      "Epoch [3/50], Step [230/436], Loss: 2.3308\n",
      "Epoch [3/50], Step [240/436], Loss: 2.3228\n",
      "Epoch [3/50], Step [250/436], Loss: 2.3109\n",
      "Epoch [3/50], Step [260/436], Loss: 2.2998\n",
      "Epoch [3/50], Step [270/436], Loss: 2.3327\n",
      "Epoch [3/50], Step [280/436], Loss: 2.3198\n",
      "Epoch [3/50], Step [290/436], Loss: 2.3182\n",
      "Epoch [3/50], Step [300/436], Loss: 2.2973\n",
      "Epoch [3/50], Step [310/436], Loss: 2.2987\n",
      "Epoch [3/50], Step [320/436], Loss: 2.3114\n",
      "Epoch [3/50], Step [330/436], Loss: 2.3042\n",
      "Epoch [3/50], Step [340/436], Loss: 2.2995\n",
      "Epoch [3/50], Step [350/436], Loss: 2.3403\n",
      "Epoch [3/50], Step [360/436], Loss: 2.2979\n",
      "Epoch [3/50], Step [370/436], Loss: 2.2835\n",
      "Epoch [3/50], Step [380/436], Loss: 2.2824\n",
      "Epoch [3/50], Step [390/436], Loss: 2.2893\n",
      "Epoch [3/50], Step [400/436], Loss: 2.3084\n",
      "Epoch [3/50], Step [410/436], Loss: 2.3169\n",
      "Epoch [3/50], Step [420/436], Loss: 2.2834\n",
      "Epoch [3/50], Step [430/436], Loss: 2.3019\n",
      "Epoch [3/50], Time: 33.73s\n",
      "Train Loss: 2.3053, Train Acc: 0.1063\n",
      "Val Loss: 2.3080, Val Acc: 0.0974\n",
      "Epoch [4/50], Step [10/436], Loss: 2.3299\n",
      "Epoch [4/50], Step [20/436], Loss: 2.3001\n",
      "Epoch [4/50], Step [30/436], Loss: 2.3212\n",
      "Epoch [4/50], Step [40/436], Loss: 2.3127\n",
      "Epoch [4/50], Step [50/436], Loss: 2.3215\n",
      "Epoch [4/50], Step [60/436], Loss: 2.3107\n",
      "Epoch [4/50], Step [70/436], Loss: 2.3362\n",
      "Epoch [4/50], Step [80/436], Loss: 2.2989\n",
      "Epoch [4/50], Step [90/436], Loss: 2.3088\n",
      "Epoch [4/50], Step [100/436], Loss: 2.3066\n",
      "Epoch [4/50], Step [110/436], Loss: 2.3138\n",
      "Epoch [4/50], Step [120/436], Loss: 2.3195\n",
      "Epoch [4/50], Step [130/436], Loss: 2.3001\n",
      "Epoch [4/50], Step [140/436], Loss: 2.3068\n",
      "Epoch [4/50], Step [150/436], Loss: 2.2995\n",
      "Epoch [4/50], Step [160/436], Loss: 2.3037\n",
      "Epoch [4/50], Step [170/436], Loss: 2.2880\n",
      "Epoch [4/50], Step [180/436], Loss: 2.2946\n",
      "Epoch [4/50], Step [190/436], Loss: 2.2917\n",
      "Epoch [4/50], Step [200/436], Loss: 2.3133\n",
      "Epoch [4/50], Step [210/436], Loss: 2.3109\n",
      "Epoch [4/50], Step [220/436], Loss: 2.2903\n",
      "Epoch [4/50], Step [230/436], Loss: 2.3020\n",
      "Epoch [4/50], Step [240/436], Loss: 2.2980\n",
      "Epoch [4/50], Step [250/436], Loss: 2.2726\n",
      "Epoch [4/50], Step [260/436], Loss: 2.2889\n",
      "Epoch [4/50], Step [270/436], Loss: 2.2989\n",
      "Epoch [4/50], Step [280/436], Loss: 2.3180\n",
      "Epoch [4/50], Step [290/436], Loss: 2.2950\n",
      "Epoch [4/50], Step [300/436], Loss: 2.2818\n",
      "Epoch [4/50], Step [310/436], Loss: 2.3028\n",
      "Epoch [4/50], Step [320/436], Loss: 2.3028\n",
      "Epoch [4/50], Step [330/436], Loss: 2.2996\n",
      "Epoch [4/50], Step [340/436], Loss: 2.3104\n",
      "Epoch [4/50], Step [350/436], Loss: 2.2893\n",
      "Epoch [4/50], Step [360/436], Loss: 2.2881\n",
      "Epoch [4/50], Step [370/436], Loss: 2.3048\n",
      "Epoch [4/50], Step [380/436], Loss: 2.3054\n",
      "Epoch [4/50], Step [390/436], Loss: 2.3074\n",
      "Epoch [4/50], Step [400/436], Loss: 2.2969\n",
      "Epoch [4/50], Step [410/436], Loss: 2.2985\n",
      "Epoch [4/50], Step [420/436], Loss: 2.3071\n",
      "Epoch [4/50], Step [430/436], Loss: 2.3178\n",
      "Epoch [4/50], Time: 32.96s\n",
      "Train Loss: 2.3046, Train Acc: 0.1042\n",
      "Val Loss: 2.3056, Val Acc: 0.1023\n",
      "Epoch [5/50], Step [10/436], Loss: 2.3057\n",
      "Epoch [5/50], Step [20/436], Loss: 2.3262\n",
      "Epoch [5/50], Step [30/436], Loss: 2.2918\n",
      "Epoch [5/50], Step [40/436], Loss: 2.3285\n",
      "Epoch [5/50], Step [50/436], Loss: 2.3008\n",
      "Epoch [5/50], Step [60/436], Loss: 2.3132\n",
      "Epoch [5/50], Step [70/436], Loss: 2.3075\n",
      "Epoch [5/50], Step [80/436], Loss: 2.3212\n",
      "Epoch [5/50], Step [90/436], Loss: 2.3007\n",
      "Epoch [5/50], Step [100/436], Loss: 2.3054\n",
      "Epoch [5/50], Step [110/436], Loss: 2.2943\n",
      "Epoch [5/50], Step [120/436], Loss: 2.2860\n",
      "Epoch [5/50], Step [130/436], Loss: 2.3096\n",
      "Epoch [5/50], Step [140/436], Loss: 2.3155\n",
      "Epoch [5/50], Step [150/436], Loss: 2.2897\n",
      "Epoch [5/50], Step [160/436], Loss: 2.3077\n",
      "Epoch [5/50], Step [170/436], Loss: 2.3003\n",
      "Epoch [5/50], Step [180/436], Loss: 2.3028\n",
      "Epoch [5/50], Step [190/436], Loss: 2.3006\n",
      "Epoch [5/50], Step [200/436], Loss: 2.3047\n",
      "Epoch [5/50], Step [210/436], Loss: 2.2994\n",
      "Epoch [5/50], Step [220/436], Loss: 2.3004\n",
      "Epoch [5/50], Step [230/436], Loss: 2.2874\n",
      "Epoch [5/50], Step [240/436], Loss: 2.3129\n",
      "Epoch [5/50], Step [250/436], Loss: 2.3176\n",
      "Epoch [5/50], Step [260/436], Loss: 2.2890\n",
      "Epoch [5/50], Step [270/436], Loss: 2.2842\n",
      "Epoch [5/50], Step [280/436], Loss: 2.2910\n",
      "Epoch [5/50], Step [290/436], Loss: 2.2924\n",
      "Epoch [5/50], Step [300/436], Loss: 2.3065\n",
      "Epoch [5/50], Step [310/436], Loss: 2.3311\n",
      "Epoch [5/50], Step [320/436], Loss: 2.3202\n",
      "Epoch [5/50], Step [330/436], Loss: 2.3186\n",
      "Epoch [5/50], Step [340/436], Loss: 2.2985\n",
      "Epoch [5/50], Step [350/436], Loss: 2.3058\n",
      "Epoch [5/50], Step [360/436], Loss: 2.3046\n",
      "Epoch [5/50], Step [370/436], Loss: 2.3159\n",
      "Epoch [5/50], Step [380/436], Loss: 2.3063\n",
      "Epoch [5/50], Step [390/436], Loss: 2.2992\n",
      "Epoch [5/50], Step [400/436], Loss: 2.3120\n",
      "Epoch [5/50], Step [410/436], Loss: 2.2902\n",
      "Epoch [5/50], Step [420/436], Loss: 2.3040\n",
      "Epoch [5/50], Step [430/436], Loss: 2.2950\n",
      "Epoch [5/50], Time: 32.21s\n",
      "Train Loss: 2.3037, Train Acc: 0.1074\n",
      "Val Loss: 2.3055, Val Acc: 0.1052\n",
      "New best model saved at epoch 5\n",
      "Epoch [6/50], Step [10/436], Loss: 2.2956\n",
      "Epoch [6/50], Step [20/436], Loss: 2.3141\n",
      "Epoch [6/50], Step [30/436], Loss: 2.3179\n",
      "Epoch [6/50], Step [40/436], Loss: 2.3001\n",
      "Epoch [6/50], Step [50/436], Loss: 2.3131\n",
      "Epoch [6/50], Step [60/436], Loss: 2.3149\n",
      "Epoch [6/50], Step [70/436], Loss: 2.3067\n",
      "Epoch [6/50], Step [80/436], Loss: 2.3101\n",
      "Epoch [6/50], Step [90/436], Loss: 2.3085\n",
      "Epoch [6/50], Step [100/436], Loss: 2.2937\n",
      "Epoch [6/50], Step [110/436], Loss: 2.3193\n",
      "Epoch [6/50], Step [120/436], Loss: 2.3137\n",
      "Epoch [6/50], Step [130/436], Loss: 2.3148\n",
      "Epoch [6/50], Step [140/436], Loss: 2.2908\n",
      "Epoch [6/50], Step [150/436], Loss: 2.2922\n",
      "Epoch [6/50], Step [160/436], Loss: 2.3034\n",
      "Epoch [6/50], Step [170/436], Loss: 2.3043\n",
      "Epoch [6/50], Step [180/436], Loss: 2.2960\n",
      "Epoch [6/50], Step [190/436], Loss: 2.2913\n",
      "Epoch [6/50], Step [200/436], Loss: 2.2848\n",
      "Epoch [6/50], Step [210/436], Loss: 2.3013\n",
      "Epoch [6/50], Step [220/436], Loss: 2.2930\n",
      "Epoch [6/50], Step [230/436], Loss: 2.3053\n",
      "Epoch [6/50], Step [240/436], Loss: 2.2997\n",
      "Epoch [6/50], Step [250/436], Loss: 2.3126\n",
      "Epoch [6/50], Step [260/436], Loss: 2.2985\n",
      "Epoch [6/50], Step [270/436], Loss: 2.3243\n",
      "Epoch [6/50], Step [280/436], Loss: 2.3085\n",
      "Epoch [6/50], Step [290/436], Loss: 2.3104\n",
      "Epoch [6/50], Step [300/436], Loss: 2.2758\n",
      "Epoch [6/50], Step [310/436], Loss: 2.3023\n",
      "Epoch [6/50], Step [320/436], Loss: 2.3123\n",
      "Epoch [6/50], Step [330/436], Loss: 2.2966\n",
      "Epoch [6/50], Step [340/436], Loss: 2.3099\n",
      "Epoch [6/50], Step [350/436], Loss: 2.3075\n",
      "Epoch [6/50], Step [360/436], Loss: 2.2946\n",
      "Epoch [6/50], Step [370/436], Loss: 2.3035\n",
      "Epoch [6/50], Step [380/436], Loss: 2.3141\n",
      "Epoch [6/50], Step [390/436], Loss: 2.3106\n",
      "Epoch [6/50], Step [400/436], Loss: 2.3086\n",
      "Epoch [6/50], Step [410/436], Loss: 2.2823\n",
      "Epoch [6/50], Step [420/436], Loss: 2.3052\n",
      "Epoch [6/50], Step [430/436], Loss: 2.2867\n",
      "Epoch [6/50], Time: 32.20s\n",
      "Train Loss: 2.3038, Train Acc: 0.1078\n",
      "Val Loss: 2.3049, Val Acc: 0.1006\n",
      "Epoch [7/50], Step [10/436], Loss: 2.3214\n",
      "Epoch [7/50], Step [20/436], Loss: 2.2901\n",
      "Epoch [7/50], Step [30/436], Loss: 2.2996\n",
      "Epoch [7/50], Step [40/436], Loss: 2.3112\n",
      "Epoch [7/50], Step [50/436], Loss: 2.3105\n",
      "Epoch [7/50], Step [60/436], Loss: 2.2896\n",
      "Epoch [7/50], Step [70/436], Loss: 2.3232\n",
      "Epoch [7/50], Step [80/436], Loss: 2.3037\n",
      "Epoch [7/50], Step [90/436], Loss: 2.3049\n",
      "Epoch [7/50], Step [100/436], Loss: 2.3144\n",
      "Epoch [7/50], Step [110/436], Loss: 2.3154\n",
      "Epoch [7/50], Step [120/436], Loss: 2.3114\n",
      "Epoch [7/50], Step [130/436], Loss: 2.3210\n",
      "Epoch [7/50], Step [140/436], Loss: 2.3104\n",
      "Epoch [7/50], Step [150/436], Loss: 2.2932\n",
      "Epoch [7/50], Step [160/436], Loss: 2.3151\n",
      "Epoch [7/50], Step [170/436], Loss: 2.3294\n",
      "Epoch [7/50], Step [180/436], Loss: 2.3180\n",
      "Epoch [7/50], Step [190/436], Loss: 2.2964\n",
      "Epoch [7/50], Step [200/436], Loss: 2.2917\n",
      "Epoch [7/50], Step [210/436], Loss: 2.3010\n",
      "Epoch [7/50], Step [220/436], Loss: 2.2962\n",
      "Epoch [7/50], Step [230/436], Loss: 2.3127\n",
      "Epoch [7/50], Step [240/436], Loss: 2.3139\n",
      "Epoch [7/50], Step [250/436], Loss: 2.3201\n",
      "Epoch [7/50], Step [260/436], Loss: 2.3118\n",
      "Epoch [7/50], Step [270/436], Loss: 2.2986\n",
      "Epoch [7/50], Step [280/436], Loss: 2.3001\n",
      "Epoch [7/50], Step [290/436], Loss: 2.2998\n",
      "Epoch [7/50], Step [300/436], Loss: 2.3096\n",
      "Epoch [7/50], Step [310/436], Loss: 2.2989\n",
      "Epoch [7/50], Step [320/436], Loss: 2.3030\n",
      "Epoch [7/50], Step [330/436], Loss: 2.3149\n",
      "Epoch [7/50], Step [340/436], Loss: 2.2995\n",
      "Epoch [7/50], Step [350/436], Loss: 2.2953\n",
      "Epoch [7/50], Step [360/436], Loss: 2.3200\n",
      "Epoch [7/50], Step [370/436], Loss: 2.2953\n",
      "Epoch [7/50], Step [380/436], Loss: 2.3170\n",
      "Epoch [7/50], Step [390/436], Loss: 2.3027\n",
      "Epoch [7/50], Step [400/436], Loss: 2.2841\n",
      "Epoch [7/50], Step [410/436], Loss: 2.3105\n",
      "Epoch [7/50], Step [420/436], Loss: 2.3029\n",
      "Epoch [7/50], Step [430/436], Loss: 2.2842\n",
      "Epoch [7/50], Time: 32.09s\n",
      "Train Loss: 2.3035, Train Acc: 0.1045\n",
      "Val Loss: 2.3059, Val Acc: 0.0955\n",
      "Epoch [8/50], Step [10/436], Loss: 2.3047\n",
      "Epoch [8/50], Step [20/436], Loss: 2.3180\n",
      "Epoch [8/50], Step [30/436], Loss: 2.2813\n",
      "Epoch [8/50], Step [40/436], Loss: 2.3178\n",
      "Epoch [8/50], Step [50/436], Loss: 2.2891\n",
      "Epoch [8/50], Step [60/436], Loss: 2.3179\n",
      "Epoch [8/50], Step [70/436], Loss: 2.3323\n",
      "Epoch [8/50], Step [80/436], Loss: 2.3242\n",
      "Epoch [8/50], Step [90/436], Loss: 2.3276\n",
      "Epoch [8/50], Step [100/436], Loss: 2.2986\n",
      "Epoch [8/50], Step [110/436], Loss: 2.3010\n",
      "Epoch [8/50], Step [120/436], Loss: 2.3095\n",
      "Epoch [8/50], Step [130/436], Loss: 2.2924\n",
      "Epoch [8/50], Step [140/436], Loss: 2.2801\n",
      "Epoch [8/50], Step [150/436], Loss: 2.3124\n",
      "Epoch [8/50], Step [160/436], Loss: 2.2805\n",
      "Epoch [8/50], Step [170/436], Loss: 2.3187\n",
      "Epoch [8/50], Step [180/436], Loss: 2.3168\n",
      "Epoch [8/50], Step [190/436], Loss: 2.3225\n",
      "Epoch [8/50], Step [200/436], Loss: 2.3163\n",
      "Epoch [8/50], Step [210/436], Loss: 2.3208\n",
      "Epoch [8/50], Step [220/436], Loss: 2.3071\n",
      "Epoch [8/50], Step [230/436], Loss: 2.3216\n",
      "Epoch [8/50], Step [240/436], Loss: 2.2790\n",
      "Epoch [8/50], Step [250/436], Loss: 2.3158\n",
      "Epoch [8/50], Step [260/436], Loss: 2.3062\n",
      "Epoch [8/50], Step [270/436], Loss: 2.3179\n",
      "Epoch [8/50], Step [280/436], Loss: 2.2859\n",
      "Epoch [8/50], Step [290/436], Loss: 2.2872\n",
      "Epoch [8/50], Step [300/436], Loss: 2.3097\n",
      "Epoch [8/50], Step [310/436], Loss: 2.2982\n",
      "Epoch [8/50], Step [320/436], Loss: 2.3141\n",
      "Epoch [8/50], Step [330/436], Loss: 2.3014\n",
      "Epoch [8/50], Step [340/436], Loss: 2.3073\n",
      "Epoch [8/50], Step [350/436], Loss: 2.3065\n",
      "Epoch [8/50], Step [360/436], Loss: 2.3009\n",
      "Epoch [8/50], Step [370/436], Loss: 2.2958\n",
      "Epoch [8/50], Step [380/436], Loss: 2.2987\n",
      "Epoch [8/50], Step [390/436], Loss: 2.3147\n",
      "Epoch [8/50], Step [400/436], Loss: 2.3055\n",
      "Epoch [8/50], Step [410/436], Loss: 2.3029\n",
      "Epoch [8/50], Step [420/436], Loss: 2.3028\n",
      "Epoch [8/50], Step [430/436], Loss: 2.2952\n",
      "Epoch [8/50], Time: 31.79s\n",
      "Train Loss: 2.3036, Train Acc: 0.1058\n",
      "Val Loss: 2.3058, Val Acc: 0.1000\n",
      "Epoch [9/50], Step [10/436], Loss: 2.2971\n",
      "Epoch [9/50], Step [20/436], Loss: 2.2890\n",
      "Epoch [9/50], Step [30/436], Loss: 2.2932\n",
      "Epoch [9/50], Step [40/436], Loss: 2.2871\n",
      "Epoch [9/50], Step [50/436], Loss: 2.3257\n",
      "Epoch [9/50], Step [60/436], Loss: 2.2912\n",
      "Epoch [9/50], Step [70/436], Loss: 2.3060\n",
      "Epoch [9/50], Step [80/436], Loss: 2.3268\n",
      "Epoch [9/50], Step [90/436], Loss: 2.2885\n",
      "Epoch [9/50], Step [100/436], Loss: 2.2972\n",
      "Epoch [9/50], Step [110/436], Loss: 2.3135\n",
      "Epoch [9/50], Step [120/436], Loss: 2.3243\n",
      "Epoch [9/50], Step [130/436], Loss: 2.3146\n",
      "Epoch [9/50], Step [140/436], Loss: 2.3017\n",
      "Epoch [9/50], Step [150/436], Loss: 2.2866\n",
      "Epoch [9/50], Step [160/436], Loss: 2.2980\n",
      "Epoch [9/50], Step [170/436], Loss: 2.3086\n",
      "Epoch [9/50], Step [180/436], Loss: 2.3036\n",
      "Epoch [9/50], Step [190/436], Loss: 2.3227\n",
      "Epoch [9/50], Step [200/436], Loss: 2.3075\n",
      "Epoch [9/50], Step [210/436], Loss: 2.3129\n",
      "Epoch [9/50], Step [220/436], Loss: 2.2986\n",
      "Epoch [9/50], Step [230/436], Loss: 2.3027\n",
      "Epoch [9/50], Step [240/436], Loss: 2.3097\n",
      "Epoch [9/50], Step [250/436], Loss: 2.2946\n",
      "Epoch [9/50], Step [260/436], Loss: 2.3020\n",
      "Epoch [9/50], Step [270/436], Loss: 2.2902\n",
      "Epoch [9/50], Step [280/436], Loss: 2.2954\n",
      "Epoch [9/50], Step [290/436], Loss: 2.3136\n",
      "Epoch [9/50], Step [300/436], Loss: 2.3213\n",
      "Epoch [9/50], Step [310/436], Loss: 2.3092\n",
      "Epoch [9/50], Step [320/436], Loss: 2.3111\n",
      "Epoch [9/50], Step [330/436], Loss: 2.2955\n",
      "Epoch [9/50], Step [340/436], Loss: 2.3054\n",
      "Epoch [9/50], Step [350/436], Loss: 2.2792\n",
      "Epoch [9/50], Step [360/436], Loss: 2.3032\n",
      "Epoch [9/50], Step [370/436], Loss: 2.3130\n",
      "Epoch [9/50], Step [380/436], Loss: 2.3090\n",
      "Epoch [9/50], Step [390/436], Loss: 2.3060\n",
      "Epoch [9/50], Step [400/436], Loss: 2.2983\n",
      "Epoch [9/50], Step [410/436], Loss: 2.3077\n",
      "Epoch [9/50], Step [420/436], Loss: 2.3142\n",
      "Epoch [9/50], Step [430/436], Loss: 2.3030\n",
      "Epoch [9/50], Time: 32.11s\n",
      "Train Loss: 2.3024, Train Acc: 0.1072\n",
      "Val Loss: 2.3051, Val Acc: 0.1016\n",
      "Epoch [10/50], Step [10/436], Loss: 2.3024\n",
      "Epoch [10/50], Step [20/436], Loss: 2.3066\n",
      "Epoch [10/50], Step [30/436], Loss: 2.2937\n",
      "Epoch [10/50], Step [40/436], Loss: 2.3073\n",
      "Epoch [10/50], Step [50/436], Loss: 2.2970\n",
      "Epoch [10/50], Step [60/436], Loss: 2.3135\n",
      "Epoch [10/50], Step [70/436], Loss: 2.2912\n",
      "Epoch [10/50], Step [80/436], Loss: 2.2818\n",
      "Epoch [10/50], Step [90/436], Loss: 2.2940\n",
      "Epoch [10/50], Step [100/436], Loss: 2.3015\n",
      "Epoch [10/50], Step [110/436], Loss: 2.3051\n",
      "Epoch [10/50], Step [120/436], Loss: 2.2957\n",
      "Epoch [10/50], Step [130/436], Loss: 2.3012\n",
      "Epoch [10/50], Step [140/436], Loss: 2.3099\n",
      "Epoch [10/50], Step [150/436], Loss: 2.2856\n",
      "Epoch [10/50], Step [160/436], Loss: 2.2918\n",
      "Epoch [10/50], Step [170/436], Loss: 2.3070\n",
      "Epoch [10/50], Step [180/436], Loss: 2.3031\n",
      "Epoch [10/50], Step [190/436], Loss: 2.3027\n",
      "Epoch [10/50], Step [200/436], Loss: 2.3023\n",
      "Epoch [10/50], Step [210/436], Loss: 2.3034\n",
      "Epoch [10/50], Step [220/436], Loss: 2.3141\n",
      "Epoch [10/50], Step [230/436], Loss: 2.3012\n",
      "Epoch [10/50], Step [240/436], Loss: 2.3150\n",
      "Epoch [10/50], Step [250/436], Loss: 2.3099\n",
      "Epoch [10/50], Step [260/436], Loss: 2.3038\n",
      "Epoch [10/50], Step [270/436], Loss: 2.2839\n",
      "Epoch [10/50], Step [280/436], Loss: 2.3044\n",
      "Epoch [10/50], Step [290/436], Loss: 2.2862\n",
      "Epoch [10/50], Step [300/436], Loss: 2.3117\n",
      "Epoch [10/50], Step [310/436], Loss: 2.2983\n",
      "Epoch [10/50], Step [320/436], Loss: 2.3128\n",
      "Epoch [10/50], Step [330/436], Loss: 2.3170\n",
      "Epoch [10/50], Step [340/436], Loss: 2.3017\n",
      "Epoch [10/50], Step [350/436], Loss: 2.3117\n",
      "Epoch [10/50], Step [360/436], Loss: 2.3096\n",
      "Epoch [10/50], Step [370/436], Loss: 2.2902\n",
      "Epoch [10/50], Step [380/436], Loss: 2.3011\n",
      "Epoch [10/50], Step [390/436], Loss: 2.3106\n",
      "Epoch [10/50], Step [400/436], Loss: 2.3100\n",
      "Epoch [10/50], Step [410/436], Loss: 2.2911\n",
      "Epoch [10/50], Step [420/436], Loss: 2.3069\n",
      "Epoch [10/50], Step [430/436], Loss: 2.2918\n",
      "Epoch [10/50], Time: 32.03s\n",
      "Train Loss: 2.3029, Train Acc: 0.1057\n",
      "Val Loss: 2.3062, Val Acc: 0.1090\n",
      "New best model saved at epoch 10\n",
      "Epoch [11/50], Step [10/436], Loss: 2.2986\n",
      "Epoch [11/50], Step [20/436], Loss: 2.3027\n",
      "Epoch [11/50], Step [30/436], Loss: 2.3029\n",
      "Epoch [11/50], Step [40/436], Loss: 2.3008\n",
      "Epoch [11/50], Step [50/436], Loss: 2.2964\n",
      "Epoch [11/50], Step [60/436], Loss: 2.2938\n",
      "Epoch [11/50], Step [70/436], Loss: 2.2897\n",
      "Epoch [11/50], Step [80/436], Loss: 2.3044\n",
      "Epoch [11/50], Step [90/436], Loss: 2.2895\n",
      "Epoch [11/50], Step [100/436], Loss: 2.3100\n",
      "Epoch [11/50], Step [110/436], Loss: 2.2940\n",
      "Epoch [11/50], Step [120/436], Loss: 2.3175\n",
      "Epoch [11/50], Step [130/436], Loss: 2.3003\n",
      "Epoch [11/50], Step [140/436], Loss: 2.2959\n",
      "Epoch [11/50], Step [150/436], Loss: 2.2927\n",
      "Epoch [11/50], Step [160/436], Loss: 2.3136\n",
      "Epoch [11/50], Step [170/436], Loss: 2.3103\n",
      "Epoch [11/50], Step [180/436], Loss: 2.3077\n",
      "Epoch [11/50], Step [190/436], Loss: 2.3047\n",
      "Epoch [11/50], Step [200/436], Loss: 2.3138\n",
      "Epoch [11/50], Step [210/436], Loss: 2.2967\n",
      "Epoch [11/50], Step [220/436], Loss: 2.3082\n",
      "Epoch [11/50], Step [230/436], Loss: 2.3221\n",
      "Epoch [11/50], Step [240/436], Loss: 2.2744\n",
      "Epoch [11/50], Step [250/436], Loss: 2.3103\n",
      "Epoch [11/50], Step [260/436], Loss: 2.2992\n",
      "Epoch [11/50], Step [270/436], Loss: 2.3275\n",
      "Epoch [11/50], Step [280/436], Loss: 2.3127\n",
      "Epoch [11/50], Step [290/436], Loss: 2.2997\n",
      "Epoch [11/50], Step [300/436], Loss: 2.3114\n",
      "Epoch [11/50], Step [310/436], Loss: 2.3014\n",
      "Epoch [11/50], Step [320/436], Loss: 2.3106\n",
      "Epoch [11/50], Step [330/436], Loss: 2.2913\n",
      "Epoch [11/50], Step [340/436], Loss: 2.2936\n",
      "Epoch [11/50], Step [350/436], Loss: 2.2966\n",
      "Epoch [11/50], Step [360/436], Loss: 2.2938\n",
      "Epoch [11/50], Step [370/436], Loss: 2.2755\n",
      "Epoch [11/50], Step [380/436], Loss: 2.2876\n",
      "Epoch [11/50], Step [390/436], Loss: 2.3042\n",
      "Epoch [11/50], Step [400/436], Loss: 2.2934\n",
      "Epoch [11/50], Step [410/436], Loss: 2.3111\n",
      "Epoch [11/50], Step [420/436], Loss: 2.3046\n",
      "Epoch [11/50], Step [430/436], Loss: 2.3072\n",
      "Epoch [11/50], Time: 32.16s\n",
      "Train Loss: 2.3026, Train Acc: 0.1106\n",
      "Val Loss: 2.3066, Val Acc: 0.1103\n",
      "New best model saved at epoch 11\n",
      "Epoch [12/50], Step [10/436], Loss: 2.3206\n",
      "Epoch [12/50], Step [20/436], Loss: 2.3053\n",
      "Epoch [12/50], Step [30/436], Loss: 2.3058\n",
      "Epoch [12/50], Step [40/436], Loss: 2.3029\n",
      "Epoch [12/50], Step [50/436], Loss: 2.3054\n",
      "Epoch [12/50], Step [60/436], Loss: 2.2872\n",
      "Epoch [12/50], Step [70/436], Loss: 2.2997\n",
      "Epoch [12/50], Step [80/436], Loss: 2.2970\n",
      "Epoch [12/50], Step [90/436], Loss: 2.3116\n",
      "Epoch [12/50], Step [100/436], Loss: 2.3030\n",
      "Epoch [12/50], Step [110/436], Loss: 2.3196\n",
      "Epoch [12/50], Step [120/436], Loss: 2.3003\n",
      "Epoch [12/50], Step [130/436], Loss: 2.3037\n",
      "Epoch [12/50], Step [140/436], Loss: 2.2918\n",
      "Epoch [12/50], Step [150/436], Loss: 2.3019\n",
      "Epoch [12/50], Step [160/436], Loss: 2.2873\n",
      "Epoch [12/50], Step [170/436], Loss: 2.2979\n",
      "Epoch [12/50], Step [180/436], Loss: 2.2990\n",
      "Epoch [12/50], Step [190/436], Loss: 2.3058\n",
      "Epoch [12/50], Step [200/436], Loss: 2.3039\n",
      "Epoch [12/50], Step [210/436], Loss: 2.3037\n",
      "Epoch [12/50], Step [220/436], Loss: 2.2977\n",
      "Epoch [12/50], Step [230/436], Loss: 2.3014\n",
      "Epoch [12/50], Step [240/436], Loss: 2.2932\n",
      "Epoch [12/50], Step [250/436], Loss: 2.3145\n",
      "Epoch [12/50], Step [260/436], Loss: 2.2940\n",
      "Epoch [12/50], Step [270/436], Loss: 2.3108\n",
      "Epoch [12/50], Step [280/436], Loss: 2.2872\n",
      "Epoch [12/50], Step [290/436], Loss: 2.2966\n",
      "Epoch [12/50], Step [300/436], Loss: 2.3127\n",
      "Epoch [12/50], Step [310/436], Loss: 2.2883\n",
      "Epoch [12/50], Step [320/436], Loss: 2.3062\n",
      "Epoch [12/50], Step [330/436], Loss: 2.3047\n",
      "Epoch [12/50], Step [340/436], Loss: 2.2831\n",
      "Epoch [12/50], Step [350/436], Loss: 2.2944\n",
      "Epoch [12/50], Step [360/436], Loss: 2.3216\n",
      "Epoch [12/50], Step [370/436], Loss: 2.2800\n",
      "Epoch [12/50], Step [380/436], Loss: 2.2985\n",
      "Epoch [12/50], Step [390/436], Loss: 2.3062\n",
      "Epoch [12/50], Step [400/436], Loss: 2.2944\n",
      "Epoch [12/50], Step [410/436], Loss: 2.2862\n",
      "Epoch [12/50], Step [420/436], Loss: 2.3167\n",
      "Epoch [12/50], Step [430/436], Loss: 2.3149\n",
      "Epoch [12/50], Time: 32.98s\n",
      "Train Loss: 2.3028, Train Acc: 0.1064\n",
      "Val Loss: 2.3059, Val Acc: 0.1013\n",
      "Epoch [13/50], Step [10/436], Loss: 2.3059\n",
      "Epoch [13/50], Step [20/436], Loss: 2.2837\n",
      "Epoch [13/50], Step [30/436], Loss: 2.3094\n",
      "Epoch [13/50], Step [40/436], Loss: 2.2992\n",
      "Epoch [13/50], Step [50/436], Loss: 2.2974\n",
      "Epoch [13/50], Step [60/436], Loss: 2.2885\n",
      "Epoch [13/50], Step [70/436], Loss: 2.2995\n",
      "Epoch [13/50], Step [80/436], Loss: 2.2902\n",
      "Epoch [13/50], Step [90/436], Loss: 2.2854\n",
      "Epoch [13/50], Step [100/436], Loss: 2.2910\n",
      "Epoch [13/50], Step [110/436], Loss: 2.2893\n",
      "Epoch [13/50], Step [120/436], Loss: 2.2928\n",
      "Epoch [13/50], Step [130/436], Loss: 2.2981\n",
      "Epoch [13/50], Step [140/436], Loss: 2.3010\n",
      "Epoch [13/50], Step [150/436], Loss: 2.2889\n",
      "Epoch [13/50], Step [160/436], Loss: 2.3001\n",
      "Epoch [13/50], Step [170/436], Loss: 2.2960\n",
      "Epoch [13/50], Step [180/436], Loss: 2.2957\n",
      "Epoch [13/50], Step [190/436], Loss: 2.3156\n",
      "Epoch [13/50], Step [200/436], Loss: 2.3016\n",
      "Epoch [13/50], Step [210/436], Loss: 2.3159\n",
      "Epoch [13/50], Step [220/436], Loss: 2.3076\n",
      "Epoch [13/50], Step [230/436], Loss: 2.3023\n",
      "Epoch [13/50], Step [240/436], Loss: 2.2908\n",
      "Epoch [13/50], Step [250/436], Loss: 2.3105\n",
      "Epoch [13/50], Step [260/436], Loss: 2.3006\n",
      "Epoch [13/50], Step [270/436], Loss: 2.3129\n",
      "Epoch [13/50], Step [280/436], Loss: 2.3062\n",
      "Epoch [13/50], Step [290/436], Loss: 2.3144\n",
      "Epoch [13/50], Step [300/436], Loss: 2.3072\n",
      "Epoch [13/50], Step [310/436], Loss: 2.3223\n",
      "Epoch [13/50], Step [320/436], Loss: 2.3126\n",
      "Epoch [13/50], Step [330/436], Loss: 2.3161\n",
      "Epoch [13/50], Step [340/436], Loss: 2.3066\n",
      "Epoch [13/50], Step [350/436], Loss: 2.2860\n",
      "Epoch [13/50], Step [360/436], Loss: 2.3118\n",
      "Epoch [13/50], Step [370/436], Loss: 2.3137\n",
      "Epoch [13/50], Step [380/436], Loss: 2.3032\n",
      "Epoch [13/50], Step [390/436], Loss: 2.2942\n",
      "Epoch [13/50], Step [400/436], Loss: 2.3138\n",
      "Epoch [13/50], Step [410/436], Loss: 2.3050\n",
      "Epoch [13/50], Step [420/436], Loss: 2.2917\n",
      "Epoch [13/50], Step [430/436], Loss: 2.2966\n",
      "Epoch [13/50], Time: 33.21s\n",
      "Train Loss: 2.3022, Train Acc: 0.1077\n",
      "Val Loss: 2.3048, Val Acc: 0.1058\n",
      "Epoch [14/50], Step [10/436], Loss: 2.3307\n",
      "Epoch [14/50], Step [20/436], Loss: 2.2999\n",
      "Epoch [14/50], Step [30/436], Loss: 2.2932\n",
      "Epoch [14/50], Step [40/436], Loss: 2.3019\n",
      "Epoch [14/50], Step [50/436], Loss: 2.3221\n",
      "Epoch [14/50], Step [60/436], Loss: 2.3054\n",
      "Epoch [14/50], Step [70/436], Loss: 2.3138\n",
      "Epoch [14/50], Step [80/436], Loss: 2.2899\n",
      "Epoch [14/50], Step [90/436], Loss: 2.2840\n",
      "Epoch [14/50], Step [100/436], Loss: 2.2873\n",
      "Epoch [14/50], Step [110/436], Loss: 2.3033\n",
      "Epoch [14/50], Step [120/436], Loss: 2.3057\n",
      "Epoch [14/50], Step [130/436], Loss: 2.2967\n",
      "Epoch [14/50], Step [140/436], Loss: 2.3073\n",
      "Epoch [14/50], Step [150/436], Loss: 2.2962\n",
      "Epoch [14/50], Step [160/436], Loss: 2.2865\n",
      "Epoch [14/50], Step [170/436], Loss: 2.3104\n",
      "Epoch [14/50], Step [180/436], Loss: 2.3021\n",
      "Epoch [14/50], Step [190/436], Loss: 2.3161\n",
      "Epoch [14/50], Step [200/436], Loss: 2.2993\n",
      "Epoch [14/50], Step [210/436], Loss: 2.3132\n",
      "Epoch [14/50], Step [220/436], Loss: 2.2948\n",
      "Epoch [14/50], Step [230/436], Loss: 2.3201\n",
      "Epoch [14/50], Step [240/436], Loss: 2.2991\n",
      "Epoch [14/50], Step [250/436], Loss: 2.2969\n",
      "Epoch [14/50], Step [260/436], Loss: 2.2955\n",
      "Epoch [14/50], Step [270/436], Loss: 2.3014\n",
      "Epoch [14/50], Step [280/436], Loss: 2.3012\n",
      "Epoch [14/50], Step [290/436], Loss: 2.3137\n",
      "Epoch [14/50], Step [300/436], Loss: 2.3116\n",
      "Epoch [14/50], Step [310/436], Loss: 2.2911\n",
      "Epoch [14/50], Step [320/436], Loss: 2.3088\n",
      "Epoch [14/50], Step [330/436], Loss: 2.2978\n",
      "Epoch [14/50], Step [340/436], Loss: 2.2955\n",
      "Epoch [14/50], Step [350/436], Loss: 2.2996\n",
      "Epoch [14/50], Step [360/436], Loss: 2.2958\n",
      "Epoch [14/50], Step [370/436], Loss: 2.3169\n",
      "Epoch [14/50], Step [380/436], Loss: 2.3099\n",
      "Epoch [14/50], Step [390/436], Loss: 2.3006\n",
      "Epoch [14/50], Step [400/436], Loss: 2.3117\n",
      "Epoch [14/50], Step [410/436], Loss: 2.2906\n",
      "Epoch [14/50], Step [420/436], Loss: 2.2897\n",
      "Epoch [14/50], Step [430/436], Loss: 2.3023\n",
      "Epoch [14/50], Time: 31.99s\n",
      "Train Loss: 2.3017, Train Acc: 0.1127\n",
      "Val Loss: 2.3049, Val Acc: 0.1016\n",
      "Epoch [15/50], Step [10/436], Loss: 2.2910\n",
      "Epoch [15/50], Step [20/436], Loss: 2.3024\n",
      "Epoch [15/50], Step [30/436], Loss: 2.3052\n",
      "Epoch [15/50], Step [40/436], Loss: 2.3068\n",
      "Epoch [15/50], Step [50/436], Loss: 2.3036\n",
      "Epoch [15/50], Step [60/436], Loss: 2.2951\n",
      "Epoch [15/50], Step [70/436], Loss: 2.3043\n",
      "Epoch [15/50], Step [80/436], Loss: 2.2880\n",
      "Epoch [15/50], Step [90/436], Loss: 2.2931\n",
      "Epoch [15/50], Step [100/436], Loss: 2.2889\n",
      "Epoch [15/50], Step [110/436], Loss: 2.3048\n",
      "Epoch [15/50], Step [120/436], Loss: 2.3151\n",
      "Epoch [15/50], Step [130/436], Loss: 2.3122\n",
      "Epoch [15/50], Step [140/436], Loss: 2.3148\n",
      "Epoch [15/50], Step [150/436], Loss: 2.2941\n",
      "Epoch [15/50], Step [160/436], Loss: 2.3222\n",
      "Epoch [15/50], Step [170/436], Loss: 2.3050\n",
      "Epoch [15/50], Step [180/436], Loss: 2.2888\n",
      "Epoch [15/50], Step [190/436], Loss: 2.3297\n",
      "Epoch [15/50], Step [200/436], Loss: 2.2730\n",
      "Epoch [15/50], Step [210/436], Loss: 2.2999\n",
      "Epoch [15/50], Step [220/436], Loss: 2.3100\n",
      "Epoch [15/50], Step [230/436], Loss: 2.3028\n",
      "Epoch [15/50], Step [240/436], Loss: 2.2935\n",
      "Epoch [15/50], Step [250/436], Loss: 2.3027\n",
      "Epoch [15/50], Step [260/436], Loss: 2.2912\n",
      "Epoch [15/50], Step [270/436], Loss: 2.2971\n",
      "Epoch [15/50], Step [280/436], Loss: 2.2928\n",
      "Epoch [15/50], Step [290/436], Loss: 2.3093\n",
      "Epoch [15/50], Step [300/436], Loss: 2.3040\n",
      "Epoch [15/50], Step [310/436], Loss: 2.2922\n",
      "Epoch [15/50], Step [320/436], Loss: 2.2985\n",
      "Epoch [15/50], Step [330/436], Loss: 2.3116\n",
      "Epoch [15/50], Step [340/436], Loss: 2.3027\n",
      "Epoch [15/50], Step [350/436], Loss: 2.3146\n",
      "Epoch [15/50], Step [360/436], Loss: 2.3200\n",
      "Epoch [15/50], Step [370/436], Loss: 2.3025\n",
      "Epoch [15/50], Step [380/436], Loss: 2.3014\n",
      "Epoch [15/50], Step [390/436], Loss: 2.2994\n",
      "Epoch [15/50], Step [400/436], Loss: 2.3120\n",
      "Epoch [15/50], Step [410/436], Loss: 2.3148\n",
      "Epoch [15/50], Step [420/436], Loss: 2.2950\n",
      "Epoch [15/50], Step [430/436], Loss: 2.2960\n",
      "Epoch [15/50], Time: 31.86s\n",
      "Train Loss: 2.3015, Train Acc: 0.1103\n",
      "Val Loss: 2.3040, Val Acc: 0.1074\n",
      "Epoch [16/50], Step [10/436], Loss: 2.2894\n",
      "Epoch [16/50], Step [20/436], Loss: 2.2977\n",
      "Epoch [16/50], Step [30/436], Loss: 2.3147\n",
      "Epoch [16/50], Step [40/436], Loss: 2.3170\n",
      "Epoch [16/50], Step [50/436], Loss: 2.2920\n",
      "Epoch [16/50], Step [60/436], Loss: 2.3128\n",
      "Epoch [16/50], Step [70/436], Loss: 2.2970\n",
      "Epoch [16/50], Step [80/436], Loss: 2.2942\n",
      "Epoch [16/50], Step [90/436], Loss: 2.3095\n",
      "Epoch [16/50], Step [100/436], Loss: 2.2943\n",
      "Epoch [16/50], Step [110/436], Loss: 2.3069\n",
      "Epoch [16/50], Step [120/436], Loss: 2.3178\n",
      "Epoch [16/50], Step [130/436], Loss: 2.3024\n",
      "Epoch [16/50], Step [140/436], Loss: 2.3007\n",
      "Epoch [16/50], Step [150/436], Loss: 2.3017\n",
      "Epoch [16/50], Step [160/436], Loss: 2.3068\n",
      "Epoch [16/50], Step [170/436], Loss: 2.2770\n",
      "Epoch [16/50], Step [180/436], Loss: 2.2995\n",
      "Epoch [16/50], Step [190/436], Loss: 2.2952\n",
      "Epoch [16/50], Step [200/436], Loss: 2.3022\n",
      "Epoch [16/50], Step [210/436], Loss: 2.3180\n",
      "Epoch [16/50], Step [220/436], Loss: 2.2908\n",
      "Epoch [16/50], Step [230/436], Loss: 2.3055\n",
      "Epoch [16/50], Step [240/436], Loss: 2.3146\n",
      "Epoch [16/50], Step [250/436], Loss: 2.3121\n",
      "Epoch [16/50], Step [260/436], Loss: 2.3102\n",
      "Epoch [16/50], Step [270/436], Loss: 2.3036\n",
      "Epoch [16/50], Step [280/436], Loss: 2.2881\n",
      "Epoch [16/50], Step [290/436], Loss: 2.2997\n",
      "Epoch [16/50], Step [300/436], Loss: 2.2738\n",
      "Epoch [16/50], Step [310/436], Loss: 2.2894\n",
      "Epoch [16/50], Step [320/436], Loss: 2.3060\n",
      "Epoch [16/50], Step [330/436], Loss: 2.2925\n",
      "Epoch [16/50], Step [340/436], Loss: 2.3118\n",
      "Epoch [16/50], Step [350/436], Loss: 2.3073\n",
      "Epoch [16/50], Step [360/436], Loss: 2.3166\n",
      "Epoch [16/50], Step [370/436], Loss: 2.2934\n",
      "Epoch [16/50], Step [380/436], Loss: 2.3035\n",
      "Epoch [16/50], Step [390/436], Loss: 2.2866\n",
      "Epoch [16/50], Step [400/436], Loss: 2.2998\n",
      "Epoch [16/50], Step [410/436], Loss: 2.2914\n",
      "Epoch [16/50], Step [420/436], Loss: 2.2822\n",
      "Epoch [16/50], Step [430/436], Loss: 2.2998\n",
      "Epoch [16/50], Time: 31.97s\n",
      "Train Loss: 2.3024, Train Acc: 0.1089\n",
      "Val Loss: 2.3043, Val Acc: 0.0997\n",
      "Epoch [17/50], Step [10/436], Loss: 2.2923\n",
      "Epoch [17/50], Step [20/436], Loss: 2.2970\n",
      "Epoch [17/50], Step [30/436], Loss: 2.3034\n",
      "Epoch [17/50], Step [40/436], Loss: 2.2974\n",
      "Epoch [17/50], Step [50/436], Loss: 2.3124\n",
      "Epoch [17/50], Step [60/436], Loss: 2.3195\n",
      "Epoch [17/50], Step [70/436], Loss: 2.2839\n",
      "Epoch [17/50], Step [80/436], Loss: 2.3168\n",
      "Epoch [17/50], Step [90/436], Loss: 2.3023\n",
      "Epoch [17/50], Step [100/436], Loss: 2.2990\n",
      "Epoch [17/50], Step [110/436], Loss: 2.2999\n",
      "Epoch [17/50], Step [120/436], Loss: 2.3068\n",
      "Epoch [17/50], Step [130/436], Loss: 2.3089\n",
      "Epoch [17/50], Step [140/436], Loss: 2.3173\n",
      "Epoch [17/50], Step [150/436], Loss: 2.3080\n",
      "Epoch [17/50], Step [160/436], Loss: 2.2952\n",
      "Epoch [17/50], Step [170/436], Loss: 2.2929\n",
      "Epoch [17/50], Step [180/436], Loss: 2.3046\n",
      "Epoch [17/50], Step [190/436], Loss: 2.3200\n",
      "Epoch [17/50], Step [200/436], Loss: 2.2978\n",
      "Epoch [17/50], Step [210/436], Loss: 2.2959\n",
      "Epoch [17/50], Step [220/436], Loss: 2.3030\n",
      "Epoch [17/50], Step [230/436], Loss: 2.2835\n",
      "Epoch [17/50], Step [240/436], Loss: 2.2949\n",
      "Epoch [17/50], Step [250/436], Loss: 2.2894\n",
      "Epoch [17/50], Step [260/436], Loss: 2.2890\n",
      "Epoch [17/50], Step [270/436], Loss: 2.3168\n",
      "Epoch [17/50], Step [280/436], Loss: 2.2948\n",
      "Epoch [17/50], Step [290/436], Loss: 2.3147\n",
      "Epoch [17/50], Step [300/436], Loss: 2.2975\n",
      "Epoch [17/50], Step [310/436], Loss: 2.3049\n",
      "Epoch [17/50], Step [320/436], Loss: 2.2979\n",
      "Epoch [17/50], Step [330/436], Loss: 2.3179\n",
      "Epoch [17/50], Step [340/436], Loss: 2.3046\n",
      "Epoch [17/50], Step [350/436], Loss: 2.3048\n",
      "Epoch [17/50], Step [360/436], Loss: 2.3068\n",
      "Epoch [17/50], Step [370/436], Loss: 2.2873\n",
      "Epoch [17/50], Step [380/436], Loss: 2.2929\n",
      "Epoch [17/50], Step [390/436], Loss: 2.3100\n",
      "Epoch [17/50], Step [400/436], Loss: 2.3057\n",
      "Epoch [17/50], Step [410/436], Loss: 2.3083\n",
      "Epoch [17/50], Step [420/436], Loss: 2.2964\n",
      "Epoch [17/50], Step [430/436], Loss: 2.3092\n",
      "Epoch [17/50], Time: 31.96s\n",
      "Train Loss: 2.3023, Train Acc: 0.1058\n",
      "Val Loss: 2.3062, Val Acc: 0.0990\n",
      "Epoch [18/50], Step [10/436], Loss: 2.3068\n",
      "Epoch [18/50], Step [20/436], Loss: 2.3052\n",
      "Epoch [18/50], Step [30/436], Loss: 2.2931\n",
      "Epoch [18/50], Step [40/436], Loss: 2.3112\n",
      "Epoch [18/50], Step [50/436], Loss: 2.2933\n",
      "Epoch [18/50], Step [60/436], Loss: 2.3067\n",
      "Epoch [18/50], Step [70/436], Loss: 2.2903\n",
      "Epoch [18/50], Step [80/436], Loss: 2.3004\n",
      "Epoch [18/50], Step [90/436], Loss: 2.2836\n",
      "Epoch [18/50], Step [100/436], Loss: 2.2951\n",
      "Epoch [18/50], Step [110/436], Loss: 2.3003\n",
      "Epoch [18/50], Step [120/436], Loss: 2.2960\n",
      "Epoch [18/50], Step [130/436], Loss: 2.3033\n",
      "Epoch [18/50], Step [140/436], Loss: 2.2837\n",
      "Epoch [18/50], Step [150/436], Loss: 2.3162\n",
      "Epoch [18/50], Step [160/436], Loss: 2.3027\n",
      "Epoch [18/50], Step [170/436], Loss: 2.3144\n",
      "Epoch [18/50], Step [180/436], Loss: 2.3110\n",
      "Epoch [18/50], Step [190/436], Loss: 2.2909\n",
      "Epoch [18/50], Step [200/436], Loss: 2.3175\n",
      "Epoch [18/50], Step [210/436], Loss: 2.2942\n",
      "Epoch [18/50], Step [220/436], Loss: 2.3129\n",
      "Epoch [18/50], Step [230/436], Loss: 2.3016\n",
      "Epoch [18/50], Step [240/436], Loss: 2.2954\n",
      "Epoch [18/50], Step [250/436], Loss: 2.3016\n",
      "Epoch [18/50], Step [260/436], Loss: 2.2964\n",
      "Epoch [18/50], Step [270/436], Loss: 2.3033\n",
      "Epoch [18/50], Step [280/436], Loss: 2.3059\n",
      "Epoch [18/50], Step [290/436], Loss: 2.2993\n",
      "Epoch [18/50], Step [300/436], Loss: 2.2939\n",
      "Epoch [18/50], Step [310/436], Loss: 2.2995\n",
      "Epoch [18/50], Step [320/436], Loss: 2.3033\n",
      "Epoch [18/50], Step [330/436], Loss: 2.2934\n",
      "Epoch [18/50], Step [340/436], Loss: 2.3109\n",
      "Epoch [18/50], Step [350/436], Loss: 2.3032\n",
      "Epoch [18/50], Step [360/436], Loss: 2.3129\n",
      "Epoch [18/50], Step [370/436], Loss: 2.2901\n",
      "Epoch [18/50], Step [380/436], Loss: 2.3008\n",
      "Epoch [18/50], Step [390/436], Loss: 2.2993\n",
      "Epoch [18/50], Step [400/436], Loss: 2.3138\n",
      "Epoch [18/50], Step [410/436], Loss: 2.3003\n",
      "Epoch [18/50], Step [420/436], Loss: 2.3029\n",
      "Epoch [18/50], Step [430/436], Loss: 2.3206\n",
      "Epoch [18/50], Time: 31.78s\n",
      "Train Loss: 2.3021, Train Acc: 0.1103\n",
      "Val Loss: 2.3044, Val Acc: 0.1035\n",
      "Epoch [19/50], Step [10/436], Loss: 2.3028\n",
      "Epoch [19/50], Step [20/436], Loss: 2.2891\n",
      "Epoch [19/50], Step [30/436], Loss: 2.3088\n",
      "Epoch [19/50], Step [40/436], Loss: 2.3014\n",
      "Epoch [19/50], Step [50/436], Loss: 2.3151\n",
      "Epoch [19/50], Step [60/436], Loss: 2.2891\n",
      "Epoch [19/50], Step [70/436], Loss: 2.3173\n",
      "Epoch [19/50], Step [80/436], Loss: 2.3173\n",
      "Epoch [19/50], Step [90/436], Loss: 2.2956\n",
      "Epoch [19/50], Step [100/436], Loss: 2.2970\n",
      "Epoch [19/50], Step [110/436], Loss: 2.2953\n",
      "Epoch [19/50], Step [120/436], Loss: 2.3078\n",
      "Epoch [19/50], Step [130/436], Loss: 2.2978\n",
      "Epoch [19/50], Step [140/436], Loss: 2.3061\n",
      "Epoch [19/50], Step [150/436], Loss: 2.2865\n",
      "Epoch [19/50], Step [160/436], Loss: 2.3236\n",
      "Epoch [19/50], Step [170/436], Loss: 2.2849\n",
      "Epoch [19/50], Step [180/436], Loss: 2.3080\n",
      "Epoch [19/50], Step [190/436], Loss: 2.3068\n",
      "Epoch [19/50], Step [200/436], Loss: 2.3011\n",
      "Epoch [19/50], Step [210/436], Loss: 2.2894\n",
      "Epoch [19/50], Step [220/436], Loss: 2.3128\n",
      "Epoch [19/50], Step [230/436], Loss: 2.3084\n",
      "Epoch [19/50], Step [240/436], Loss: 2.2898\n",
      "Epoch [19/50], Step [250/436], Loss: 2.3082\n",
      "Epoch [19/50], Step [260/436], Loss: 2.2985\n",
      "Epoch [19/50], Step [270/436], Loss: 2.2894\n",
      "Epoch [19/50], Step [280/436], Loss: 2.3008\n",
      "Epoch [19/50], Step [290/436], Loss: 2.3074\n",
      "Epoch [19/50], Step [300/436], Loss: 2.3092\n",
      "Epoch [19/50], Step [310/436], Loss: 2.2968\n",
      "Epoch [19/50], Step [320/436], Loss: 2.3030\n",
      "Epoch [19/50], Step [330/436], Loss: 2.3120\n",
      "Epoch [19/50], Step [340/436], Loss: 2.3002\n",
      "Epoch [19/50], Step [350/436], Loss: 2.2811\n",
      "Epoch [19/50], Step [360/436], Loss: 2.3112\n",
      "Epoch [19/50], Step [370/436], Loss: 2.2921\n",
      "Epoch [19/50], Step [380/436], Loss: 2.2961\n",
      "Epoch [19/50], Step [390/436], Loss: 2.3141\n",
      "Epoch [19/50], Step [400/436], Loss: 2.3025\n",
      "Epoch [19/50], Step [410/436], Loss: 2.2976\n",
      "Epoch [19/50], Step [420/436], Loss: 2.2999\n",
      "Epoch [19/50], Step [430/436], Loss: 2.3192\n",
      "Epoch [19/50], Time: 31.82s\n",
      "Train Loss: 2.3021, Train Acc: 0.1042\n",
      "Val Loss: 2.3046, Val Acc: 0.1026\n",
      "Epoch [20/50], Step [10/436], Loss: 2.3055\n",
      "Epoch [20/50], Step [20/436], Loss: 2.3171\n",
      "Epoch [20/50], Step [30/436], Loss: 2.3064\n",
      "Epoch [20/50], Step [40/436], Loss: 2.2996\n",
      "Epoch [20/50], Step [50/436], Loss: 2.2971\n",
      "Epoch [20/50], Step [60/436], Loss: 2.2900\n",
      "Epoch [20/50], Step [70/436], Loss: 2.3055\n",
      "Epoch [20/50], Step [80/436], Loss: 2.3021\n",
      "Epoch [20/50], Step [90/436], Loss: 2.3032\n",
      "Epoch [20/50], Step [100/436], Loss: 2.2987\n",
      "Epoch [20/50], Step [110/436], Loss: 2.2957\n",
      "Epoch [20/50], Step [120/436], Loss: 2.3012\n",
      "Epoch [20/50], Step [130/436], Loss: 2.3108\n",
      "Epoch [20/50], Step [140/436], Loss: 2.3053\n",
      "Epoch [20/50], Step [150/436], Loss: 2.3060\n",
      "Epoch [20/50], Step [160/436], Loss: 2.3010\n",
      "Epoch [20/50], Step [170/436], Loss: 2.2985\n",
      "Epoch [20/50], Step [180/436], Loss: 2.2966\n",
      "Epoch [20/50], Step [190/436], Loss: 2.2888\n",
      "Epoch [20/50], Step [200/436], Loss: 2.2910\n",
      "Epoch [20/50], Step [210/436], Loss: 2.3103\n",
      "Epoch [20/50], Step [220/436], Loss: 2.3232\n",
      "Epoch [20/50], Step [230/436], Loss: 2.2984\n",
      "Epoch [20/50], Step [240/436], Loss: 2.3039\n",
      "Epoch [20/50], Step [250/436], Loss: 2.3055\n",
      "Epoch [20/50], Step [260/436], Loss: 2.3129\n",
      "Epoch [20/50], Step [270/436], Loss: 2.2933\n",
      "Epoch [20/50], Step [280/436], Loss: 2.3150\n",
      "Epoch [20/50], Step [290/436], Loss: 2.2984\n",
      "Epoch [20/50], Step [300/436], Loss: 2.2914\n",
      "Epoch [20/50], Step [310/436], Loss: 2.3087\n",
      "Epoch [20/50], Step [320/436], Loss: 2.3004\n",
      "Epoch [20/50], Step [330/436], Loss: 2.3052\n",
      "Epoch [20/50], Step [340/436], Loss: 2.3042\n",
      "Epoch [20/50], Step [350/436], Loss: 2.2961\n",
      "Epoch [20/50], Step [360/436], Loss: 2.2793\n",
      "Epoch [20/50], Step [370/436], Loss: 2.3275\n",
      "Epoch [20/50], Step [380/436], Loss: 2.3001\n",
      "Epoch [20/50], Step [390/436], Loss: 2.2933\n",
      "Epoch [20/50], Step [400/436], Loss: 2.2962\n",
      "Epoch [20/50], Step [410/436], Loss: 2.2974\n",
      "Epoch [20/50], Step [420/436], Loss: 2.3133\n",
      "Epoch [20/50], Step [430/436], Loss: 2.3114\n",
      "Epoch [20/50], Time: 31.59s\n",
      "Train Loss: 2.3020, Train Acc: 0.1114\n",
      "Val Loss: 2.3048, Val Acc: 0.0977\n",
      "Epoch [21/50], Step [10/436], Loss: 2.2941\n",
      "Epoch [21/50], Step [20/436], Loss: 2.2913\n",
      "Epoch [21/50], Step [30/436], Loss: 2.3181\n",
      "Epoch [21/50], Step [40/436], Loss: 2.2983\n",
      "Epoch [21/50], Step [50/436], Loss: 2.2950\n",
      "Epoch [21/50], Step [60/436], Loss: 2.2961\n",
      "Epoch [21/50], Step [70/436], Loss: 2.3002\n",
      "Epoch [21/50], Step [80/436], Loss: 2.2920\n",
      "Epoch [21/50], Step [90/436], Loss: 2.2970\n",
      "Epoch [21/50], Step [100/436], Loss: 2.3024\n",
      "Epoch [21/50], Step [110/436], Loss: 2.3217\n",
      "Epoch [21/50], Step [120/436], Loss: 2.2944\n",
      "Epoch [21/50], Step [130/436], Loss: 2.3094\n",
      "Epoch [21/50], Step [140/436], Loss: 2.3012\n",
      "Epoch [21/50], Step [150/436], Loss: 2.3060\n",
      "Epoch [21/50], Step [160/436], Loss: 2.3090\n",
      "Epoch [21/50], Step [170/436], Loss: 2.2840\n",
      "Epoch [21/50], Step [180/436], Loss: 2.3217\n",
      "Epoch [21/50], Step [190/436], Loss: 2.3137\n",
      "Epoch [21/50], Step [200/436], Loss: 2.3015\n",
      "Epoch [21/50], Step [210/436], Loss: 2.3069\n",
      "Epoch [21/50], Step [220/436], Loss: 2.2937\n",
      "Epoch [21/50], Step [230/436], Loss: 2.3037\n",
      "Epoch [21/50], Step [240/436], Loss: 2.2956\n",
      "Epoch [21/50], Step [250/436], Loss: 2.3169\n",
      "Epoch [21/50], Step [260/436], Loss: 2.3053\n",
      "Epoch [21/50], Step [270/436], Loss: 2.3070\n",
      "Epoch [21/50], Step [280/436], Loss: 2.3090\n",
      "Epoch [21/50], Step [290/436], Loss: 2.3093\n",
      "Epoch [21/50], Step [300/436], Loss: 2.3081\n",
      "Epoch [21/50], Step [310/436], Loss: 2.3016\n",
      "Epoch [21/50], Step [320/436], Loss: 2.3103\n",
      "Epoch [21/50], Step [330/436], Loss: 2.3191\n",
      "Epoch [21/50], Step [340/436], Loss: 2.3068\n",
      "Epoch [21/50], Step [350/436], Loss: 2.3024\n",
      "Epoch [21/50], Step [360/436], Loss: 2.3060\n",
      "Epoch [21/50], Step [370/436], Loss: 2.3057\n",
      "Epoch [21/50], Step [380/436], Loss: 2.2895\n",
      "Epoch [21/50], Step [390/436], Loss: 2.3038\n",
      "Epoch [21/50], Step [400/436], Loss: 2.3089\n",
      "Epoch [21/50], Step [410/436], Loss: 2.2892\n",
      "Epoch [21/50], Step [420/436], Loss: 2.3074\n",
      "Epoch [21/50], Step [430/436], Loss: 2.3100\n",
      "Epoch [21/50], Time: 31.80s\n",
      "Train Loss: 2.3020, Train Acc: 0.1074\n",
      "Val Loss: 2.3046, Val Acc: 0.1052\n",
      "Epoch [22/50], Step [10/436], Loss: 2.3017\n",
      "Epoch [22/50], Step [20/436], Loss: 2.3152\n",
      "Epoch [22/50], Step [30/436], Loss: 2.3156\n",
      "Epoch [22/50], Step [40/436], Loss: 2.3158\n",
      "Epoch [22/50], Step [50/436], Loss: 2.3096\n",
      "Epoch [22/50], Step [60/436], Loss: 2.2986\n",
      "Epoch [22/50], Step [70/436], Loss: 2.2988\n",
      "Epoch [22/50], Step [80/436], Loss: 2.3113\n",
      "Epoch [22/50], Step [90/436], Loss: 2.3009\n",
      "Epoch [22/50], Step [100/436], Loss: 2.2949\n",
      "Epoch [22/50], Step [110/436], Loss: 2.3065\n",
      "Epoch [22/50], Step [120/436], Loss: 2.2969\n",
      "Epoch [22/50], Step [130/436], Loss: 2.2994\n",
      "Epoch [22/50], Step [140/436], Loss: 2.3037\n",
      "Epoch [22/50], Step [150/436], Loss: 2.3072\n",
      "Epoch [22/50], Step [160/436], Loss: 2.3004\n",
      "Epoch [22/50], Step [170/436], Loss: 2.2985\n",
      "Epoch [22/50], Step [180/436], Loss: 2.2986\n",
      "Epoch [22/50], Step [190/436], Loss: 2.2977\n",
      "Epoch [22/50], Step [200/436], Loss: 2.3184\n",
      "Epoch [22/50], Step [210/436], Loss: 2.3189\n",
      "Epoch [22/50], Step [220/436], Loss: 2.3038\n",
      "Epoch [22/50], Step [230/436], Loss: 2.3241\n",
      "Epoch [22/50], Step [240/436], Loss: 2.2949\n",
      "Epoch [22/50], Step [250/436], Loss: 2.3054\n",
      "Epoch [22/50], Step [260/436], Loss: 2.3114\n",
      "Epoch [22/50], Step [270/436], Loss: 2.3069\n",
      "Epoch [22/50], Step [280/436], Loss: 2.3089\n",
      "Epoch [22/50], Step [290/436], Loss: 2.3059\n",
      "Epoch [22/50], Step [300/436], Loss: 2.3051\n",
      "Epoch [22/50], Step [310/436], Loss: 2.2825\n",
      "Epoch [22/50], Step [320/436], Loss: 2.2900\n",
      "Epoch [22/50], Step [330/436], Loss: 2.3200\n",
      "Epoch [22/50], Step [340/436], Loss: 2.2930\n",
      "Epoch [22/50], Step [350/436], Loss: 2.2958\n",
      "Epoch [22/50], Step [360/436], Loss: 2.3034\n",
      "Epoch [22/50], Step [370/436], Loss: 2.3083\n",
      "Epoch [22/50], Step [380/436], Loss: 2.2924\n",
      "Epoch [22/50], Step [390/436], Loss: 2.3214\n",
      "Epoch [22/50], Step [400/436], Loss: 2.2947\n",
      "Epoch [22/50], Step [410/436], Loss: 2.3006\n",
      "Epoch [22/50], Step [420/436], Loss: 2.3152\n",
      "Epoch [22/50], Step [430/436], Loss: 2.3060\n",
      "Epoch [22/50], Time: 31.61s\n",
      "Train Loss: 2.3023, Train Acc: 0.1087\n",
      "Val Loss: 2.3033, Val Acc: 0.1032\n",
      "Epoch [23/50], Step [10/436], Loss: 2.3152\n",
      "Epoch [23/50], Step [20/436], Loss: 2.3137\n",
      "Epoch [23/50], Step [30/436], Loss: 2.2989\n",
      "Epoch [23/50], Step [40/436], Loss: 2.2914\n",
      "Epoch [23/50], Step [50/436], Loss: 2.3159\n",
      "Epoch [23/50], Step [60/436], Loss: 2.3208\n",
      "Epoch [23/50], Step [70/436], Loss: 2.3108\n",
      "Epoch [23/50], Step [80/436], Loss: 2.3183\n",
      "Epoch [23/50], Step [90/436], Loss: 2.2968\n",
      "Epoch [23/50], Step [100/436], Loss: 2.3081\n",
      "Epoch [23/50], Step [110/436], Loss: 2.3015\n",
      "Epoch [23/50], Step [120/436], Loss: 2.3052\n",
      "Epoch [23/50], Step [130/436], Loss: 2.2938\n",
      "Epoch [23/50], Step [140/436], Loss: 2.2998\n",
      "Epoch [23/50], Step [150/436], Loss: 2.2922\n",
      "Epoch [23/50], Step [160/436], Loss: 2.3067\n",
      "Epoch [23/50], Step [170/436], Loss: 2.3084\n",
      "Epoch [23/50], Step [180/436], Loss: 2.3026\n",
      "Epoch [23/50], Step [190/436], Loss: 2.3060\n",
      "Epoch [23/50], Step [200/436], Loss: 2.3012\n",
      "Epoch [23/50], Step [210/436], Loss: 2.3033\n",
      "Epoch [23/50], Step [220/436], Loss: 2.2785\n",
      "Epoch [23/50], Step [230/436], Loss: 2.2945\n",
      "Epoch [23/50], Step [240/436], Loss: 2.2892\n",
      "Epoch [23/50], Step [250/436], Loss: 2.2960\n",
      "Epoch [23/50], Step [260/436], Loss: 2.3155\n",
      "Epoch [23/50], Step [270/436], Loss: 2.3006\n",
      "Epoch [23/50], Step [280/436], Loss: 2.3042\n",
      "Epoch [23/50], Step [290/436], Loss: 2.2985\n",
      "Epoch [23/50], Step [300/436], Loss: 2.2998\n",
      "Epoch [23/50], Step [310/436], Loss: 2.3024\n",
      "Epoch [23/50], Step [320/436], Loss: 2.3128\n",
      "Epoch [23/50], Step [330/436], Loss: 2.3124\n",
      "Epoch [23/50], Step [340/436], Loss: 2.2856\n",
      "Epoch [23/50], Step [350/436], Loss: 2.3131\n",
      "Epoch [23/50], Step [360/436], Loss: 2.2982\n",
      "Epoch [23/50], Step [370/436], Loss: 2.2921\n",
      "Epoch [23/50], Step [380/436], Loss: 2.2900\n",
      "Epoch [23/50], Step [390/436], Loss: 2.2725\n",
      "Epoch [23/50], Step [400/436], Loss: 2.2984\n",
      "Epoch [23/50], Step [410/436], Loss: 2.3015\n",
      "Epoch [23/50], Step [420/436], Loss: 2.3021\n",
      "Epoch [23/50], Step [430/436], Loss: 2.3212\n",
      "Epoch [23/50], Time: 31.85s\n",
      "Train Loss: 2.3021, Train Acc: 0.1074\n",
      "Val Loss: 2.3035, Val Acc: 0.1042\n",
      "Epoch [24/50], Step [10/436], Loss: 2.2910\n",
      "Epoch [24/50], Step [20/436], Loss: 2.2956\n",
      "Epoch [24/50], Step [30/436], Loss: 2.3085\n",
      "Epoch [24/50], Step [40/436], Loss: 2.3023\n",
      "Epoch [24/50], Step [50/436], Loss: 2.2975\n",
      "Epoch [24/50], Step [60/436], Loss: 2.3057\n",
      "Epoch [24/50], Step [70/436], Loss: 2.2963\n",
      "Epoch [24/50], Step [80/436], Loss: 2.2919\n",
      "Epoch [24/50], Step [90/436], Loss: 2.3211\n",
      "Epoch [24/50], Step [100/436], Loss: 2.2935\n",
      "Epoch [24/50], Step [110/436], Loss: 2.3240\n",
      "Epoch [24/50], Step [120/436], Loss: 2.2870\n",
      "Epoch [24/50], Step [130/436], Loss: 2.2750\n",
      "Epoch [24/50], Step [140/436], Loss: 2.3012\n",
      "Epoch [24/50], Step [150/436], Loss: 2.3185\n",
      "Epoch [24/50], Step [160/436], Loss: 2.3113\n",
      "Epoch [24/50], Step [170/436], Loss: 2.3052\n",
      "Epoch [24/50], Step [180/436], Loss: 2.2995\n",
      "Epoch [24/50], Step [190/436], Loss: 2.2960\n",
      "Epoch [24/50], Step [200/436], Loss: 2.3063\n",
      "Epoch [24/50], Step [210/436], Loss: 2.2823\n",
      "Epoch [24/50], Step [220/436], Loss: 2.3026\n",
      "Epoch [24/50], Step [230/436], Loss: 2.3045\n",
      "Epoch [24/50], Step [240/436], Loss: 2.2966\n",
      "Epoch [24/50], Step [250/436], Loss: 2.2994\n",
      "Epoch [24/50], Step [260/436], Loss: 2.3129\n",
      "Epoch [24/50], Step [270/436], Loss: 2.3036\n",
      "Epoch [24/50], Step [280/436], Loss: 2.2979\n",
      "Epoch [24/50], Step [290/436], Loss: 2.3072\n",
      "Epoch [24/50], Step [300/436], Loss: 2.3116\n",
      "Epoch [24/50], Step [310/436], Loss: 2.2974\n",
      "Epoch [24/50], Step [320/436], Loss: 2.2890\n",
      "Epoch [24/50], Step [330/436], Loss: 2.2978\n",
      "Epoch [24/50], Step [340/436], Loss: 2.2807\n",
      "Epoch [24/50], Step [350/436], Loss: 2.2949\n",
      "Epoch [24/50], Step [360/436], Loss: 2.3243\n",
      "Epoch [24/50], Step [370/436], Loss: 2.2980\n",
      "Epoch [24/50], Step [380/436], Loss: 2.2958\n",
      "Epoch [24/50], Step [390/436], Loss: 2.3033\n",
      "Epoch [24/50], Step [400/436], Loss: 2.3086\n",
      "Epoch [24/50], Step [410/436], Loss: 2.2976\n",
      "Epoch [24/50], Step [420/436], Loss: 2.3003\n",
      "Epoch [24/50], Step [430/436], Loss: 2.3048\n",
      "Epoch [24/50], Time: 31.84s\n",
      "Train Loss: 2.3017, Train Acc: 0.1072\n",
      "Val Loss: 2.3045, Val Acc: 0.1042\n",
      "Epoch [25/50], Step [10/436], Loss: 2.3009\n",
      "Epoch [25/50], Step [20/436], Loss: 2.2946\n",
      "Epoch [25/50], Step [30/436], Loss: 2.2864\n",
      "Epoch [25/50], Step [40/436], Loss: 2.3016\n",
      "Epoch [25/50], Step [50/436], Loss: 2.3026\n",
      "Epoch [25/50], Step [60/436], Loss: 2.2929\n",
      "Epoch [25/50], Step [70/436], Loss: 2.2963\n",
      "Epoch [25/50], Step [80/436], Loss: 2.3209\n",
      "Epoch [25/50], Step [90/436], Loss: 2.3084\n",
      "Epoch [25/50], Step [100/436], Loss: 2.2920\n",
      "Epoch [25/50], Step [110/436], Loss: 2.2959\n",
      "Epoch [25/50], Step [120/436], Loss: 2.3027\n",
      "Epoch [25/50], Step [130/436], Loss: 2.2961\n",
      "Epoch [25/50], Step [140/436], Loss: 2.3092\n",
      "Epoch [25/50], Step [150/436], Loss: 2.3159\n",
      "Epoch [25/50], Step [160/436], Loss: 2.3008\n",
      "Epoch [25/50], Step [170/436], Loss: 2.2969\n",
      "Epoch [25/50], Step [180/436], Loss: 2.3088\n",
      "Epoch [25/50], Step [190/436], Loss: 2.2927\n",
      "Epoch [25/50], Step [200/436], Loss: 2.3090\n",
      "Epoch [25/50], Step [210/436], Loss: 2.2955\n",
      "Epoch [25/50], Step [220/436], Loss: 2.2965\n",
      "Epoch [25/50], Step [230/436], Loss: 2.2991\n",
      "Epoch [25/50], Step [240/436], Loss: 2.2902\n",
      "Epoch [25/50], Step [250/436], Loss: 2.3084\n",
      "Epoch [25/50], Step [260/436], Loss: 2.3134\n",
      "Epoch [25/50], Step [270/436], Loss: 2.3102\n",
      "Epoch [25/50], Step [280/436], Loss: 2.3094\n",
      "Epoch [25/50], Step [290/436], Loss: 2.3073\n",
      "Epoch [25/50], Step [300/436], Loss: 2.3057\n",
      "Epoch [25/50], Step [310/436], Loss: 2.2930\n",
      "Epoch [25/50], Step [320/436], Loss: 2.3062\n",
      "Epoch [25/50], Step [330/436], Loss: 2.3016\n",
      "Epoch [25/50], Step [340/436], Loss: 2.2987\n",
      "Epoch [25/50], Step [350/436], Loss: 2.3015\n",
      "Epoch [25/50], Step [360/436], Loss: 2.3012\n",
      "Epoch [25/50], Step [370/436], Loss: 2.3023\n",
      "Epoch [25/50], Step [380/436], Loss: 2.3035\n",
      "Epoch [25/50], Step [390/436], Loss: 2.3020\n",
      "Epoch [25/50], Step [400/436], Loss: 2.3097\n",
      "Epoch [25/50], Step [410/436], Loss: 2.3060\n",
      "Epoch [25/50], Step [420/436], Loss: 2.3053\n",
      "Epoch [25/50], Step [430/436], Loss: 2.2944\n",
      "Epoch [25/50], Time: 31.70s\n",
      "Train Loss: 2.3015, Train Acc: 0.1093\n",
      "Val Loss: 2.3053, Val Acc: 0.0981\n",
      "Epoch [26/50], Step [10/436], Loss: 2.3191\n",
      "Epoch [26/50], Step [20/436], Loss: 2.3036\n",
      "Epoch [26/50], Step [30/436], Loss: 2.2996\n",
      "Epoch [26/50], Step [40/436], Loss: 2.3072\n",
      "Epoch [26/50], Step [50/436], Loss: 2.3061\n",
      "Epoch [26/50], Step [60/436], Loss: 2.3090\n",
      "Epoch [26/50], Step [70/436], Loss: 2.3011\n",
      "Epoch [26/50], Step [80/436], Loss: 2.3000\n",
      "Epoch [26/50], Step [90/436], Loss: 2.3105\n",
      "Epoch [26/50], Step [100/436], Loss: 2.3107\n",
      "Epoch [26/50], Step [110/436], Loss: 2.3200\n",
      "Epoch [26/50], Step [120/436], Loss: 2.3126\n",
      "Epoch [26/50], Step [130/436], Loss: 2.3080\n",
      "Epoch [26/50], Step [140/436], Loss: 2.2957\n",
      "Epoch [26/50], Step [150/436], Loss: 2.3154\n",
      "Epoch [26/50], Step [160/436], Loss: 2.3043\n",
      "Epoch [26/50], Step [170/436], Loss: 2.2964\n",
      "Epoch [26/50], Step [180/436], Loss: 2.3044\n",
      "Epoch [26/50], Step [190/436], Loss: 2.3103\n",
      "Epoch [26/50], Step [200/436], Loss: 2.3028\n",
      "Epoch [26/50], Step [210/436], Loss: 2.3158\n",
      "Epoch [26/50], Step [220/436], Loss: 2.3029\n",
      "Epoch [26/50], Step [230/436], Loss: 2.3044\n",
      "Epoch [26/50], Step [240/436], Loss: 2.3048\n",
      "Epoch [26/50], Step [250/436], Loss: 2.2978\n",
      "Epoch [26/50], Step [260/436], Loss: 2.3005\n",
      "Epoch [26/50], Step [270/436], Loss: 2.3104\n",
      "Epoch [26/50], Step [280/436], Loss: 2.3066\n",
      "Epoch [26/50], Step [290/436], Loss: 2.2968\n",
      "Epoch [26/50], Step [300/436], Loss: 2.3142\n",
      "Epoch [26/50], Step [310/436], Loss: 2.3109\n",
      "Epoch [26/50], Step [320/436], Loss: 2.3038\n",
      "Epoch [26/50], Step [330/436], Loss: 2.3184\n",
      "Epoch [26/50], Step [340/436], Loss: 2.3153\n",
      "Epoch [26/50], Step [350/436], Loss: 2.3080\n",
      "Epoch [26/50], Step [360/436], Loss: 2.3057\n",
      "Epoch [26/50], Step [370/436], Loss: 2.3059\n",
      "Epoch [26/50], Step [380/436], Loss: 2.3033\n",
      "Epoch [26/50], Step [390/436], Loss: 2.3101\n",
      "Epoch [26/50], Step [400/436], Loss: 2.3010\n",
      "Epoch [26/50], Step [410/436], Loss: 2.3205\n",
      "Epoch [26/50], Step [420/436], Loss: 2.3275\n",
      "Epoch [26/50], Step [430/436], Loss: 2.3095\n",
      "Epoch [26/50], Time: 31.96s\n",
      "Train Loss: 2.3017, Train Acc: 0.1100\n",
      "Val Loss: 2.3044, Val Acc: 0.1029\n",
      "Epoch [27/50], Step [10/436], Loss: 2.3076\n",
      "Epoch [27/50], Step [20/436], Loss: 2.3214\n",
      "Epoch [27/50], Step [30/436], Loss: 2.3043\n",
      "Epoch [27/50], Step [40/436], Loss: 2.2946\n",
      "Epoch [27/50], Step [50/436], Loss: 2.2893\n",
      "Epoch [27/50], Step [60/436], Loss: 2.3072\n",
      "Epoch [27/50], Step [70/436], Loss: 2.2850\n",
      "Epoch [27/50], Step [80/436], Loss: 2.2743\n",
      "Epoch [27/50], Step [90/436], Loss: 2.2991\n",
      "Epoch [27/50], Step [100/436], Loss: 2.2892\n",
      "Epoch [27/50], Step [110/436], Loss: 2.3056\n",
      "Epoch [27/50], Step [120/436], Loss: 2.3311\n",
      "Epoch [27/50], Step [130/436], Loss: 2.2881\n",
      "Epoch [27/50], Step [140/436], Loss: 2.2985\n",
      "Epoch [27/50], Step [150/436], Loss: 2.3145\n",
      "Epoch [27/50], Step [160/436], Loss: 2.3062\n",
      "Epoch [27/50], Step [170/436], Loss: 2.2925\n",
      "Epoch [27/50], Step [180/436], Loss: 2.2887\n",
      "Epoch [27/50], Step [190/436], Loss: 2.3000\n",
      "Epoch [27/50], Step [200/436], Loss: 2.3150\n",
      "Epoch [27/50], Step [210/436], Loss: 2.3140\n",
      "Epoch [27/50], Step [220/436], Loss: 2.3050\n",
      "Epoch [27/50], Step [230/436], Loss: 2.2933\n",
      "Epoch [27/50], Step [240/436], Loss: 2.3011\n",
      "Epoch [27/50], Step [250/436], Loss: 2.2839\n",
      "Epoch [27/50], Step [260/436], Loss: 2.3137\n",
      "Epoch [27/50], Step [270/436], Loss: 2.3050\n",
      "Epoch [27/50], Step [280/436], Loss: 2.2956\n",
      "Epoch [27/50], Step [290/436], Loss: 2.2957\n",
      "Epoch [27/50], Step [300/436], Loss: 2.2962\n",
      "Epoch [27/50], Step [310/436], Loss: 2.3246\n",
      "Epoch [27/50], Step [320/436], Loss: 2.3009\n",
      "Epoch [27/50], Step [330/436], Loss: 2.2763\n",
      "Epoch [27/50], Step [340/436], Loss: 2.2838\n",
      "Epoch [27/50], Step [350/436], Loss: 2.2961\n",
      "Epoch [27/50], Step [360/436], Loss: 2.3029\n",
      "Epoch [27/50], Step [370/436], Loss: 2.3086\n",
      "Epoch [27/50], Step [380/436], Loss: 2.3146\n",
      "Epoch [27/50], Step [390/436], Loss: 2.2867\n",
      "Epoch [27/50], Step [400/436], Loss: 2.3034\n",
      "Epoch [27/50], Step [410/436], Loss: 2.2824\n",
      "Epoch [27/50], Step [420/436], Loss: 2.3091\n",
      "Epoch [27/50], Step [430/436], Loss: 2.3197\n",
      "Epoch [27/50], Time: 32.86s\n",
      "Train Loss: 2.3012, Train Acc: 0.1103\n",
      "Val Loss: 2.3045, Val Acc: 0.1013\n",
      "Epoch [28/50], Step [10/436], Loss: 2.3121\n",
      "Epoch [28/50], Step [20/436], Loss: 2.2966\n",
      "Epoch [28/50], Step [30/436], Loss: 2.2881\n",
      "Epoch [28/50], Step [40/436], Loss: 2.2926\n",
      "Epoch [28/50], Step [50/436], Loss: 2.3083\n",
      "Epoch [28/50], Step [60/436], Loss: 2.3048\n",
      "Epoch [28/50], Step [70/436], Loss: 2.2966\n",
      "Epoch [28/50], Step [80/436], Loss: 2.2969\n",
      "Epoch [28/50], Step [90/436], Loss: 2.3099\n",
      "Epoch [28/50], Step [100/436], Loss: 2.3043\n",
      "Epoch [28/50], Step [110/436], Loss: 2.3093\n",
      "Epoch [28/50], Step [120/436], Loss: 2.2807\n",
      "Epoch [28/50], Step [130/436], Loss: 2.3040\n",
      "Epoch [28/50], Step [140/436], Loss: 2.2907\n",
      "Epoch [28/50], Step [150/436], Loss: 2.2943\n",
      "Epoch [28/50], Step [160/436], Loss: 2.3088\n",
      "Epoch [28/50], Step [170/436], Loss: 2.2929\n",
      "Epoch [28/50], Step [180/436], Loss: 2.2936\n",
      "Epoch [28/50], Step [190/436], Loss: 2.2905\n",
      "Epoch [28/50], Step [200/436], Loss: 2.2922\n",
      "Epoch [28/50], Step [210/436], Loss: 2.3090\n",
      "Epoch [28/50], Step [220/436], Loss: 2.3017\n",
      "Epoch [28/50], Step [230/436], Loss: 2.2946\n",
      "Epoch [28/50], Step [240/436], Loss: 2.2926\n",
      "Epoch [28/50], Step [250/436], Loss: 2.3016\n",
      "Epoch [28/50], Step [260/436], Loss: 2.2851\n",
      "Epoch [28/50], Step [270/436], Loss: 2.2948\n",
      "Epoch [28/50], Step [280/436], Loss: 2.3085\n",
      "Epoch [28/50], Step [290/436], Loss: 2.2935\n",
      "Epoch [28/50], Step [300/436], Loss: 2.3050\n",
      "Epoch [28/50], Step [310/436], Loss: 2.2802\n",
      "Epoch [28/50], Step [320/436], Loss: 2.3015\n",
      "Epoch [28/50], Step [330/436], Loss: 2.3087\n",
      "Epoch [28/50], Step [340/436], Loss: 2.2985\n",
      "Epoch [28/50], Step [350/436], Loss: 2.2905\n",
      "Epoch [28/50], Step [360/436], Loss: 2.3000\n",
      "Epoch [28/50], Step [370/436], Loss: 2.2976\n",
      "Epoch [28/50], Step [380/436], Loss: 2.3091\n",
      "Epoch [28/50], Step [390/436], Loss: 2.2884\n",
      "Epoch [28/50], Step [400/436], Loss: 2.3082\n",
      "Epoch [28/50], Step [410/436], Loss: 2.3103\n",
      "Epoch [28/50], Step [420/436], Loss: 2.3175\n",
      "Epoch [28/50], Step [430/436], Loss: 2.3042\n",
      "Epoch [28/50], Time: 32.37s\n",
      "Train Loss: 2.3016, Train Acc: 0.1058\n",
      "Val Loss: 2.3043, Val Acc: 0.1094\n",
      "Epoch [29/50], Step [10/436], Loss: 2.2900\n",
      "Epoch [29/50], Step [20/436], Loss: 2.3157\n",
      "Epoch [29/50], Step [30/436], Loss: 2.2968\n",
      "Epoch [29/50], Step [40/436], Loss: 2.3117\n",
      "Epoch [29/50], Step [50/436], Loss: 2.2918\n",
      "Epoch [29/50], Step [60/436], Loss: 2.3217\n",
      "Epoch [29/50], Step [70/436], Loss: 2.3121\n",
      "Epoch [29/50], Step [80/436], Loss: 2.3182\n",
      "Epoch [29/50], Step [90/436], Loss: 2.3114\n",
      "Epoch [29/50], Step [100/436], Loss: 2.3068\n",
      "Epoch [29/50], Step [110/436], Loss: 2.2934\n",
      "Epoch [29/50], Step [120/436], Loss: 2.3184\n",
      "Epoch [29/50], Step [130/436], Loss: 2.2974\n",
      "Epoch [29/50], Step [140/436], Loss: 2.3217\n",
      "Epoch [29/50], Step [150/436], Loss: 2.2904\n",
      "Epoch [29/50], Step [160/436], Loss: 2.3034\n",
      "Epoch [29/50], Step [170/436], Loss: 2.3129\n",
      "Epoch [29/50], Step [180/436], Loss: 2.2987\n",
      "Epoch [29/50], Step [190/436], Loss: 2.3157\n",
      "Epoch [29/50], Step [200/436], Loss: 2.3133\n",
      "Epoch [29/50], Step [210/436], Loss: 2.3017\n",
      "Epoch [29/50], Step [220/436], Loss: 2.2905\n",
      "Epoch [29/50], Step [230/436], Loss: 2.3018\n",
      "Epoch [29/50], Step [240/436], Loss: 2.2996\n",
      "Epoch [29/50], Step [250/436], Loss: 2.2984\n",
      "Epoch [29/50], Step [260/436], Loss: 2.2830\n",
      "Epoch [29/50], Step [270/436], Loss: 2.2945\n",
      "Epoch [29/50], Step [280/436], Loss: 2.2948\n",
      "Epoch [29/50], Step [290/436], Loss: 2.2852\n",
      "Epoch [29/50], Step [300/436], Loss: 2.3008\n",
      "Epoch [29/50], Step [310/436], Loss: 2.2958\n",
      "Epoch [29/50], Step [320/436], Loss: 2.2975\n",
      "Epoch [29/50], Step [330/436], Loss: 2.2998\n",
      "Epoch [29/50], Step [340/436], Loss: 2.2838\n",
      "Epoch [29/50], Step [350/436], Loss: 2.3172\n",
      "Epoch [29/50], Step [360/436], Loss: 2.3054\n",
      "Epoch [29/50], Step [370/436], Loss: 2.3010\n",
      "Epoch [29/50], Step [380/436], Loss: 2.2789\n",
      "Epoch [29/50], Step [390/436], Loss: 2.2973\n",
      "Epoch [29/50], Step [400/436], Loss: 2.3087\n",
      "Epoch [29/50], Step [410/436], Loss: 2.3096\n",
      "Epoch [29/50], Step [420/436], Loss: 2.2939\n",
      "Epoch [29/50], Step [430/436], Loss: 2.3024\n",
      "Epoch [29/50], Time: 32.46s\n",
      "Train Loss: 2.3021, Train Acc: 0.1063\n",
      "Val Loss: 2.3044, Val Acc: 0.0997\n",
      "Epoch [30/50], Step [10/436], Loss: 2.3017\n",
      "Epoch [30/50], Step [20/436], Loss: 2.3039\n",
      "Epoch [30/50], Step [30/436], Loss: 2.3044\n",
      "Epoch [30/50], Step [40/436], Loss: 2.2843\n",
      "Epoch [30/50], Step [50/436], Loss: 2.3182\n",
      "Epoch [30/50], Step [60/436], Loss: 2.3110\n",
      "Epoch [30/50], Step [70/436], Loss: 2.2987\n",
      "Epoch [30/50], Step [80/436], Loss: 2.2981\n",
      "Epoch [30/50], Step [90/436], Loss: 2.2945\n",
      "Epoch [30/50], Step [100/436], Loss: 2.2902\n",
      "Epoch [30/50], Step [110/436], Loss: 2.2971\n",
      "Epoch [30/50], Step [120/436], Loss: 2.3004\n",
      "Epoch [30/50], Step [130/436], Loss: 2.3090\n",
      "Epoch [30/50], Step [140/436], Loss: 2.3075\n",
      "Epoch [30/50], Step [150/436], Loss: 2.3169\n",
      "Epoch [30/50], Step [160/436], Loss: 2.3089\n",
      "Epoch [30/50], Step [170/436], Loss: 2.3027\n",
      "Epoch [30/50], Step [180/436], Loss: 2.2923\n",
      "Epoch [30/50], Step [190/436], Loss: 2.2964\n",
      "Epoch [30/50], Step [200/436], Loss: 2.3068\n",
      "Epoch [30/50], Step [210/436], Loss: 2.2957\n",
      "Epoch [30/50], Step [220/436], Loss: 2.3149\n",
      "Epoch [30/50], Step [230/436], Loss: 2.3096\n",
      "Epoch [30/50], Step [240/436], Loss: 2.3066\n",
      "Epoch [30/50], Step [250/436], Loss: 2.2843\n",
      "Epoch [30/50], Step [260/436], Loss: 2.2997\n",
      "Epoch [30/50], Step [270/436], Loss: 2.3091\n",
      "Epoch [30/50], Step [280/436], Loss: 2.3024\n",
      "Epoch [30/50], Step [290/436], Loss: 2.3129\n",
      "Epoch [30/50], Step [300/436], Loss: 2.3126\n",
      "Epoch [30/50], Step [310/436], Loss: 2.3082\n",
      "Epoch [30/50], Step [320/436], Loss: 2.3079\n",
      "Epoch [30/50], Step [330/436], Loss: 2.3129\n",
      "Epoch [30/50], Step [340/436], Loss: 2.3081\n",
      "Epoch [30/50], Step [350/436], Loss: 2.2847\n",
      "Epoch [30/50], Step [360/436], Loss: 2.2917\n",
      "Epoch [30/50], Step [370/436], Loss: 2.3008\n",
      "Epoch [30/50], Step [380/436], Loss: 2.3085\n",
      "Epoch [30/50], Step [390/436], Loss: 2.3014\n",
      "Epoch [30/50], Step [400/436], Loss: 2.3025\n",
      "Epoch [30/50], Step [410/436], Loss: 2.2907\n",
      "Epoch [30/50], Step [420/436], Loss: 2.3051\n",
      "Epoch [30/50], Step [430/436], Loss: 2.2964\n",
      "Epoch [30/50], Time: 33.35s\n",
      "Train Loss: 2.3019, Train Acc: 0.1092\n",
      "Val Loss: 2.3030, Val Acc: 0.1045\n",
      "Epoch [31/50], Step [10/436], Loss: 2.2915\n",
      "Epoch [31/50], Step [20/436], Loss: 2.3029\n",
      "Epoch [31/50], Step [30/436], Loss: 2.2989\n",
      "Epoch [31/50], Step [40/436], Loss: 2.2995\n",
      "Epoch [31/50], Step [50/436], Loss: 2.3197\n",
      "Epoch [31/50], Step [60/436], Loss: 2.2919\n",
      "Epoch [31/50], Step [70/436], Loss: 2.3037\n",
      "Epoch [31/50], Step [80/436], Loss: 2.3025\n",
      "Epoch [31/50], Step [90/436], Loss: 2.3204\n",
      "Epoch [31/50], Step [100/436], Loss: 2.3025\n",
      "Epoch [31/50], Step [110/436], Loss: 2.2816\n",
      "Epoch [31/50], Step [120/436], Loss: 2.2812\n",
      "Epoch [31/50], Step [130/436], Loss: 2.3068\n",
      "Epoch [31/50], Step [140/436], Loss: 2.3114\n",
      "Epoch [31/50], Step [150/436], Loss: 2.3070\n",
      "Epoch [31/50], Step [160/436], Loss: 2.2808\n",
      "Epoch [31/50], Step [170/436], Loss: 2.3016\n",
      "Epoch [31/50], Step [180/436], Loss: 2.2947\n",
      "Epoch [31/50], Step [190/436], Loss: 2.2967\n",
      "Epoch [31/50], Step [200/436], Loss: 2.3068\n",
      "Epoch [31/50], Step [210/436], Loss: 2.3105\n",
      "Epoch [31/50], Step [220/436], Loss: 2.2960\n",
      "Epoch [31/50], Step [230/436], Loss: 2.2999\n",
      "Epoch [31/50], Step [240/436], Loss: 2.3008\n",
      "Epoch [31/50], Step [250/436], Loss: 2.3094\n",
      "Epoch [31/50], Step [260/436], Loss: 2.3006\n",
      "Epoch [31/50], Step [270/436], Loss: 2.2998\n",
      "Epoch [31/50], Step [280/436], Loss: 2.2998\n",
      "Epoch [31/50], Step [290/436], Loss: 2.3029\n",
      "Epoch [31/50], Step [300/436], Loss: 2.3033\n",
      "Epoch [31/50], Step [310/436], Loss: 2.2985\n",
      "Epoch [31/50], Step [320/436], Loss: 2.3048\n",
      "Epoch [31/50], Step [330/436], Loss: 2.3023\n",
      "Epoch [31/50], Step [340/436], Loss: 2.2867\n",
      "Epoch [31/50], Step [350/436], Loss: 2.3032\n",
      "Epoch [31/50], Step [360/436], Loss: 2.3217\n",
      "Epoch [31/50], Step [370/436], Loss: 2.2813\n",
      "Epoch [31/50], Step [380/436], Loss: 2.3018\n",
      "Epoch [31/50], Step [390/436], Loss: 2.3126\n",
      "Epoch [31/50], Step [400/436], Loss: 2.2900\n",
      "Epoch [31/50], Step [410/436], Loss: 2.3026\n",
      "Epoch [31/50], Step [420/436], Loss: 2.3063\n",
      "Epoch [31/50], Step [430/436], Loss: 2.3100\n",
      "Epoch [31/50], Time: 31.51s\n",
      "Train Loss: 2.3017, Train Acc: 0.1083\n",
      "Val Loss: 2.3037, Val Acc: 0.0981\n",
      "Epoch [32/50], Step [10/436], Loss: 2.2943\n",
      "Epoch [32/50], Step [20/436], Loss: 2.2935\n",
      "Epoch [32/50], Step [30/436], Loss: 2.3157\n",
      "Epoch [32/50], Step [40/436], Loss: 2.2990\n",
      "Epoch [32/50], Step [50/436], Loss: 2.3000\n",
      "Epoch [32/50], Step [60/436], Loss: 2.3009\n",
      "Epoch [32/50], Step [70/436], Loss: 2.3073\n",
      "Epoch [32/50], Step [80/436], Loss: 2.2884\n",
      "Epoch [32/50], Step [90/436], Loss: 2.3056\n",
      "Epoch [32/50], Step [100/436], Loss: 2.2925\n",
      "Epoch [32/50], Step [110/436], Loss: 2.3078\n",
      "Epoch [32/50], Step [120/436], Loss: 2.2948\n",
      "Epoch [32/50], Step [130/436], Loss: 2.3058\n",
      "Epoch [32/50], Step [140/436], Loss: 2.3207\n",
      "Epoch [32/50], Step [150/436], Loss: 2.2961\n",
      "Epoch [32/50], Step [160/436], Loss: 2.2922\n",
      "Epoch [32/50], Step [170/436], Loss: 2.3066\n",
      "Epoch [32/50], Step [180/436], Loss: 2.3086\n",
      "Epoch [32/50], Step [190/436], Loss: 2.3031\n",
      "Epoch [32/50], Step [200/436], Loss: 2.3149\n",
      "Epoch [32/50], Step [210/436], Loss: 2.3073\n",
      "Epoch [32/50], Step [220/436], Loss: 2.3186\n",
      "Epoch [32/50], Step [230/436], Loss: 2.3126\n",
      "Epoch [32/50], Step [240/436], Loss: 2.3046\n",
      "Epoch [32/50], Step [250/436], Loss: 2.3002\n",
      "Epoch [32/50], Step [260/436], Loss: 2.3151\n",
      "Epoch [32/50], Step [270/436], Loss: 2.2990\n",
      "Epoch [32/50], Step [280/436], Loss: 2.3095\n",
      "Epoch [32/50], Step [290/436], Loss: 2.2799\n",
      "Epoch [32/50], Step [300/436], Loss: 2.3056\n",
      "Epoch [32/50], Step [310/436], Loss: 2.3002\n",
      "Epoch [32/50], Step [320/436], Loss: 2.3105\n",
      "Epoch [32/50], Step [330/436], Loss: 2.2813\n",
      "Epoch [32/50], Step [340/436], Loss: 2.2978\n",
      "Epoch [32/50], Step [350/436], Loss: 2.2932\n",
      "Epoch [32/50], Step [360/436], Loss: 2.3268\n",
      "Epoch [32/50], Step [370/436], Loss: 2.3186\n",
      "Epoch [32/50], Step [380/436], Loss: 2.3069\n",
      "Epoch [32/50], Step [390/436], Loss: 2.2954\n",
      "Epoch [32/50], Step [400/436], Loss: 2.3108\n",
      "Epoch [32/50], Step [410/436], Loss: 2.2919\n",
      "Epoch [32/50], Step [420/436], Loss: 2.2882\n",
      "Epoch [32/50], Step [430/436], Loss: 2.2958\n",
      "Epoch [32/50], Time: 32.01s\n",
      "Train Loss: 2.3017, Train Acc: 0.1088\n",
      "Val Loss: 2.3046, Val Acc: 0.1032\n",
      "Epoch [33/50], Step [10/436], Loss: 2.2962\n",
      "Epoch [33/50], Step [20/436], Loss: 2.2822\n",
      "Epoch [33/50], Step [30/436], Loss: 2.2997\n",
      "Epoch [33/50], Step [40/436], Loss: 2.3068\n",
      "Epoch [33/50], Step [50/436], Loss: 2.2881\n",
      "Epoch [33/50], Step [60/436], Loss: 2.3082\n",
      "Epoch [33/50], Step [70/436], Loss: 2.3099\n",
      "Epoch [33/50], Step [80/436], Loss: 2.2968\n",
      "Epoch [33/50], Step [90/436], Loss: 2.3139\n",
      "Epoch [33/50], Step [100/436], Loss: 2.3075\n",
      "Epoch [33/50], Step [110/436], Loss: 2.3083\n",
      "Epoch [33/50], Step [120/436], Loss: 2.2966\n",
      "Epoch [33/50], Step [130/436], Loss: 2.3118\n",
      "Epoch [33/50], Step [140/436], Loss: 2.3058\n",
      "Epoch [33/50], Step [150/436], Loss: 2.3071\n",
      "Epoch [33/50], Step [160/436], Loss: 2.2792\n",
      "Epoch [33/50], Step [170/436], Loss: 2.3149\n",
      "Epoch [33/50], Step [180/436], Loss: 2.2994\n",
      "Epoch [33/50], Step [190/436], Loss: 2.3000\n",
      "Epoch [33/50], Step [200/436], Loss: 2.3083\n",
      "Epoch [33/50], Step [210/436], Loss: 2.2950\n",
      "Epoch [33/50], Step [220/436], Loss: 2.2961\n",
      "Epoch [33/50], Step [230/436], Loss: 2.2863\n",
      "Epoch [33/50], Step [240/436], Loss: 2.3085\n",
      "Epoch [33/50], Step [250/436], Loss: 2.2847\n",
      "Epoch [33/50], Step [260/436], Loss: 2.2956\n",
      "Epoch [33/50], Step [270/436], Loss: 2.3024\n",
      "Epoch [33/50], Step [280/436], Loss: 2.3147\n",
      "Epoch [33/50], Step [290/436], Loss: 2.3093\n",
      "Epoch [33/50], Step [300/436], Loss: 2.2959\n",
      "Epoch [33/50], Step [310/436], Loss: 2.2925\n",
      "Epoch [33/50], Step [320/436], Loss: 2.3046\n",
      "Epoch [33/50], Step [330/436], Loss: 2.2758\n",
      "Epoch [33/50], Step [340/436], Loss: 2.2903\n",
      "Epoch [33/50], Step [350/436], Loss: 2.3006\n",
      "Epoch [33/50], Step [360/436], Loss: 2.3046\n",
      "Epoch [33/50], Step [370/436], Loss: 2.3368\n",
      "Epoch [33/50], Step [380/436], Loss: 2.3044\n",
      "Epoch [33/50], Step [390/436], Loss: 2.2802\n",
      "Epoch [33/50], Step [400/436], Loss: 2.2984\n",
      "Epoch [33/50], Step [410/436], Loss: 2.3067\n",
      "Epoch [33/50], Step [420/436], Loss: 2.3049\n",
      "Epoch [33/50], Step [430/436], Loss: 2.3031\n",
      "Epoch [33/50], Time: 32.86s\n",
      "Train Loss: 2.3015, Train Acc: 0.1087\n",
      "Val Loss: 2.3039, Val Acc: 0.1058\n",
      "Epoch [34/50], Step [10/436], Loss: 2.3007\n",
      "Epoch [34/50], Step [20/436], Loss: 2.2964\n",
      "Epoch [34/50], Step [30/436], Loss: 2.3063\n",
      "Epoch [34/50], Step [40/436], Loss: 2.3033\n",
      "Epoch [34/50], Step [50/436], Loss: 2.2935\n",
      "Epoch [34/50], Step [60/436], Loss: 2.2975\n",
      "Epoch [34/50], Step [70/436], Loss: 2.2939\n",
      "Epoch [34/50], Step [80/436], Loss: 2.3134\n",
      "Epoch [34/50], Step [90/436], Loss: 2.3153\n",
      "Epoch [34/50], Step [100/436], Loss: 2.3025\n",
      "Epoch [34/50], Step [110/436], Loss: 2.3127\n",
      "Epoch [34/50], Step [120/436], Loss: 2.2926\n",
      "Epoch [34/50], Step [130/436], Loss: 2.3224\n",
      "Epoch [34/50], Step [140/436], Loss: 2.2986\n",
      "Epoch [34/50], Step [150/436], Loss: 2.2976\n",
      "Epoch [34/50], Step [160/436], Loss: 2.3091\n",
      "Epoch [34/50], Step [170/436], Loss: 2.2989\n",
      "Epoch [34/50], Step [180/436], Loss: 2.2940\n",
      "Epoch [34/50], Step [190/436], Loss: 2.3110\n",
      "Epoch [34/50], Step [200/436], Loss: 2.3023\n",
      "Epoch [34/50], Step [210/436], Loss: 2.3085\n",
      "Epoch [34/50], Step [220/436], Loss: 2.2883\n",
      "Epoch [34/50], Step [230/436], Loss: 2.2983\n",
      "Epoch [34/50], Step [240/436], Loss: 2.2989\n",
      "Epoch [34/50], Step [250/436], Loss: 2.3035\n",
      "Epoch [34/50], Step [260/436], Loss: 2.2933\n",
      "Epoch [34/50], Step [270/436], Loss: 2.3053\n",
      "Epoch [34/50], Step [280/436], Loss: 2.2930\n",
      "Epoch [34/50], Step [290/436], Loss: 2.3230\n",
      "Epoch [34/50], Step [300/436], Loss: 2.3062\n",
      "Epoch [34/50], Step [310/436], Loss: 2.3061\n",
      "Epoch [34/50], Step [320/436], Loss: 2.3015\n",
      "Epoch [34/50], Step [330/436], Loss: 2.2990\n",
      "Epoch [34/50], Step [340/436], Loss: 2.2906\n",
      "Epoch [34/50], Step [350/436], Loss: 2.3068\n",
      "Epoch [34/50], Step [360/436], Loss: 2.3028\n",
      "Epoch [34/50], Step [370/436], Loss: 2.2984\n",
      "Epoch [34/50], Step [380/436], Loss: 2.3032\n",
      "Epoch [34/50], Step [390/436], Loss: 2.3157\n",
      "Epoch [34/50], Step [400/436], Loss: 2.2816\n",
      "Epoch [34/50], Step [410/436], Loss: 2.2864\n",
      "Epoch [34/50], Step [420/436], Loss: 2.3006\n",
      "Epoch [34/50], Step [430/436], Loss: 2.3249\n",
      "Epoch [34/50], Time: 35.29s\n",
      "Train Loss: 2.3018, Train Acc: 0.1090\n",
      "Val Loss: 2.3038, Val Acc: 0.1042\n",
      "Epoch [35/50], Step [10/436], Loss: 2.2943\n",
      "Epoch [35/50], Step [20/436], Loss: 2.2930\n",
      "Epoch [35/50], Step [30/436], Loss: 2.2974\n",
      "Epoch [35/50], Step [40/436], Loss: 2.3210\n",
      "Epoch [35/50], Step [50/436], Loss: 2.3001\n",
      "Epoch [35/50], Step [60/436], Loss: 2.3012\n",
      "Epoch [35/50], Step [70/436], Loss: 2.3046\n",
      "Epoch [35/50], Step [80/436], Loss: 2.2955\n",
      "Epoch [35/50], Step [90/436], Loss: 2.3050\n",
      "Epoch [35/50], Step [100/436], Loss: 2.3147\n",
      "Epoch [35/50], Step [110/436], Loss: 2.3213\n",
      "Epoch [35/50], Step [120/436], Loss: 2.3086\n",
      "Epoch [35/50], Step [130/436], Loss: 2.2842\n",
      "Epoch [35/50], Step [140/436], Loss: 2.2918\n",
      "Epoch [35/50], Step [150/436], Loss: 2.2928\n",
      "Epoch [35/50], Step [160/436], Loss: 2.3193\n",
      "Epoch [35/50], Step [170/436], Loss: 2.3110\n",
      "Epoch [35/50], Step [180/436], Loss: 2.2944\n",
      "Epoch [35/50], Step [190/436], Loss: 2.2876\n",
      "Epoch [35/50], Step [200/436], Loss: 2.3054\n",
      "Epoch [35/50], Step [210/436], Loss: 2.3042\n",
      "Epoch [35/50], Step [220/436], Loss: 2.3035\n",
      "Epoch [35/50], Step [230/436], Loss: 2.3049\n",
      "Epoch [35/50], Step [240/436], Loss: 2.2784\n",
      "Epoch [35/50], Step [250/436], Loss: 2.3265\n",
      "Epoch [35/50], Step [260/436], Loss: 2.3101\n",
      "Epoch [35/50], Step [270/436], Loss: 2.3097\n",
      "Epoch [35/50], Step [280/436], Loss: 2.3138\n",
      "Epoch [35/50], Step [290/436], Loss: 2.3050\n",
      "Epoch [35/50], Step [300/436], Loss: 2.3071\n",
      "Epoch [35/50], Step [310/436], Loss: 2.2907\n",
      "Epoch [35/50], Step [320/436], Loss: 2.3073\n",
      "Epoch [35/50], Step [330/436], Loss: 2.3000\n",
      "Epoch [35/50], Step [340/436], Loss: 2.3099\n",
      "Epoch [35/50], Step [350/436], Loss: 2.3085\n",
      "Epoch [35/50], Step [360/436], Loss: 2.3071\n",
      "Epoch [35/50], Step [370/436], Loss: 2.3082\n",
      "Epoch [35/50], Step [380/436], Loss: 2.3082\n",
      "Epoch [35/50], Step [390/436], Loss: 2.3051\n",
      "Epoch [35/50], Step [400/436], Loss: 2.3093\n",
      "Epoch [35/50], Step [410/436], Loss: 2.2970\n",
      "Epoch [35/50], Step [420/436], Loss: 2.3021\n",
      "Epoch [35/50], Step [430/436], Loss: 2.3045\n",
      "Epoch [35/50], Time: 32.18s\n",
      "Train Loss: 2.3021, Train Acc: 0.1089\n",
      "Val Loss: 2.3041, Val Acc: 0.1035\n",
      "Epoch [36/50], Step [10/436], Loss: 2.2943\n",
      "Epoch [36/50], Step [20/436], Loss: 2.2968\n",
      "Epoch [36/50], Step [30/436], Loss: 2.3127\n",
      "Epoch [36/50], Step [40/436], Loss: 2.3037\n",
      "Epoch [36/50], Step [50/436], Loss: 2.3002\n",
      "Epoch [36/50], Step [60/436], Loss: 2.3093\n",
      "Epoch [36/50], Step [70/436], Loss: 2.2941\n",
      "Epoch [36/50], Step [80/436], Loss: 2.2932\n",
      "Epoch [36/50], Step [90/436], Loss: 2.3028\n",
      "Epoch [36/50], Step [100/436], Loss: 2.3003\n",
      "Epoch [36/50], Step [110/436], Loss: 2.2890\n",
      "Epoch [36/50], Step [120/436], Loss: 2.3087\n",
      "Epoch [36/50], Step [130/436], Loss: 2.2833\n",
      "Epoch [36/50], Step [140/436], Loss: 2.2986\n",
      "Epoch [36/50], Step [150/436], Loss: 2.3117\n",
      "Epoch [36/50], Step [160/436], Loss: 2.2931\n",
      "Epoch [36/50], Step [170/436], Loss: 2.2814\n",
      "Epoch [36/50], Step [180/436], Loss: 2.2918\n",
      "Epoch [36/50], Step [190/436], Loss: 2.3027\n",
      "Epoch [36/50], Step [200/436], Loss: 2.2996\n",
      "Epoch [36/50], Step [210/436], Loss: 2.2870\n",
      "Epoch [36/50], Step [220/436], Loss: 2.2964\n",
      "Epoch [36/50], Step [230/436], Loss: 2.2982\n",
      "Epoch [36/50], Step [240/436], Loss: 2.2997\n",
      "Epoch [36/50], Step [250/436], Loss: 2.3130\n",
      "Epoch [36/50], Step [260/436], Loss: 2.3225\n",
      "Epoch [36/50], Step [270/436], Loss: 2.3076\n",
      "Epoch [36/50], Step [280/436], Loss: 2.3047\n",
      "Epoch [36/50], Step [290/436], Loss: 2.3004\n",
      "Epoch [36/50], Step [300/436], Loss: 2.2970\n",
      "Epoch [36/50], Step [310/436], Loss: 2.3103\n",
      "Epoch [36/50], Step [320/436], Loss: 2.3153\n",
      "Epoch [36/50], Step [330/436], Loss: 2.3049\n",
      "Epoch [36/50], Step [340/436], Loss: 2.2974\n",
      "Epoch [36/50], Step [350/436], Loss: 2.2970\n",
      "Epoch [36/50], Step [360/436], Loss: 2.3132\n",
      "Epoch [36/50], Step [370/436], Loss: 2.3038\n",
      "Epoch [36/50], Step [380/436], Loss: 2.2964\n",
      "Epoch [36/50], Step [390/436], Loss: 2.2882\n",
      "Epoch [36/50], Step [400/436], Loss: 2.3034\n",
      "Epoch [36/50], Step [410/436], Loss: 2.2910\n",
      "Epoch [36/50], Step [420/436], Loss: 2.2826\n",
      "Epoch [36/50], Step [430/436], Loss: 2.3107\n",
      "Epoch [36/50], Time: 32.65s\n",
      "Train Loss: 2.3013, Train Acc: 0.1091\n",
      "Val Loss: 2.3044, Val Acc: 0.0994\n",
      "Epoch [37/50], Step [10/436], Loss: 2.3028\n",
      "Epoch [37/50], Step [20/436], Loss: 2.3000\n",
      "Epoch [37/50], Step [30/436], Loss: 2.3089\n",
      "Epoch [37/50], Step [40/436], Loss: 2.3006\n",
      "Epoch [37/50], Step [50/436], Loss: 2.2874\n",
      "Epoch [37/50], Step [60/436], Loss: 2.2958\n",
      "Epoch [37/50], Step [70/436], Loss: 2.2945\n",
      "Epoch [37/50], Step [80/436], Loss: 2.2993\n",
      "Epoch [37/50], Step [90/436], Loss: 2.3068\n",
      "Epoch [37/50], Step [100/436], Loss: 2.3049\n",
      "Epoch [37/50], Step [110/436], Loss: 2.2872\n",
      "Epoch [37/50], Step [120/436], Loss: 2.2987\n",
      "Epoch [37/50], Step [130/436], Loss: 2.3044\n",
      "Epoch [37/50], Step [140/436], Loss: 2.3126\n",
      "Epoch [37/50], Step [150/436], Loss: 2.3008\n",
      "Epoch [37/50], Step [160/436], Loss: 2.3001\n",
      "Epoch [37/50], Step [170/436], Loss: 2.2905\n",
      "Epoch [37/50], Step [180/436], Loss: 2.3072\n",
      "Epoch [37/50], Step [190/436], Loss: 2.3014\n",
      "Epoch [37/50], Step [200/436], Loss: 2.3009\n",
      "Epoch [37/50], Step [210/436], Loss: 2.3067\n",
      "Epoch [37/50], Step [220/436], Loss: 2.3064\n",
      "Epoch [37/50], Step [230/436], Loss: 2.2902\n",
      "Epoch [37/50], Step [240/436], Loss: 2.3066\n",
      "Epoch [37/50], Step [250/436], Loss: 2.3094\n",
      "Epoch [37/50], Step [260/436], Loss: 2.3068\n",
      "Epoch [37/50], Step [270/436], Loss: 2.3056\n",
      "Epoch [37/50], Step [280/436], Loss: 2.3036\n",
      "Epoch [37/50], Step [290/436], Loss: 2.3155\n",
      "Epoch [37/50], Step [300/436], Loss: 2.3018\n",
      "Epoch [37/50], Step [310/436], Loss: 2.2866\n",
      "Epoch [37/50], Step [320/436], Loss: 2.2915\n",
      "Epoch [37/50], Step [330/436], Loss: 2.2938\n",
      "Epoch [37/50], Step [340/436], Loss: 2.3073\n",
      "Epoch [37/50], Step [350/436], Loss: 2.2969\n",
      "Epoch [37/50], Step [360/436], Loss: 2.3002\n",
      "Epoch [37/50], Step [370/436], Loss: 2.3149\n",
      "Epoch [37/50], Step [380/436], Loss: 2.3117\n",
      "Epoch [37/50], Step [390/436], Loss: 2.3051\n",
      "Epoch [37/50], Step [400/436], Loss: 2.2937\n",
      "Epoch [37/50], Step [410/436], Loss: 2.2923\n",
      "Epoch [37/50], Step [420/436], Loss: 2.3005\n",
      "Epoch [37/50], Step [430/436], Loss: 2.3156\n",
      "Epoch [37/50], Time: 31.70s\n",
      "Train Loss: 2.3018, Train Acc: 0.1071\n",
      "Val Loss: 2.3050, Val Acc: 0.1048\n",
      "Epoch [38/50], Step [10/436], Loss: 2.2849\n",
      "Epoch [38/50], Step [20/436], Loss: 2.3101\n",
      "Epoch [38/50], Step [30/436], Loss: 2.3101\n",
      "Epoch [38/50], Step [40/436], Loss: 2.2988\n",
      "Epoch [38/50], Step [50/436], Loss: 2.3016\n",
      "Epoch [38/50], Step [60/436], Loss: 2.3154\n",
      "Epoch [38/50], Step [70/436], Loss: 2.2951\n",
      "Epoch [38/50], Step [80/436], Loss: 2.3127\n",
      "Epoch [38/50], Step [90/436], Loss: 2.3069\n",
      "Epoch [38/50], Step [100/436], Loss: 2.3030\n",
      "Epoch [38/50], Step [110/436], Loss: 2.2934\n",
      "Epoch [38/50], Step [120/436], Loss: 2.3171\n",
      "Epoch [38/50], Step [130/436], Loss: 2.2998\n",
      "Epoch [38/50], Step [140/436], Loss: 2.2999\n",
      "Epoch [38/50], Step [150/436], Loss: 2.3055\n",
      "Epoch [38/50], Step [160/436], Loss: 2.3163\n",
      "Epoch [38/50], Step [170/436], Loss: 2.3012\n",
      "Epoch [38/50], Step [180/436], Loss: 2.2947\n",
      "Epoch [38/50], Step [190/436], Loss: 2.2959\n",
      "Epoch [38/50], Step [200/436], Loss: 2.3047\n",
      "Epoch [38/50], Step [210/436], Loss: 2.3037\n",
      "Epoch [38/50], Step [220/436], Loss: 2.3097\n",
      "Epoch [38/50], Step [230/436], Loss: 2.2872\n",
      "Epoch [38/50], Step [240/436], Loss: 2.2858\n",
      "Epoch [38/50], Step [250/436], Loss: 2.3067\n",
      "Epoch [38/50], Step [260/436], Loss: 2.3101\n",
      "Epoch [38/50], Step [270/436], Loss: 2.2898\n",
      "Epoch [38/50], Step [280/436], Loss: 2.2947\n",
      "Epoch [38/50], Step [290/436], Loss: 2.3071\n",
      "Epoch [38/50], Step [300/436], Loss: 2.2950\n",
      "Epoch [38/50], Step [310/436], Loss: 2.2982\n",
      "Epoch [38/50], Step [320/436], Loss: 2.3188\n",
      "Epoch [38/50], Step [330/436], Loss: 2.3097\n",
      "Epoch [38/50], Step [340/436], Loss: 2.3031\n",
      "Epoch [38/50], Step [350/436], Loss: 2.2994\n",
      "Epoch [38/50], Step [360/436], Loss: 2.2962\n",
      "Epoch [38/50], Step [370/436], Loss: 2.2989\n",
      "Epoch [38/50], Step [380/436], Loss: 2.2945\n",
      "Epoch [38/50], Step [390/436], Loss: 2.3083\n",
      "Epoch [38/50], Step [400/436], Loss: 2.2948\n",
      "Epoch [38/50], Step [410/436], Loss: 2.2808\n",
      "Epoch [38/50], Step [420/436], Loss: 2.3272\n",
      "Epoch [38/50], Step [430/436], Loss: 2.2980\n",
      "Epoch [38/50], Time: 31.77s\n",
      "Train Loss: 2.3016, Train Acc: 0.1103\n",
      "Val Loss: 2.3052, Val Acc: 0.1048\n",
      "Epoch [39/50], Step [10/436], Loss: 2.2981\n",
      "Epoch [39/50], Step [20/436], Loss: 2.3091\n",
      "Epoch [39/50], Step [30/436], Loss: 2.3115\n",
      "Epoch [39/50], Step [40/436], Loss: 2.3167\n",
      "Epoch [39/50], Step [50/436], Loss: 2.2900\n",
      "Epoch [39/50], Step [60/436], Loss: 2.3057\n",
      "Epoch [39/50], Step [70/436], Loss: 2.3130\n",
      "Epoch [39/50], Step [80/436], Loss: 2.3023\n",
      "Epoch [39/50], Step [90/436], Loss: 2.2924\n",
      "Epoch [39/50], Step [100/436], Loss: 2.3061\n",
      "Epoch [39/50], Step [110/436], Loss: 2.2940\n",
      "Epoch [39/50], Step [120/436], Loss: 2.2960\n",
      "Epoch [39/50], Step [130/436], Loss: 2.2867\n",
      "Epoch [39/50], Step [140/436], Loss: 2.3180\n",
      "Epoch [39/50], Step [150/436], Loss: 2.2978\n",
      "Epoch [39/50], Step [160/436], Loss: 2.2923\n",
      "Epoch [39/50], Step [170/436], Loss: 2.2969\n",
      "Epoch [39/50], Step [180/436], Loss: 2.3024\n",
      "Epoch [39/50], Step [190/436], Loss: 2.3018\n",
      "Epoch [39/50], Step [200/436], Loss: 2.2794\n",
      "Epoch [39/50], Step [210/436], Loss: 2.2998\n",
      "Epoch [39/50], Step [220/436], Loss: 2.2975\n",
      "Epoch [39/50], Step [230/436], Loss: 2.2986\n",
      "Epoch [39/50], Step [240/436], Loss: 2.3099\n",
      "Epoch [39/50], Step [250/436], Loss: 2.2968\n",
      "Epoch [39/50], Step [260/436], Loss: 2.3205\n",
      "Epoch [39/50], Step [270/436], Loss: 2.3000\n",
      "Epoch [39/50], Step [280/436], Loss: 2.3126\n",
      "Epoch [39/50], Step [290/436], Loss: 2.3003\n",
      "Epoch [39/50], Step [300/436], Loss: 2.2919\n",
      "Epoch [39/50], Step [310/436], Loss: 2.2882\n",
      "Epoch [39/50], Step [320/436], Loss: 2.3023\n",
      "Epoch [39/50], Step [330/436], Loss: 2.3024\n",
      "Epoch [39/50], Step [340/436], Loss: 2.3002\n",
      "Epoch [39/50], Step [350/436], Loss: 2.2922\n",
      "Epoch [39/50], Step [360/436], Loss: 2.3152\n",
      "Epoch [39/50], Step [370/436], Loss: 2.2918\n",
      "Epoch [39/50], Step [380/436], Loss: 2.2925\n",
      "Epoch [39/50], Step [390/436], Loss: 2.3246\n",
      "Epoch [39/50], Step [400/436], Loss: 2.3124\n",
      "Epoch [39/50], Step [410/436], Loss: 2.2826\n",
      "Epoch [39/50], Step [420/436], Loss: 2.3169\n",
      "Epoch [39/50], Step [430/436], Loss: 2.3040\n",
      "Epoch [39/50], Time: 33.99s\n",
      "Train Loss: 2.3019, Train Acc: 0.1082\n",
      "Val Loss: 2.3038, Val Acc: 0.1074\n",
      "Epoch [40/50], Step [10/436], Loss: 2.3299\n",
      "Epoch [40/50], Step [20/436], Loss: 2.3179\n",
      "Epoch [40/50], Step [30/436], Loss: 2.2946\n",
      "Epoch [40/50], Step [40/436], Loss: 2.2948\n",
      "Epoch [40/50], Step [50/436], Loss: 2.3073\n",
      "Epoch [40/50], Step [60/436], Loss: 2.2883\n",
      "Epoch [40/50], Step [70/436], Loss: 2.3113\n",
      "Epoch [40/50], Step [80/436], Loss: 2.2871\n",
      "Epoch [40/50], Step [90/436], Loss: 2.3109\n",
      "Epoch [40/50], Step [100/436], Loss: 2.2946\n",
      "Epoch [40/50], Step [110/436], Loss: 2.3335\n",
      "Epoch [40/50], Step [120/436], Loss: 2.2963\n",
      "Epoch [40/50], Step [130/436], Loss: 2.2857\n",
      "Epoch [40/50], Step [140/436], Loss: 2.3044\n",
      "Epoch [40/50], Step [150/436], Loss: 2.3035\n",
      "Epoch [40/50], Step [160/436], Loss: 2.3051\n",
      "Epoch [40/50], Step [170/436], Loss: 2.3023\n",
      "Epoch [40/50], Step [180/436], Loss: 2.2994\n",
      "Epoch [40/50], Step [190/436], Loss: 2.3082\n",
      "Epoch [40/50], Step [200/436], Loss: 2.2935\n",
      "Epoch [40/50], Step [210/436], Loss: 2.3060\n",
      "Epoch [40/50], Step [220/436], Loss: 2.3017\n",
      "Epoch [40/50], Step [230/436], Loss: 2.3045\n",
      "Epoch [40/50], Step [240/436], Loss: 2.3100\n",
      "Epoch [40/50], Step [250/436], Loss: 2.2954\n",
      "Epoch [40/50], Step [260/436], Loss: 2.3085\n",
      "Epoch [40/50], Step [270/436], Loss: 2.2957\n",
      "Epoch [40/50], Step [280/436], Loss: 2.3090\n",
      "Epoch [40/50], Step [290/436], Loss: 2.2788\n",
      "Epoch [40/50], Step [300/436], Loss: 2.2907\n",
      "Epoch [40/50], Step [310/436], Loss: 2.3155\n",
      "Epoch [40/50], Step [320/436], Loss: 2.3092\n",
      "Epoch [40/50], Step [330/436], Loss: 2.3039\n",
      "Epoch [40/50], Step [340/436], Loss: 2.2826\n",
      "Epoch [40/50], Step [350/436], Loss: 2.3001\n",
      "Epoch [40/50], Step [360/436], Loss: 2.2940\n",
      "Epoch [40/50], Step [370/436], Loss: 2.3034\n",
      "Epoch [40/50], Step [380/436], Loss: 2.3057\n",
      "Epoch [40/50], Step [390/436], Loss: 2.2975\n",
      "Epoch [40/50], Step [400/436], Loss: 2.2841\n",
      "Epoch [40/50], Step [410/436], Loss: 2.2894\n",
      "Epoch [40/50], Step [420/436], Loss: 2.3058\n",
      "Epoch [40/50], Step [430/436], Loss: 2.3052\n",
      "Epoch [40/50], Time: 32.47s\n",
      "Train Loss: 2.3017, Train Acc: 0.1082\n",
      "Val Loss: 2.3049, Val Acc: 0.1019\n",
      "Epoch [41/50], Step [10/436], Loss: 2.2948\n",
      "Epoch [41/50], Step [20/436], Loss: 2.2982\n",
      "Epoch [41/50], Step [30/436], Loss: 2.3053\n",
      "Epoch [41/50], Step [40/436], Loss: 2.2981\n",
      "Epoch [41/50], Step [50/436], Loss: 2.3109\n",
      "Epoch [41/50], Step [60/436], Loss: 2.3038\n",
      "Epoch [41/50], Step [70/436], Loss: 2.2894\n",
      "Epoch [41/50], Step [80/436], Loss: 2.3263\n",
      "Epoch [41/50], Step [90/436], Loss: 2.2940\n",
      "Epoch [41/50], Step [100/436], Loss: 2.3062\n",
      "Epoch [41/50], Step [110/436], Loss: 2.2817\n",
      "Epoch [41/50], Step [120/436], Loss: 2.3097\n",
      "Epoch [41/50], Step [130/436], Loss: 2.2983\n",
      "Epoch [41/50], Step [140/436], Loss: 2.3150\n",
      "Epoch [41/50], Step [150/436], Loss: 2.3036\n",
      "Epoch [41/50], Step [160/436], Loss: 2.3010\n",
      "Epoch [41/50], Step [170/436], Loss: 2.2898\n",
      "Epoch [41/50], Step [180/436], Loss: 2.3088\n",
      "Epoch [41/50], Step [190/436], Loss: 2.2988\n",
      "Epoch [41/50], Step [200/436], Loss: 2.3011\n",
      "Epoch [41/50], Step [210/436], Loss: 2.3068\n",
      "Epoch [41/50], Step [220/436], Loss: 2.3081\n",
      "Epoch [41/50], Step [230/436], Loss: 2.2974\n",
      "Epoch [41/50], Step [240/436], Loss: 2.2958\n",
      "Epoch [41/50], Step [250/436], Loss: 2.2668\n",
      "Epoch [41/50], Step [260/436], Loss: 2.2990\n",
      "Epoch [41/50], Step [270/436], Loss: 2.2847\n",
      "Epoch [41/50], Step [280/436], Loss: 2.3027\n",
      "Epoch [41/50], Step [290/436], Loss: 2.2824\n",
      "Epoch [41/50], Step [300/436], Loss: 2.2893\n",
      "Epoch [41/50], Step [310/436], Loss: 2.3024\n",
      "Epoch [41/50], Step [320/436], Loss: 2.2978\n",
      "Epoch [41/50], Step [330/436], Loss: 2.3183\n",
      "Epoch [41/50], Step [340/436], Loss: 2.2972\n",
      "Epoch [41/50], Step [350/436], Loss: 2.3075\n",
      "Epoch [41/50], Step [360/436], Loss: 2.2824\n",
      "Epoch [41/50], Step [370/436], Loss: 2.3112\n",
      "Epoch [41/50], Step [380/436], Loss: 2.2922\n",
      "Epoch [41/50], Step [390/436], Loss: 2.2955\n",
      "Epoch [41/50], Step [400/436], Loss: 2.3000\n",
      "Epoch [41/50], Step [410/436], Loss: 2.2969\n",
      "Epoch [41/50], Step [420/436], Loss: 2.2977\n",
      "Epoch [41/50], Step [430/436], Loss: 2.3086\n",
      "Epoch [41/50], Time: 32.37s\n",
      "Train Loss: 2.3016, Train Acc: 0.1115\n",
      "Val Loss: 2.3036, Val Acc: 0.1045\n",
      "Epoch [42/50], Step [10/436], Loss: 2.3049\n",
      "Epoch [42/50], Step [20/436], Loss: 2.3156\n",
      "Epoch [42/50], Step [30/436], Loss: 2.2953\n",
      "Epoch [42/50], Step [40/436], Loss: 2.3065\n",
      "Epoch [42/50], Step [50/436], Loss: 2.3132\n",
      "Epoch [42/50], Step [60/436], Loss: 2.2997\n",
      "Epoch [42/50], Step [70/436], Loss: 2.3194\n",
      "Epoch [42/50], Step [80/436], Loss: 2.3025\n",
      "Epoch [42/50], Step [90/436], Loss: 2.3048\n",
      "Epoch [42/50], Step [100/436], Loss: 2.2923\n",
      "Epoch [42/50], Step [110/436], Loss: 2.2853\n",
      "Epoch [42/50], Step [120/436], Loss: 2.3083\n",
      "Epoch [42/50], Step [130/436], Loss: 2.3068\n",
      "Epoch [42/50], Step [140/436], Loss: 2.3172\n",
      "Epoch [42/50], Step [150/436], Loss: 2.2996\n",
      "Epoch [42/50], Step [160/436], Loss: 2.2986\n",
      "Epoch [42/50], Step [170/436], Loss: 2.3057\n",
      "Epoch [42/50], Step [180/436], Loss: 2.2865\n",
      "Epoch [42/50], Step [190/436], Loss: 2.2951\n",
      "Epoch [42/50], Step [200/436], Loss: 2.2990\n",
      "Epoch [42/50], Step [210/436], Loss: 2.3123\n",
      "Epoch [42/50], Step [220/436], Loss: 2.3135\n",
      "Epoch [42/50], Step [230/436], Loss: 2.3013\n",
      "Epoch [42/50], Step [240/436], Loss: 2.2940\n",
      "Epoch [42/50], Step [250/436], Loss: 2.3135\n",
      "Epoch [42/50], Step [260/436], Loss: 2.2808\n",
      "Epoch [42/50], Step [270/436], Loss: 2.2850\n",
      "Epoch [42/50], Step [280/436], Loss: 2.2964\n",
      "Epoch [42/50], Step [290/436], Loss: 2.2853\n",
      "Epoch [42/50], Step [300/436], Loss: 2.3072\n",
      "Epoch [42/50], Step [310/436], Loss: 2.3035\n",
      "Epoch [42/50], Step [320/436], Loss: 2.3133\n",
      "Epoch [42/50], Step [330/436], Loss: 2.2866\n",
      "Epoch [42/50], Step [340/436], Loss: 2.3109\n",
      "Epoch [42/50], Step [350/436], Loss: 2.3107\n",
      "Epoch [42/50], Step [360/436], Loss: 2.2930\n",
      "Epoch [42/50], Step [370/436], Loss: 2.3086\n",
      "Epoch [42/50], Step [380/436], Loss: 2.3033\n",
      "Epoch [42/50], Step [390/436], Loss: 2.2926\n",
      "Epoch [42/50], Step [400/436], Loss: 2.2829\n",
      "Epoch [42/50], Step [410/436], Loss: 2.3103\n",
      "Epoch [42/50], Step [420/436], Loss: 2.3014\n",
      "Epoch [42/50], Step [430/436], Loss: 2.3004\n",
      "Epoch [42/50], Time: 31.59s\n",
      "Train Loss: 2.3022, Train Acc: 0.1105\n",
      "Val Loss: 2.3036, Val Acc: 0.1055\n",
      "Epoch [43/50], Step [10/436], Loss: 2.3144\n",
      "Epoch [43/50], Step [20/436], Loss: 2.3109\n",
      "Epoch [43/50], Step [30/436], Loss: 2.3107\n",
      "Epoch [43/50], Step [40/436], Loss: 2.3165\n",
      "Epoch [43/50], Step [50/436], Loss: 2.3023\n",
      "Epoch [43/50], Step [60/436], Loss: 2.3104\n",
      "Epoch [43/50], Step [70/436], Loss: 2.3079\n",
      "Epoch [43/50], Step [80/436], Loss: 2.3003\n",
      "Epoch [43/50], Step [90/436], Loss: 2.3086\n",
      "Epoch [43/50], Step [100/436], Loss: 2.3133\n",
      "Epoch [43/50], Step [110/436], Loss: 2.2871\n",
      "Epoch [43/50], Step [120/436], Loss: 2.3031\n",
      "Epoch [43/50], Step [130/436], Loss: 2.2922\n",
      "Epoch [43/50], Step [140/436], Loss: 2.3064\n",
      "Epoch [43/50], Step [150/436], Loss: 2.3012\n",
      "Epoch [43/50], Step [160/436], Loss: 2.3065\n",
      "Epoch [43/50], Step [170/436], Loss: 2.2940\n",
      "Epoch [43/50], Step [180/436], Loss: 2.2863\n",
      "Epoch [43/50], Step [190/436], Loss: 2.3022\n",
      "Epoch [43/50], Step [200/436], Loss: 2.3204\n",
      "Epoch [43/50], Step [210/436], Loss: 2.2992\n",
      "Epoch [43/50], Step [220/436], Loss: 2.2902\n",
      "Epoch [43/50], Step [230/436], Loss: 2.3023\n",
      "Epoch [43/50], Step [240/436], Loss: 2.2978\n",
      "Epoch [43/50], Step [250/436], Loss: 2.2952\n",
      "Epoch [43/50], Step [260/436], Loss: 2.2993\n",
      "Epoch [43/50], Step [270/436], Loss: 2.3023\n",
      "Epoch [43/50], Step [280/436], Loss: 2.3023\n",
      "Epoch [43/50], Step [290/436], Loss: 2.2937\n",
      "Epoch [43/50], Step [300/436], Loss: 2.2983\n",
      "Epoch [43/50], Step [310/436], Loss: 2.3018\n",
      "Epoch [43/50], Step [320/436], Loss: 2.3200\n",
      "Epoch [43/50], Step [330/436], Loss: 2.3052\n",
      "Epoch [43/50], Step [340/436], Loss: 2.3019\n",
      "Epoch [43/50], Step [350/436], Loss: 2.3108\n",
      "Epoch [43/50], Step [360/436], Loss: 2.3052\n",
      "Epoch [43/50], Step [370/436], Loss: 2.3084\n",
      "Epoch [43/50], Step [380/436], Loss: 2.3135\n",
      "Epoch [43/50], Step [390/436], Loss: 2.3008\n",
      "Epoch [43/50], Step [400/436], Loss: 2.3159\n",
      "Epoch [43/50], Step [410/436], Loss: 2.2988\n",
      "Epoch [43/50], Step [420/436], Loss: 2.3043\n",
      "Epoch [43/50], Step [430/436], Loss: 2.2902\n",
      "Epoch [43/50], Time: 31.56s\n",
      "Train Loss: 2.3017, Train Acc: 0.1097\n",
      "Val Loss: 2.3051, Val Acc: 0.0994\n",
      "Epoch [44/50], Step [10/436], Loss: 2.2923\n",
      "Epoch [44/50], Step [20/436], Loss: 2.2968\n",
      "Epoch [44/50], Step [30/436], Loss: 2.2925\n",
      "Epoch [44/50], Step [40/436], Loss: 2.3045\n",
      "Epoch [44/50], Step [50/436], Loss: 2.2966\n",
      "Epoch [44/50], Step [60/436], Loss: 2.3169\n",
      "Epoch [44/50], Step [70/436], Loss: 2.2989\n",
      "Epoch [44/50], Step [80/436], Loss: 2.3030\n",
      "Epoch [44/50], Step [90/436], Loss: 2.3140\n",
      "Epoch [44/50], Step [100/436], Loss: 2.2896\n",
      "Epoch [44/50], Step [110/436], Loss: 2.3267\n",
      "Epoch [44/50], Step [120/436], Loss: 2.3228\n",
      "Epoch [44/50], Step [130/436], Loss: 2.2979\n",
      "Epoch [44/50], Step [140/436], Loss: 2.3130\n",
      "Epoch [44/50], Step [150/436], Loss: 2.2932\n",
      "Epoch [44/50], Step [160/436], Loss: 2.2941\n",
      "Epoch [44/50], Step [170/436], Loss: 2.3056\n",
      "Epoch [44/50], Step [180/436], Loss: 2.3021\n",
      "Epoch [44/50], Step [190/436], Loss: 2.2937\n",
      "Epoch [44/50], Step [200/436], Loss: 2.2951\n",
      "Epoch [44/50], Step [210/436], Loss: 2.2820\n",
      "Epoch [44/50], Step [220/436], Loss: 2.2903\n",
      "Epoch [44/50], Step [230/436], Loss: 2.3148\n",
      "Epoch [44/50], Step [240/436], Loss: 2.3009\n",
      "Epoch [44/50], Step [250/436], Loss: 2.3051\n",
      "Epoch [44/50], Step [260/436], Loss: 2.3057\n",
      "Epoch [44/50], Step [270/436], Loss: 2.3038\n",
      "Epoch [44/50], Step [280/436], Loss: 2.2926\n",
      "Epoch [44/50], Step [290/436], Loss: 2.3061\n",
      "Epoch [44/50], Step [300/436], Loss: 2.2971\n",
      "Epoch [44/50], Step [310/436], Loss: 2.3036\n",
      "Epoch [44/50], Step [320/436], Loss: 2.3093\n",
      "Epoch [44/50], Step [330/436], Loss: 2.2942\n",
      "Epoch [44/50], Step [340/436], Loss: 2.3086\n",
      "Epoch [44/50], Step [350/436], Loss: 2.3003\n",
      "Epoch [44/50], Step [360/436], Loss: 2.3024\n",
      "Epoch [44/50], Step [370/436], Loss: 2.3006\n",
      "Epoch [44/50], Step [380/436], Loss: 2.3075\n",
      "Epoch [44/50], Step [390/436], Loss: 2.2949\n",
      "Epoch [44/50], Step [400/436], Loss: 2.2751\n",
      "Epoch [44/50], Step [410/436], Loss: 2.2929\n",
      "Epoch [44/50], Step [420/436], Loss: 2.3052\n",
      "Epoch [44/50], Step [430/436], Loss: 2.3033\n",
      "Epoch [44/50], Time: 32.08s\n",
      "Train Loss: 2.3020, Train Acc: 0.1071\n",
      "Val Loss: 2.3035, Val Acc: 0.1097\n",
      "Epoch [45/50], Step [10/436], Loss: 2.3027\n",
      "Epoch [45/50], Step [20/436], Loss: 2.2958\n",
      "Epoch [45/50], Step [30/436], Loss: 2.3095\n",
      "Epoch [45/50], Step [40/436], Loss: 2.2894\n",
      "Epoch [45/50], Step [50/436], Loss: 2.3079\n",
      "Epoch [45/50], Step [60/436], Loss: 2.3090\n",
      "Epoch [45/50], Step [70/436], Loss: 2.2980\n",
      "Epoch [45/50], Step [80/436], Loss: 2.2902\n",
      "Epoch [45/50], Step [90/436], Loss: 2.2881\n",
      "Epoch [45/50], Step [100/436], Loss: 2.3021\n",
      "Epoch [45/50], Step [110/436], Loss: 2.2917\n",
      "Epoch [45/50], Step [120/436], Loss: 2.2888\n",
      "Epoch [45/50], Step [130/436], Loss: 2.3021\n",
      "Epoch [45/50], Step [140/436], Loss: 2.2830\n",
      "Epoch [45/50], Step [150/436], Loss: 2.3038\n",
      "Epoch [45/50], Step [160/436], Loss: 2.3037\n",
      "Epoch [45/50], Step [170/436], Loss: 2.2902\n",
      "Epoch [45/50], Step [180/436], Loss: 2.3184\n",
      "Epoch [45/50], Step [190/436], Loss: 2.2984\n",
      "Epoch [45/50], Step [200/436], Loss: 2.3081\n",
      "Epoch [45/50], Step [210/436], Loss: 2.3064\n",
      "Epoch [45/50], Step [220/436], Loss: 2.3026\n",
      "Epoch [45/50], Step [230/436], Loss: 2.3030\n",
      "Epoch [45/50], Step [240/436], Loss: 2.3220\n",
      "Epoch [45/50], Step [250/436], Loss: 2.3122\n",
      "Epoch [45/50], Step [260/436], Loss: 2.3015\n",
      "Epoch [45/50], Step [270/436], Loss: 2.3140\n",
      "Epoch [45/50], Step [280/436], Loss: 2.3008\n",
      "Epoch [45/50], Step [290/436], Loss: 2.3091\n",
      "Epoch [45/50], Step [300/436], Loss: 2.3040\n",
      "Epoch [45/50], Step [310/436], Loss: 2.3008\n",
      "Epoch [45/50], Step [320/436], Loss: 2.2995\n",
      "Epoch [45/50], Step [330/436], Loss: 2.2963\n",
      "Epoch [45/50], Step [340/436], Loss: 2.3129\n",
      "Epoch [45/50], Step [350/436], Loss: 2.2953\n",
      "Epoch [45/50], Step [360/436], Loss: 2.3084\n",
      "Epoch [45/50], Step [370/436], Loss: 2.3094\n",
      "Epoch [45/50], Step [380/436], Loss: 2.2843\n",
      "Epoch [45/50], Step [390/436], Loss: 2.3063\n",
      "Epoch [45/50], Step [400/436], Loss: 2.3046\n",
      "Epoch [45/50], Step [410/436], Loss: 2.2987\n",
      "Epoch [45/50], Step [420/436], Loss: 2.2956\n",
      "Epoch [45/50], Step [430/436], Loss: 2.2956\n",
      "Epoch [45/50], Time: 32.36s\n",
      "Train Loss: 2.3017, Train Acc: 0.1063\n",
      "Val Loss: 2.3056, Val Acc: 0.1045\n",
      "Epoch [46/50], Step [10/436], Loss: 2.2869\n",
      "Epoch [46/50], Step [20/436], Loss: 2.2912\n",
      "Epoch [46/50], Step [30/436], Loss: 2.2869\n",
      "Epoch [46/50], Step [40/436], Loss: 2.3053\n",
      "Epoch [46/50], Step [50/436], Loss: 2.2861\n",
      "Epoch [46/50], Step [60/436], Loss: 2.3036\n",
      "Epoch [46/50], Step [70/436], Loss: 2.2897\n",
      "Epoch [46/50], Step [80/436], Loss: 2.3089\n",
      "Epoch [46/50], Step [90/436], Loss: 2.3120\n",
      "Epoch [46/50], Step [100/436], Loss: 2.2895\n",
      "Epoch [46/50], Step [110/436], Loss: 2.3239\n",
      "Epoch [46/50], Step [120/436], Loss: 2.2980\n",
      "Epoch [46/50], Step [130/436], Loss: 2.3033\n",
      "Epoch [46/50], Step [140/436], Loss: 2.3003\n",
      "Epoch [46/50], Step [150/436], Loss: 2.3094\n",
      "Epoch [46/50], Step [160/436], Loss: 2.2937\n",
      "Epoch [46/50], Step [170/436], Loss: 2.2995\n",
      "Epoch [46/50], Step [180/436], Loss: 2.3057\n",
      "Epoch [46/50], Step [190/436], Loss: 2.3198\n",
      "Epoch [46/50], Step [200/436], Loss: 2.3061\n",
      "Epoch [46/50], Step [210/436], Loss: 2.3023\n",
      "Epoch [46/50], Step [220/436], Loss: 2.3016\n",
      "Epoch [46/50], Step [230/436], Loss: 2.2842\n",
      "Epoch [46/50], Step [240/436], Loss: 2.3157\n",
      "Epoch [46/50], Step [250/436], Loss: 2.2869\n",
      "Epoch [46/50], Step [260/436], Loss: 2.2951\n",
      "Epoch [46/50], Step [270/436], Loss: 2.2955\n",
      "Epoch [46/50], Step [280/436], Loss: 2.3088\n",
      "Epoch [46/50], Step [290/436], Loss: 2.3141\n",
      "Epoch [46/50], Step [300/436], Loss: 2.3046\n",
      "Epoch [46/50], Step [310/436], Loss: 2.2989\n",
      "Epoch [46/50], Step [320/436], Loss: 2.3002\n",
      "Epoch [46/50], Step [330/436], Loss: 2.2995\n",
      "Epoch [46/50], Step [340/436], Loss: 2.2965\n",
      "Epoch [46/50], Step [350/436], Loss: 2.2880\n",
      "Epoch [46/50], Step [360/436], Loss: 2.3163\n",
      "Epoch [46/50], Step [370/436], Loss: 2.3170\n",
      "Epoch [46/50], Step [380/436], Loss: 2.2974\n",
      "Epoch [46/50], Step [390/436], Loss: 2.2971\n",
      "Epoch [46/50], Step [400/436], Loss: 2.3005\n",
      "Epoch [46/50], Step [410/436], Loss: 2.3020\n",
      "Epoch [46/50], Step [420/436], Loss: 2.3045\n",
      "Epoch [46/50], Step [430/436], Loss: 2.2867\n",
      "Epoch [46/50], Time: 31.39s\n",
      "Train Loss: 2.3021, Train Acc: 0.1084\n",
      "Val Loss: 2.3040, Val Acc: 0.0990\n",
      "Epoch [47/50], Step [10/436], Loss: 2.2870\n",
      "Epoch [47/50], Step [20/436], Loss: 2.3109\n",
      "Epoch [47/50], Step [30/436], Loss: 2.2980\n",
      "Epoch [47/50], Step [40/436], Loss: 2.2900\n",
      "Epoch [47/50], Step [50/436], Loss: 2.2925\n",
      "Epoch [47/50], Step [60/436], Loss: 2.2868\n",
      "Epoch [47/50], Step [70/436], Loss: 2.3014\n",
      "Epoch [47/50], Step [80/436], Loss: 2.3023\n",
      "Epoch [47/50], Step [90/436], Loss: 2.2963\n",
      "Epoch [47/50], Step [100/436], Loss: 2.3188\n",
      "Epoch [47/50], Step [110/436], Loss: 2.2932\n",
      "Epoch [47/50], Step [120/436], Loss: 2.2901\n",
      "Epoch [47/50], Step [130/436], Loss: 2.2949\n",
      "Epoch [47/50], Step [140/436], Loss: 2.2852\n",
      "Epoch [47/50], Step [150/436], Loss: 2.3064\n",
      "Epoch [47/50], Step [160/436], Loss: 2.3047\n",
      "Epoch [47/50], Step [170/436], Loss: 2.3019\n",
      "Epoch [47/50], Step [180/436], Loss: 2.3181\n",
      "Epoch [47/50], Step [190/436], Loss: 2.3028\n",
      "Epoch [47/50], Step [200/436], Loss: 2.2952\n",
      "Epoch [47/50], Step [210/436], Loss: 2.2954\n",
      "Epoch [47/50], Step [220/436], Loss: 2.3061\n",
      "Epoch [47/50], Step [230/436], Loss: 2.2962\n",
      "Epoch [47/50], Step [240/436], Loss: 2.2958\n",
      "Epoch [47/50], Step [250/436], Loss: 2.3013\n",
      "Epoch [47/50], Step [260/436], Loss: 2.3033\n",
      "Epoch [47/50], Step [270/436], Loss: 2.2914\n",
      "Epoch [47/50], Step [280/436], Loss: 2.2857\n",
      "Epoch [47/50], Step [290/436], Loss: 2.3047\n",
      "Epoch [47/50], Step [300/436], Loss: 2.3141\n",
      "Epoch [47/50], Step [310/436], Loss: 2.3107\n",
      "Epoch [47/50], Step [320/436], Loss: 2.2997\n",
      "Epoch [47/50], Step [330/436], Loss: 2.3082\n",
      "Epoch [47/50], Step [340/436], Loss: 2.2962\n",
      "Epoch [47/50], Step [350/436], Loss: 2.3305\n",
      "Epoch [47/50], Step [360/436], Loss: 2.2949\n",
      "Epoch [47/50], Step [370/436], Loss: 2.3032\n",
      "Epoch [47/50], Step [380/436], Loss: 2.3030\n",
      "Epoch [47/50], Step [390/436], Loss: 2.3062\n",
      "Epoch [47/50], Step [400/436], Loss: 2.3155\n",
      "Epoch [47/50], Step [410/436], Loss: 2.3023\n",
      "Epoch [47/50], Step [420/436], Loss: 2.2858\n",
      "Epoch [47/50], Step [430/436], Loss: 2.2945\n",
      "Epoch [47/50], Time: 34.01s\n",
      "Train Loss: 2.3017, Train Acc: 0.1082\n",
      "Val Loss: 2.3036, Val Acc: 0.1052\n",
      "Epoch [48/50], Step [10/436], Loss: 2.2770\n",
      "Epoch [48/50], Step [20/436], Loss: 2.2914\n",
      "Epoch [48/50], Step [30/436], Loss: 2.2939\n",
      "Epoch [48/50], Step [40/436], Loss: 2.3034\n",
      "Epoch [48/50], Step [50/436], Loss: 2.2950\n",
      "Epoch [48/50], Step [60/436], Loss: 2.2834\n",
      "Epoch [48/50], Step [70/436], Loss: 2.3140\n",
      "Epoch [48/50], Step [80/436], Loss: 2.3115\n",
      "Epoch [48/50], Step [90/436], Loss: 2.3014\n",
      "Epoch [48/50], Step [100/436], Loss: 2.3167\n",
      "Epoch [48/50], Step [110/436], Loss: 2.3170\n",
      "Epoch [48/50], Step [120/436], Loss: 2.3043\n",
      "Epoch [48/50], Step [130/436], Loss: 2.3029\n",
      "Epoch [48/50], Step [140/436], Loss: 2.3193\n",
      "Epoch [48/50], Step [150/436], Loss: 2.3033\n",
      "Epoch [48/50], Step [160/436], Loss: 2.3193\n",
      "Epoch [48/50], Step [170/436], Loss: 2.2894\n",
      "Epoch [48/50], Step [180/436], Loss: 2.3024\n",
      "Epoch [48/50], Step [190/436], Loss: 2.3036\n",
      "Epoch [48/50], Step [200/436], Loss: 2.2937\n",
      "Epoch [48/50], Step [210/436], Loss: 2.3017\n",
      "Epoch [48/50], Step [220/436], Loss: 2.2929\n",
      "Epoch [48/50], Step [230/436], Loss: 2.3059\n",
      "Epoch [48/50], Step [240/436], Loss: 2.3017\n",
      "Epoch [48/50], Step [250/436], Loss: 2.3040\n",
      "Epoch [48/50], Step [260/436], Loss: 2.3035\n",
      "Epoch [48/50], Step [270/436], Loss: 2.3047\n",
      "Epoch [48/50], Step [280/436], Loss: 2.3025\n",
      "Epoch [48/50], Step [290/436], Loss: 2.2913\n",
      "Epoch [48/50], Step [300/436], Loss: 2.3128\n",
      "Epoch [48/50], Step [310/436], Loss: 2.3011\n",
      "Epoch [48/50], Step [320/436], Loss: 2.3166\n",
      "Epoch [48/50], Step [330/436], Loss: 2.3169\n",
      "Epoch [48/50], Step [340/436], Loss: 2.2997\n",
      "Epoch [48/50], Step [350/436], Loss: 2.3050\n",
      "Epoch [48/50], Step [360/436], Loss: 2.3099\n",
      "Epoch [48/50], Step [370/436], Loss: 2.2982\n",
      "Epoch [48/50], Step [380/436], Loss: 2.3008\n",
      "Epoch [48/50], Step [390/436], Loss: 2.2959\n",
      "Epoch [48/50], Step [400/436], Loss: 2.2855\n",
      "Epoch [48/50], Step [410/436], Loss: 2.3117\n",
      "Epoch [48/50], Step [420/436], Loss: 2.3020\n",
      "Epoch [48/50], Step [430/436], Loss: 2.2970\n",
      "Epoch [48/50], Time: 34.53s\n",
      "Train Loss: 2.3019, Train Acc: 0.1077\n",
      "Val Loss: 2.3041, Val Acc: 0.1106\n",
      "New best model saved at epoch 48\n",
      "Epoch [49/50], Step [10/436], Loss: 2.3007\n",
      "Epoch [49/50], Step [20/436], Loss: 2.2929\n",
      "Epoch [49/50], Step [30/436], Loss: 2.2895\n",
      "Epoch [49/50], Step [40/436], Loss: 2.3028\n",
      "Epoch [49/50], Step [50/436], Loss: 2.2928\n",
      "Epoch [49/50], Step [60/436], Loss: 2.3179\n",
      "Epoch [49/50], Step [70/436], Loss: 2.2942\n",
      "Epoch [49/50], Step [80/436], Loss: 2.3091\n",
      "Epoch [49/50], Step [90/436], Loss: 2.2962\n",
      "Epoch [49/50], Step [100/436], Loss: 2.3103\n",
      "Epoch [49/50], Step [110/436], Loss: 2.2987\n",
      "Epoch [49/50], Step [120/436], Loss: 2.2992\n",
      "Epoch [49/50], Step [130/436], Loss: 2.2883\n",
      "Epoch [49/50], Step [140/436], Loss: 2.3029\n",
      "Epoch [49/50], Step [150/436], Loss: 2.3087\n",
      "Epoch [49/50], Step [160/436], Loss: 2.3032\n",
      "Epoch [49/50], Step [170/436], Loss: 2.3210\n",
      "Epoch [49/50], Step [180/436], Loss: 2.3154\n",
      "Epoch [49/50], Step [190/436], Loss: 2.2878\n",
      "Epoch [49/50], Step [200/436], Loss: 2.2971\n",
      "Epoch [49/50], Step [210/436], Loss: 2.3098\n",
      "Epoch [49/50], Step [220/436], Loss: 2.2880\n",
      "Epoch [49/50], Step [230/436], Loss: 2.2915\n",
      "Epoch [49/50], Step [240/436], Loss: 2.3074\n",
      "Epoch [49/50], Step [250/436], Loss: 2.2913\n",
      "Epoch [49/50], Step [260/436], Loss: 2.3002\n",
      "Epoch [49/50], Step [270/436], Loss: 2.2954\n",
      "Epoch [49/50], Step [280/436], Loss: 2.2907\n",
      "Epoch [49/50], Step [290/436], Loss: 2.3117\n",
      "Epoch [49/50], Step [300/436], Loss: 2.2942\n",
      "Epoch [49/50], Step [310/436], Loss: 2.3107\n",
      "Epoch [49/50], Step [320/436], Loss: 2.3130\n",
      "Epoch [49/50], Step [330/436], Loss: 2.2998\n",
      "Epoch [49/50], Step [340/436], Loss: 2.3133\n",
      "Epoch [49/50], Step [350/436], Loss: 2.2981\n",
      "Epoch [49/50], Step [360/436], Loss: 2.3074\n",
      "Epoch [49/50], Step [370/436], Loss: 2.3003\n",
      "Epoch [49/50], Step [380/436], Loss: 2.3026\n",
      "Epoch [49/50], Step [390/436], Loss: 2.2944\n",
      "Epoch [49/50], Step [400/436], Loss: 2.3011\n",
      "Epoch [49/50], Step [410/436], Loss: 2.2871\n",
      "Epoch [49/50], Step [420/436], Loss: 2.2913\n",
      "Epoch [49/50], Step [430/436], Loss: 2.3104\n",
      "Epoch [49/50], Time: 31.66s\n",
      "Train Loss: 2.3022, Train Acc: 0.1090\n",
      "Val Loss: 2.3043, Val Acc: 0.1039\n",
      "Epoch [50/50], Step [10/436], Loss: 2.2994\n",
      "Epoch [50/50], Step [20/436], Loss: 2.3070\n",
      "Epoch [50/50], Step [30/436], Loss: 2.3229\n",
      "Epoch [50/50], Step [40/436], Loss: 2.3260\n",
      "Epoch [50/50], Step [50/436], Loss: 2.2896\n",
      "Epoch [50/50], Step [60/436], Loss: 2.2924\n",
      "Epoch [50/50], Step [70/436], Loss: 2.2966\n",
      "Epoch [50/50], Step [80/436], Loss: 2.3105\n",
      "Epoch [50/50], Step [90/436], Loss: 2.3001\n",
      "Epoch [50/50], Step [100/436], Loss: 2.3017\n",
      "Epoch [50/50], Step [110/436], Loss: 2.3100\n",
      "Epoch [50/50], Step [120/436], Loss: 2.2951\n",
      "Epoch [50/50], Step [130/436], Loss: 2.3069\n",
      "Epoch [50/50], Step [140/436], Loss: 2.2867\n",
      "Epoch [50/50], Step [150/436], Loss: 2.3075\n",
      "Epoch [50/50], Step [160/436], Loss: 2.2935\n",
      "Epoch [50/50], Step [170/436], Loss: 2.2980\n",
      "Epoch [50/50], Step [180/436], Loss: 2.2873\n",
      "Epoch [50/50], Step [190/436], Loss: 2.3012\n",
      "Epoch [50/50], Step [200/436], Loss: 2.2956\n",
      "Epoch [50/50], Step [210/436], Loss: 2.2921\n",
      "Epoch [50/50], Step [220/436], Loss: 2.3076\n",
      "Epoch [50/50], Step [230/436], Loss: 2.3006\n",
      "Epoch [50/50], Step [240/436], Loss: 2.3098\n",
      "Epoch [50/50], Step [250/436], Loss: 2.3099\n",
      "Epoch [50/50], Step [260/436], Loss: 2.3121\n",
      "Epoch [50/50], Step [270/436], Loss: 2.2912\n",
      "Epoch [50/50], Step [280/436], Loss: 2.3037\n",
      "Epoch [50/50], Step [290/436], Loss: 2.3047\n",
      "Epoch [50/50], Step [300/436], Loss: 2.3016\n",
      "Epoch [50/50], Step [310/436], Loss: 2.3047\n",
      "Epoch [50/50], Step [320/436], Loss: 2.2997\n",
      "Epoch [50/50], Step [330/436], Loss: 2.2928\n",
      "Epoch [50/50], Step [340/436], Loss: 2.2849\n",
      "Epoch [50/50], Step [350/436], Loss: 2.3067\n",
      "Epoch [50/50], Step [360/436], Loss: 2.3049\n",
      "Epoch [50/50], Step [370/436], Loss: 2.2954\n",
      "Epoch [50/50], Step [380/436], Loss: 2.3146\n",
      "Epoch [50/50], Step [390/436], Loss: 2.3037\n",
      "Epoch [50/50], Step [400/436], Loss: 2.3017\n",
      "Epoch [50/50], Step [410/436], Loss: 2.3111\n",
      "Epoch [50/50], Step [420/436], Loss: 2.2955\n",
      "Epoch [50/50], Step [430/436], Loss: 2.2913\n",
      "Epoch [50/50], Time: 32.11s\n",
      "Train Loss: 2.3018, Train Acc: 0.1118\n",
      "Val Loss: 2.3040, Val Acc: 0.1116\n",
      "New best model saved at epoch 50\n",
      "Best validation accuracy: 0.1116\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'glob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 64\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 64\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 45\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m     44\u001b[0m best_model_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_transfer_model_epoch_*.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 45\u001b[0m best_model_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[43mglob\u001b[49m\u001b[38;5;241m.\u001b[39mglob(best_model_path), key\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mgetctime)\n\u001b[0;32m     46\u001b[0m transfer_model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(best_model_file))\n\u001b[0;32m     47\u001b[0m transfer_model\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'glob' is not defined"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load and prepare data\n",
    "    (train_images, train_labels), (val_images, val_labels), (test_images, test_labels) = load_and_prepare_data('test_digits')\n",
    "\n",
    "    # Create datasets and dataloaders\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "        transforms.RandomPerspective(distortion_scale=0.2, p=0.5),\n",
    "    ])\n",
    "\n",
    "    train_dataset = DigitDataset(train_images, train_labels, transform=train_transform)\n",
    "    val_dataset = DigitDataset(val_images, val_labels)\n",
    "    test_dataset = DigitDataset(test_images, test_labels)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=0, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=0, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "    # Create and prepare the model\n",
    "    base_ensemble = create_ensemble()\n",
    "    print(\"Base ensemble structure:\")\n",
    "    print(base_ensemble)\n",
    "    \n",
    "    transfer_model = TransferEnsembleModel(base_ensemble).to(device)\n",
    "    print(\"Transfer model structure:\")\n",
    "    print(transfer_model)\n",
    "\n",
    "    # Training setup\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(transfer_model.new_fc.parameters(), lr=0.001)\n",
    "\n",
    "    # Create directory for saving models\n",
    "    save_dir = 'best_boi_models_finetuned_standard'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Train the model\n",
    "    train_model(transfer_model, train_loader, val_loader, criterion, optimizer, num_epochs=50, device=device, save_dir=save_dir)\n",
    "\n",
    "    # Evaluate the model\n",
    "    best_model_path = os.path.join(save_dir, 'best_transfer_model_epoch_*.pth')\n",
    "    best_model_file = max(glob.glob(best_model_path), key=os.path.getctime)\n",
    "    transfer_model.load_state_dict(torch.load(best_model_file))\n",
    "    transfer_model.eval()\n",
    "\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = transfer_model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_acc = test_correct / test_total\n",
    "    print(f'Test Accuracy: {test_acc:.4f}')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
